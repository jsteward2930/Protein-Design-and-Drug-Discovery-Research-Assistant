example_id,input_messages,output_messages,metadata_output,metadata_llm,metadata_openinference,metadata_input,metadata_annotations,input_input,input_model,input_voice,output_output,metadata_message_length
RGF0YXNldEV4YW1wbGU6MQ==,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/setup-tracing\ntitle: Setup Tracing\n\nSetup Tracing\nPhoenix uses OTLP (OpenTelemetry Language Protocol) to receive traces from your application. First you will have to setup you application to send traces to phoenix.\nSetup Tracing: Python\nSetup Tracing: TS\nNext Steps\nPhoenix supports many integrations in the form of auto-Instrumentors as well as manually creating spans by instrumenting your code:\nAuto Instrumentation\nManual Instrumentation\nPrevious\nHow-to: Tracing\nNext\nSetup Tracing: Python\nLast updated \n8 hours ago\n\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/how-does-tracing-work\ntitle: How Tracing Works\n\nHow Tracing Works\nThe components behind tracing\nThe phoenix server is collector of traces over OTLP\nInstrumentation\nIn order for an application to emit traces for analysis, the application must be \ninstrumented\n.  Your application can be \nmanually\n instrumented or be \nautomatically\n instrumented.\n\nWith phoenix, there a set of plugins (\ninstrumentors\n) that can be added to your application's startup process that perform auto-instrumentation. These plugins collect spans for your application and export them for collection and visualization. For phoenix, all the instrumentors are managed via a single repository called \nOpenInference\n. The comprehensive list of instrumentors can be found in the \nhow-to guide.\nExporter\nAn exporter takes the spans created via \ninstrumentation \nand exports them to a \ncollector\n. In simple terms, it just sends the data to the Phoenix. When using Phoenix, most of this is completely done under the hood when you call instrument on an instrumentor.\nCollector\nThe Phoenix server is a collector and a UI that helps you troubleshoot your application in real time. When you run or run phoenix (e.x. \npx.launch_app()\n, container), Phoenix starts receiving spans form any application(s) that is exporting spans to it.\nOpenTelememetry Protocol\nOpenTelemetetry Protocol (or OTLP for short) is the means by which traces arrive from your application to the Phoenix collector. Phoenix currently supports OTLP over HTTP.\nPrevious\nTraces\nNext\nAnnotating Traces\nLast updated \n3 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I send traces to Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'You can send traces to Phoenix by setting up your application to emit traces and then exporting them to a collector using instrumentation. Phoenix supports auto-instrumentation through plugins called instrumentors, which collect spans for your application and export them for visualization. Additionally, an exporter takes the spans created via instrumentation and sends them to the Phoenix collector.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/setup-tracing\ntitle: Setup Tracing\n\nSetup Tracing\nPhoenix uses OTLP (OpenTelemetry Language Protocol) to receive traces from your application. First you will have to setup you application to send traces to phoenix.\nSetup Tracing: Python\nSetup Tracing: TS\nNext Steps\nPhoenix supports many integrations in the form of auto-Instrumentors as well as manually creating spans by instrumenting your code:\nAuto Instrumentation\nManual Instrumentation\nPrevious\nHow-to: Tracing\nNext\nSetup Tracing: Python\nLast updated \n8 hours ago\n\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/how-does-tracing-work\ntitle: How Tracing Works\n\nHow Tracing Works\nThe components behind tracing\nThe phoenix server is collector of traces over OTLP\nInstrumentation\nIn order for an application to emit traces for analysis, the application must be \ninstrumented\n.  Your application can be \nmanually\n instrumented or be \nautomatically\n instrumented.\n\nWith phoenix, there a set of plugins (\ninstrumentors\n) that can be added to your application's startup process that perform auto-instrumentation. These plugins collect spans for your application and export them for collection and visualization. For phoenix, all the instrumentors are managed via a single repository called \nOpenInference\n. The comprehensive list of instrumentors can be found in the \nhow-to guide.\nExporter\nAn exporter takes the spans created via \ninstrumentation \nand exports them to a \ncollector\n. In simple terms, it just sends the data to the Phoenix. When using Phoenix, most of this is completely done under the hood when you call instrument on an instrumentor.\nCollector\nThe Phoenix server is a collector and a UI that helps you troubleshoot your application in real time. When you run or run phoenix (e.x. \npx.launch_app()\n, container), Phoenix starts receiving spans form any application(s) that is exporting spans to it.\nOpenTelememetry Protocol\nOpenTelemetetry Protocol (or OTLP for short) is the means by which traces arrive from your application to the Phoenix collector. Phoenix currently supports OTLP over HTTP.\nPrevious\nTraces\nNext\nAnnotating Traces\nLast updated \n3 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I send traces to Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'You can send traces to Phoenix by setting up your application to emit traces and then exporting them to a collector using instrumentation. Phoenix supports auto-instrumentation through plugins called instrumentors, which collect spans for your application and export them for visualization. Additionally, an exporter takes the spans created via instrumentation and sends them to the Phoenix collector.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6Mg==,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nThe reply half may be formatted for response purposes (e.g., contain blocks for completion by the addressee).\\nb. A double card must be folded before mailing and prepared so that the \\naddress on the reply half is on the inside when the double card is originally \\nmailed. The address side of the reply half may be prepared as Business \\nReply Mail, Courtesy Reply Mail, meter reply mail, or as a USPS Returns service label.\\nc. Plain stickers, seals, or a single wire stitch (staple) may be used to fasten the \\nopen edge at the top or bottom once the card is folded if affixed so that the \\ninner surfaces of the cards can be readily examined. Fasteners must be \\naffixed according to the applicable preparation requirements for the price claimed. Any sealing on the left and right sides of the cards, no matter the \\nsealing process used, is not permitted.\\nd. The first half of a double card must be detached when the reply half is \\nmailed for return. \\n6.2.10   Enclosures\\nEnclosures in double postcards are prohibited at card prices. \\n6.3 Nonmachinable Pieces\\n6.3.1   Nonmachinable Letters\\nLetter-size pieces (except card-size pieces) that meet one or more of the \\nnonmachinable characteristics in 1.2 are subject to the nonmachinable \\nsurcharge (see 133.1.7 ). \\n6.3.2   Nonmachinable Flats\\nFlat-size pieces that do not meet the standards in 2.0 are considered parcels, \\nand the mailer must pay the applicable parcel price.  \\n6.4 Parcels \\n[7-9-23]  USPS Ground Advantage \\u2014 Retail parcels are eligible for USPS \\nTracking and Signature Confirmation service. A USPS Ground Advantage \\u2014 \\nRetail parcel is the following:\\na. A mailpiece that exceeds any one of the maximum dimensions for a flat \\n(large envelope). See 2.1.\\nb. A flat-size mailpiece, regardless of thickness, that is rigid or nonrectangular. \\nc. A flat-size mailpiece that is not uniformly thick under 2.4. \\nd.[7-9-23]  A mailpiece that does not exceed 130 inches in combined length \\nand girth.\\n7.0 Additional Physical Standards for Media Mail and Library \\nMail\\nThese standards apply to Media Mail and Library Mail:\\n\\npage_label: 4\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.1Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\n4.0 Additional Physical Standa rds for Priority Mail Express\\nEach piece of Priority Mail Express may not weigh more than 70 pounds. The \\ncombined length and girth of a piece (the length of its longest side plus the \\ndistance around its thickest part) may not exceed 108 inches. Lower size or weight standards apply to Priority Mail Express addressed to certain APO/FPO \\nand DPOs. Priority Mail Express items must be large enough to hold the required \\nmailing labels and indicia on a single optical plane without bending or folding.\\n5.0 Additional Physical St andards for Priority Mail\\nThe maximum weight is 70 pounds. The combined length and girth of a piece \\n(the length of its longest side plus the distance around its thickest part) may not \\nexceed 108 inches. Lower size and weight standards apply for some APO/FPO \\nand DPO mail subject to 703.2.0 , and 703.4.0 , and for Department of State mail \\nsubject to 703.3.0 . \\n[7-9-23] \\n6.0 Additional Physical Standa rds for First-Class Mail and \\nUSPS Ground Advantage \\u2014 Retail\\n[7-9-23]\\n6.1 Maximum Weight\\n6.1.1   First-Class Mail\\nFirst-Class Mail (letters and flats) must not exceed 13 ounces.\n\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nNot more than 4-1/4 inches high, or more than 6 inches long, or greater than \\n0.016 inch thick.\\nd. Not more than 3.5 ounces (Charge flat-size prices for First-Class Mail \\ncard-type pieces over 3.5 ounces.)\\n\\n  Instruction: Based on the above documents, provide a detailed answer for the user question below.\\n  Answer \\""don\'t know\\"" if not present in the document.\\n  "",\n\n\n       \n""llm.input_messages.1.message.role""\n:\n \n""user""\n,\n\n\n       \n""llm.input_messages.1.message.content""\n:\n \n""Hello""\n,\n\n\n       \n""llm.model_name""\n:\n \n""gpt-4-turbo-preview""\n,\n\n\n       \n""llm.invocation_parameters""\n:\n \n""{\\""temperature\\"": 0.1, \\""model\\"": \\""gpt-4-turbo-preview\\""}""\n,\n\n\n       \n""output.value""\n:\n \n""How are you?""\n }\n,\n\n\n   \n""events""\n:\n []\n,\n\n\n   \n""links""\n:\n []\n,\n\n\n   \n""resource""\n:\n {\n\n\n       \n""attributes""\n:\n {}\n,\n\n\n       \n""schema_url""\n:\n \n""""\n\n\n   }\n\n\n}\nSpans can be nested, as is implied by the presence of a parent span ID: child spans represent sub-operations. This allows spans to more accurately capture the work done in an application.\nTraces\nA trace records the paths taken by requests (made by an application or end-user) as they propagate through multiple steps.\nWithout tracing, it is challenging to pinpoint the cause of performance problems in a system.\nIt improves the visibility of our application or system’s health and lets us debug behavior that is difficult to reproduce locally. Tracing is essential for LLM applications, which commonly have nondeterministic problems or are too complicated to reproduce locally.\nTracing makes debugging and understanding LLM applications less daunting by breaking down what happens within a request as it flows through a system.\nA trace is made of one or more spans. The first span represents the root span. Each root span represents a request from start to finish. The spans underneath the parent provide a more in-depth context of what occurs during a request (or what steps make up a request).\nProjects\nA \nproject\n is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces. Projects can be useful for various use-cases such as separating out environments, logging traces for evaluation runs, etc. To learn more about how to setup projects,  see the \nhow-to guide.\nSpan Kind\nWhen a span is created,  it is created as one of the following: Chain, Retriever, Reranker, LLM, Embedding, Agent, or Tool. \nCHAIN\nA Chain is a starting point or a link between different LLM application steps. For example, a Chain span could be used to represent the beginning of a request to an LLM application or the glue code that passes context from a retriever to and LLM call.\nRETRIEVER\nA Retriever is a span that represents a data retrieval step. For example, a Retriever span could be used to represent a call to a vector store or a database.\nRERANKER\nA Reranker is a span that represents the reranking of a set of input documents. For example, a cross-encoder may be used to compute the input documents\' relevance scores with respect to a user query, and the top K documents with the highest scores are then returned by the Reranker.\nLLM\nAn LLM is a span that represents a call to an LLM. For example, an LLM span could be used to represent a call to OpenAI or Llama.\nEMBEDDING\nAn Embedding is a span that represents a call to an LLM for an embedding. For example, an Embedding span could be used to represent a call OpenAI to get an ada-2 embedding for retrieval.\nTOOL\nA Tool is a span that represents a call to an external tool such as a calculator or a weather API.\nAGENT\nA span that encompasses calls to LLMs and Tools. An agent describes a reasoning block that acts on tools using the guidance of an LLM.\n\nSpan Attributes\nAttributes are key-value pairs that contain metadata that you can use to annotate a span to carry information about the operation it is tracking.\nFor example, if a span invokes an LLM, you can capture the model name, the invocation parameters, the token count, and so on.\nAttributes have the following rules:\nKeys must be non-null string values\nValues must be a non-null string, boolean, floating point value, integer, or an array of these values Additionally, there are Semantic Attributes, which are known naming conventions for metadata that is typically present in common operations. It\'s helpful to use semantic attribute naming wherever possible so that common kinds of metadata are standardized across systems. See \nsemantic conventions\n for more information.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What happens if I send the same traces twice? \nAnswer: '}, {'role': 'assistant', 'content': 'Tracing allows you to record the paths taken by requests as they move through different steps. Sending the same traces twice can result in duplicate records of the paths taken by those requests, which may lead to redundant or overlapping information in the trace data.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nThe reply half may be formatted for response purposes (e.g., contain blocks for completion by the addressee).\\nb. A double card must be folded before mailing and prepared so that the \\naddress on the reply half is on the inside when the double card is originally \\nmailed. The address side of the reply half may be prepared as Business \\nReply Mail, Courtesy Reply Mail, meter reply mail, or as a USPS Returns service label.\\nc. Plain stickers, seals, or a single wire stitch (staple) may be used to fasten the \\nopen edge at the top or bottom once the card is folded if affixed so that the \\ninner surfaces of the cards can be readily examined. Fasteners must be \\naffixed according to the applicable preparation requirements for the price claimed. Any sealing on the left and right sides of the cards, no matter the \\nsealing process used, is not permitted.\\nd. The first half of a double card must be detached when the reply half is \\nmailed for return. \\n6.2.10   Enclosures\\nEnclosures in double postcards are prohibited at card prices. \\n6.3 Nonmachinable Pieces\\n6.3.1   Nonmachinable Letters\\nLetter-size pieces (except card-size pieces) that meet one or more of the \\nnonmachinable characteristics in 1.2 are subject to the nonmachinable \\nsurcharge (see 133.1.7 ). \\n6.3.2   Nonmachinable Flats\\nFlat-size pieces that do not meet the standards in 2.0 are considered parcels, \\nand the mailer must pay the applicable parcel price.  \\n6.4 Parcels \\n[7-9-23]  USPS Ground Advantage \\u2014 Retail parcels are eligible for USPS \\nTracking and Signature Confirmation service. A USPS Ground Advantage \\u2014 \\nRetail parcel is the following:\\na. A mailpiece that exceeds any one of the maximum dimensions for a flat \\n(large envelope). See 2.1.\\nb. A flat-size mailpiece, regardless of thickness, that is rigid or nonrectangular. \\nc. A flat-size mailpiece that is not uniformly thick under 2.4. \\nd.[7-9-23]  A mailpiece that does not exceed 130 inches in combined length \\nand girth.\\n7.0 Additional Physical Standards for Media Mail and Library \\nMail\\nThese standards apply to Media Mail and Library Mail:\\n\\npage_label: 4\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.1Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\n4.0 Additional Physical Standa rds for Priority Mail Express\\nEach piece of Priority Mail Express may not weigh more than 70 pounds. The \\ncombined length and girth of a piece (the length of its longest side plus the \\ndistance around its thickest part) may not exceed 108 inches. Lower size or weight standards apply to Priority Mail Express addressed to certain APO/FPO \\nand DPOs. Priority Mail Express items must be large enough to hold the required \\nmailing labels and indicia on a single optical plane without bending or folding.\\n5.0 Additional Physical St andards for Priority Mail\\nThe maximum weight is 70 pounds. The combined length and girth of a piece \\n(the length of its longest side plus the distance around its thickest part) may not \\nexceed 108 inches. Lower size and weight standards apply for some APO/FPO \\nand DPO mail subject to 703.2.0 , and 703.4.0 , and for Department of State mail \\nsubject to 703.3.0 . \\n[7-9-23] \\n6.0 Additional Physical Standa rds for First-Class Mail and \\nUSPS Ground Advantage \\u2014 Retail\\n[7-9-23]\\n6.1 Maximum Weight\\n6.1.1   First-Class Mail\\nFirst-Class Mail (letters and flats) must not exceed 13 ounces.\n\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nNot more than 4-1/4 inches high, or more than 6 inches long, or greater than \\n0.016 inch thick.\\nd. Not more than 3.5 ounces (Charge flat-size prices for First-Class Mail \\ncard-type pieces over 3.5 ounces.)\\n\\n  Instruction: Based on the above documents, provide a detailed answer for the user question below.\\n  Answer \\""don\'t know\\"" if not present in the document.\\n  "",\n\n\n       \n""llm.input_messages.1.message.role""\n:\n \n""user""\n,\n\n\n       \n""llm.input_messages.1.message.content""\n:\n \n""Hello""\n,\n\n\n       \n""llm.model_name""\n:\n \n""gpt-4-turbo-preview""\n,\n\n\n       \n""llm.invocation_parameters""\n:\n \n""{\\""temperature\\"": 0.1, \\""model\\"": \\""gpt-4-turbo-preview\\""}""\n,\n\n\n       \n""output.value""\n:\n \n""How are you?""\n }\n,\n\n\n   \n""events""\n:\n []\n,\n\n\n   \n""links""\n:\n []\n,\n\n\n   \n""resource""\n:\n {\n\n\n       \n""attributes""\n:\n {}\n,\n\n\n       \n""schema_url""\n:\n \n""""\n\n\n   }\n\n\n}\nSpans can be nested, as is implied by the presence of a parent span ID: child spans represent sub-operations. This allows spans to more accurately capture the work done in an application.\nTraces\nA trace records the paths taken by requests (made by an application or end-user) as they propagate through multiple steps.\nWithout tracing, it is challenging to pinpoint the cause of performance problems in a system.\nIt improves the visibility of our application or system’s health and lets us debug behavior that is difficult to reproduce locally. Tracing is essential for LLM applications, which commonly have nondeterministic problems or are too complicated to reproduce locally.\nTracing makes debugging and understanding LLM applications less daunting by breaking down what happens within a request as it flows through a system.\nA trace is made of one or more spans. The first span represents the root span. Each root span represents a request from start to finish. The spans underneath the parent provide a more in-depth context of what occurs during a request (or what steps make up a request).\nProjects\nA \nproject\n is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces. Projects can be useful for various use-cases such as separating out environments, logging traces for evaluation runs, etc. To learn more about how to setup projects,  see the \nhow-to guide.\nSpan Kind\nWhen a span is created,  it is created as one of the following: Chain, Retriever, Reranker, LLM, Embedding, Agent, or Tool. \nCHAIN\nA Chain is a starting point or a link between different LLM application steps. For example, a Chain span could be used to represent the beginning of a request to an LLM application or the glue code that passes context from a retriever to and LLM call.\nRETRIEVER\nA Retriever is a span that represents a data retrieval step. For example, a Retriever span could be used to represent a call to a vector store or a database.\nRERANKER\nA Reranker is a span that represents the reranking of a set of input documents. For example, a cross-encoder may be used to compute the input documents\' relevance scores with respect to a user query, and the top K documents with the highest scores are then returned by the Reranker.\nLLM\nAn LLM is a span that represents a call to an LLM. For example, an LLM span could be used to represent a call to OpenAI or Llama.\nEMBEDDING\nAn Embedding is a span that represents a call to an LLM for an embedding. For example, an Embedding span could be used to represent a call OpenAI to get an ada-2 embedding for retrieval.\nTOOL\nA Tool is a span that represents a call to an external tool such as a calculator or a weather API.\nAGENT\nA span that encompasses calls to LLMs and Tools. An agent describes a reasoning block that acts on tools using the guidance of an LLM.\n\nSpan Attributes\nAttributes are key-value pairs that contain metadata that you can use to annotate a span to carry information about the operation it is tracking.\nFor example, if a span invokes an LLM, you can capture the model name, the invocation parameters, the token count, and so on.\nAttributes have the following rules:\nKeys must be non-null string values\nValues must be a non-null string, boolean, floating point value, integer, or an array of these values Additionally, there are Semantic Attributes, which are known naming conventions for metadata that is typically present in common operations. It\'s helpful to use semantic attribute naming wherever possible so that common kinds of metadata are standardized across systems. See \nsemantic conventions\n for more information.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What happens if I send the same traces twice? \nAnswer: '}, {'role': 'assistant', 'content': 'Tracing allows you to record the paths taken by requests as they move through different steps. Sending the same traces twice can result in duplicate records of the paths taken by those requests, which may lead to redundant or overlapping information in the trace data.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6Mw==,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix's tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you'll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they're geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix's search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix's flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix's evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n\nsource: https://docs.arize.com/phoenix/evaluation/evaluation-models\ntitle: Evaluation Models\n\nEvaluation Models\narize-phoenix-evals\n supports a large set of foundation models for Evals such as:\nOpenAI\nVertex AI\nAzure Open AI\nAnthropic\nMixtral/Mistral\nAWS Bedrock\nFalcon\nCode Llama\nLlama3\nDeepseek\nDeberta\nDBRX\nQwen\nAnd many more.\nThere are direct model integrations in Phoenix and indirect model integrations (e.x. local modals) through \nLiteLLM\n.\nDirect Integrations:\nThese integrations are native to the Phoenix Evals package and have better throughput, rate limit and error management.\nVertex AI\nOpenAI\nAzure OpenAI\nAnthropic\nMistral\nPrevious\nOnline Evals\nNext\nOverview: Datasets\nLast updated \n7 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Which frameworks and LLM providers are supported by Arize Phoenix for seamless integration?\nAnswer: ""}, {'role': 'assistant', 'content': 'Arize Phoenix supports various frameworks and LLM providers for seamless integration, including OpenAI, Vertex AI, Azure Open AI, Anthropic, Mixtral/Mistral, AWS Bedrock, Falcon, Code Llama, Llama3, Deepseek, Deberta, DBRX, and Qwen.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix's tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you'll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they're geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix's search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix's flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix's evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n\nsource: https://docs.arize.com/phoenix/evaluation/evaluation-models\ntitle: Evaluation Models\n\nEvaluation Models\narize-phoenix-evals\n supports a large set of foundation models for Evals such as:\nOpenAI\nVertex AI\nAzure Open AI\nAnthropic\nMixtral/Mistral\nAWS Bedrock\nFalcon\nCode Llama\nLlama3\nDeepseek\nDeberta\nDBRX\nQwen\nAnd many more.\nThere are direct model integrations in Phoenix and indirect model integrations (e.x. local modals) through \nLiteLLM\n.\nDirect Integrations:\nThese integrations are native to the Phoenix Evals package and have better throughput, rate limit and error management.\nVertex AI\nOpenAI\nAzure OpenAI\nAnthropic\nMistral\nPrevious\nOnline Evals\nNext\nOverview: Datasets\nLast updated \n7 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Which frameworks and LLM providers are supported by Arize Phoenix for seamless integration?\nAnswer: ""}, {'role': 'assistant', 'content': 'Arize Phoenix supports various frameworks and LLM providers for seamless integration, including OpenAI, Vertex AI, Azure Open AI, Anthropic, Mixtral/Mistral, AWS Bedrock, Falcon, Code Llama, Llama3, Deepseek, Deberta, DBRX, and Qwen.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NA==,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/integrations/arize/bring-production-data-to-notebook-for-eda-or-retraining\ntitle: Export Data from Arize to Phoenix\n\nExport Data from Arize to Phoenix\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nOftentimes, the team that notices an issue in their model, for example a prompt/response LLM model, may not be the same team that continues the investigations or kicks off retraining workflows.\nTo help connect teams and workflows, Phoenix enables continued analysis of production data from \nArize\n in a notebook environment for fine tuning workflows.\nFor example, a user may have noticed in \nArize\n that this prompt template is not performing well.\nWith a few lines of Python code, users can export this data into Phoenix for further analysis. This allows team members, such as data scientists, who may not have access to production data today, an easy way to access relevant product data for further analysis in an environment they are familiar with.\nThey can then easily augment and fine tune the data and verify improved performance, before deploying back to production.\nThere are two ways export data out of \nArize\n for further investigation:\nThe easiest way is to click the export button on the Embeddings and Inferences pages. This will produce a code snippet that you can copy into a Python environment and install Phoenix. This code snippet will include the date range you have selected in the \nArize\n platform, in addition to the inferences you have selected.\nUsers can also query \nArize\n for data directly using the Arize Python export client. We recommend doing this once you're more comfortable with the in-platform export functionality, as you will need to manually enter in the data ranges and data you want to export.\nCopy\nos\n.\nenviron\n[\n'ARIZE_API_KEY'\n]\n \n=\n ARIZE_API_KEY\n\n\n\n\nfrom\n datetime \nimport\n datetime\n\n\n\n\nfrom\n arize\n.\nexporter \nimport\n ArizeExportClient\n\n\nfrom\n arize\n.\nutils\n.\ntypes \nimport\n Environments\n\n\n\n\nclient \n=\n \nArizeExportClient\n()\n\n\n\n\nprimary_df \n=\n client\n.\nexport_model_to_df\n(\n\n\n    space_id\n=\n'U3BhY2U6NzU0'\n,\n\n\n    model_name\n=\n'test_home_prices_LLM'\n,\n\n\n    environment\n=\nEnvironments.PRODUCTION,\n\n\n    start_time\n=\ndatetime.\nfromisoformat\n(\n'2023-02-11T07:00:00.000+00:00'\n),\n\n\n    end_time\n=\ndatetime.\nfromisoformat\n(\n'2023-03-14T00:59:59.999+00:00'\n),\n\n\n)\nTest out this workflow by signing up for a \nfree Arize account\n.\nPrevious\nArize\nNext\nRagas\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix's tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you'll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they're geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix's search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix's flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix's evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can users create and manage datasets for prompt testing and fine-tuning in Arize Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'Users can create and manage datasets for prompt testing and fine-tuning in Arize Phoenix by utilizing the tools provided for data exploration, cleaning, and labeling. This enables teams to curate representative data covering a wide range of use cases and edge conditions, facilitating effective testing and fine-tuning of prompts within the platform.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/integrations/arize/bring-production-data-to-notebook-for-eda-or-retraining\ntitle: Export Data from Arize to Phoenix\n\nExport Data from Arize to Phoenix\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nOftentimes, the team that notices an issue in their model, for example a prompt/response LLM model, may not be the same team that continues the investigations or kicks off retraining workflows.\nTo help connect teams and workflows, Phoenix enables continued analysis of production data from \nArize\n in a notebook environment for fine tuning workflows.\nFor example, a user may have noticed in \nArize\n that this prompt template is not performing well.\nWith a few lines of Python code, users can export this data into Phoenix for further analysis. This allows team members, such as data scientists, who may not have access to production data today, an easy way to access relevant product data for further analysis in an environment they are familiar with.\nThey can then easily augment and fine tune the data and verify improved performance, before deploying back to production.\nThere are two ways export data out of \nArize\n for further investigation:\nThe easiest way is to click the export button on the Embeddings and Inferences pages. This will produce a code snippet that you can copy into a Python environment and install Phoenix. This code snippet will include the date range you have selected in the \nArize\n platform, in addition to the inferences you have selected.\nUsers can also query \nArize\n for data directly using the Arize Python export client. We recommend doing this once you're more comfortable with the in-platform export functionality, as you will need to manually enter in the data ranges and data you want to export.\nCopy\nos\n.\nenviron\n[\n'ARIZE_API_KEY'\n]\n \n=\n ARIZE_API_KEY\n\n\n\n\nfrom\n datetime \nimport\n datetime\n\n\n\n\nfrom\n arize\n.\nexporter \nimport\n ArizeExportClient\n\n\nfrom\n arize\n.\nutils\n.\ntypes \nimport\n Environments\n\n\n\n\nclient \n=\n \nArizeExportClient\n()\n\n\n\n\nprimary_df \n=\n client\n.\nexport_model_to_df\n(\n\n\n    space_id\n=\n'U3BhY2U6NzU0'\n,\n\n\n    model_name\n=\n'test_home_prices_LLM'\n,\n\n\n    environment\n=\nEnvironments.PRODUCTION,\n\n\n    start_time\n=\ndatetime.\nfromisoformat\n(\n'2023-02-11T07:00:00.000+00:00'\n),\n\n\n    end_time\n=\ndatetime.\nfromisoformat\n(\n'2023-03-14T00:59:59.999+00:00'\n),\n\n\n)\nTest out this workflow by signing up for a \nfree Arize account\n.\nPrevious\nArize\nNext\nRagas\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix's tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you'll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they're geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix's search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix's flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix's evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can users create and manage datasets for prompt testing and fine-tuning in Arize Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'Users can create and manage datasets for prompt testing and fine-tuning in Arize Phoenix by utilizing the tools provided for data exploration, cleaning, and labeling. This enables teams to curate representative data covering a wide range of use cases and edge conditions, facilitating effective testing and fine-tuning of prompts within the platform.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NQ==,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing\ntitle: Integrations: Tracing\n\nIntegrations: Tracing\nPhoenix natively works with a variety of frameworks and SDKs across Python and JavaScript via OpenTelemetry\n auto-instrumentation\n.\nPython\nLibrary\nInstrumentation Package\nopeninference-instrumentation-llama-index\nopeninference-instrumentation-langchain\nopeninference-instrumentation-openai\nMistralAI\n\n\nopeninference-instrumentation-mistralai\nVertexAI\n\n\nopeninference-instrumentation-vertexai\nopeninference-instrumentation-dspy\nopeninference-instrumentation-bedrock\nopeninference-instrumentation-guardrails\nopeninference-instrumentation-haystack\nopeninference-instrumentation-crewai\nJavascript\nLibrary\nInstrumentation\n@arizeai/openinference-instrumentation-openai\n@arizeai/openinference-instrumentation-langchain\nPrevious\nQuickstart: Tracing\nNext\nOpenAI\nLast updated \n16 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How does Arize Phoenix use OpenTelemetry?\nAnswer: '}, {'role': 'assistant', 'content': 'Arize Phoenix uses OpenTelemetry for instrumentation, allowing it to natively work with a variety of frameworks and SDKs across Python and JavaScript.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing\ntitle: Integrations: Tracing\n\nIntegrations: Tracing\nPhoenix natively works with a variety of frameworks and SDKs across Python and JavaScript via OpenTelemetry\n auto-instrumentation\n.\nPython\nLibrary\nInstrumentation Package\nopeninference-instrumentation-llama-index\nopeninference-instrumentation-langchain\nopeninference-instrumentation-openai\nMistralAI\n\n\nopeninference-instrumentation-mistralai\nVertexAI\n\n\nopeninference-instrumentation-vertexai\nopeninference-instrumentation-dspy\nopeninference-instrumentation-bedrock\nopeninference-instrumentation-guardrails\nopeninference-instrumentation-haystack\nopeninference-instrumentation-crewai\nJavascript\nLibrary\nInstrumentation\n@arizeai/openinference-instrumentation-openai\n@arizeai/openinference-instrumentation-langchain\nPrevious\nQuickstart: Tracing\nNext\nOpenAI\nLast updated \n16 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How does Arize Phoenix use OpenTelemetry?\nAnswer: '}, {'role': 'assistant', 'content': 'Arize Phoenix uses OpenTelemetry for instrumentation, allowing it to natively work with a variety of frameworks and SDKs across Python and JavaScript.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6Ng==,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/inferences/use-cases-inferences/embeddings-analysis\ntitle: Embeddings Analysis\n\nEmbeddings Analysis\nEmbedding Details\nFor each \nembedding\n described in the inference \nschema\n(s), Phoenix serves a embeddings troubleshooting view to help you identify areas of drift and performance degradation. Let's start with embedding drift.\nEmbedding Drift Over Time\nThe picture below shows a time series graph of the drift between two groups of vectors –- the primary (typically production) vectors and reference / baseline vectors. Phoenix uses euclidean distance as the primary measure of embedding drift and helps us identify times where your inference set is diverging from a given reference baseline. \nNote that when you are troubleshooting search and retrieval using \ncorpus\n inferences, the euclidean distance of your queries to your knowledge base vectors is presented as \nquery distance \nEuclidean distance over time\nMoments of high euclidean distance is an indication that the primary inference set is starting to drift from the reference inference set. As the primary inferences move further away from the reference (both in angle and in magnitude), the euclidean distance increases as well. For this reason times of high euclidean distance are a good starting point for trying to identify new anomalies and areas of drift.\nCentroids of the two inferences are used to calculate euclidean and cosine distance\nFor an in-depth guide of euclidean distance and embedding drift, check out\n Arze's ML course \nIn Phoenix, you can views the drift of a particular embedding in a time series graph at the top of the page. To diagnose the cause of the  drift, click on the graph at different times to view a breakdown of the embeddings at particular time.\nClick on a particular time to view why the inference embeddings are drifting\nClusters\nPhoenix automatically breaks up your embeddings into groups of inferences using a clustering algorithm called \nHDBSCAN\n. This is particularly useful if you are trying to identify areas of your embeddings that are drifting or performing badly.\nWhen twos are used to initialize phoenix, the clusters are automatically ordered by drift. This means that clusters that are suffering from the highest amount of under-sampling (more in the primary inferences than the reference) are bubbled to the top. You can click on these clusters to view the details of the points contained in each cluster. \nUMAP Point-Cloud\nPhoenix projects the embeddings you provided into lower dimensional space (3 dimensions) using a dimension reduction algorithm called \nUMAP\n (stands for Uniform Manifold Approximation and Projection).  This lets us understand how your \nembeddings have encoded semantic meaning\n in a visually understandable way.\n\nIn addition to the point-cloud, another dimension we have at our disposal is color (and in some cases shape). Out of the box phoenix let's you assign colors to the UMAP point-cloud by dimension (features, tags, predictions, actuals), performance (correctness which distinguishes true positives and true negatives from the incorrect predictions), and inference (to highlight areas of drift). This helps you explore your point-cloud from different perspectives depending on what you are looking for.\nPrevious\nUse-Cases: Inferences\nNext\nInferences and Schema\nLast updated \n4 months ago\n\nsource: https://docs.arize.com/phoenix/inferences/phoenix-inferences\ntitle: Quickstart: Inferences\n\n📺 To view the Phoenix app in a notebook, run `px.active_session().view()`\n\n\n📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\nYou are now ready to conduct comparative Root Cause Analysis!\n✅\n \nCheckpoint B.\nOptional - try the following exercises to familiarize yourself more with Phoenix:\nClick into \nimage_embedding\n under the Embeddings listing to enter the UMAP projector\nSelect a point on the time series where there is high drift (hint: as given by Euclidean Distance), and see the datapoints from the time selection being rendered below\nWhile colorizing the data by 'Dataset', select the datapoints with the lasso tool where there exists only production data (hint: this is a set of data that has emerged in prod, and is a cause for the increase in drift!)\nExport the selected cluster from Phoenix\nDescribe in words the process you went through to understand increased drift in your production data\nDiscuss your answers in our\n \ncommunity\n!\nStep 7 (Optional): Export data\nOnce you have identified datapoints of interest, you can export this data directly from the Phoenix app for further analysis, or to incorporate these into downstream model retraining and finetuning flows.\nSee more on exporting data \nhere\n.\nStep 8 (Optional): Enable production observability with Arize\nOnce your model is ready for production, you can add Arize to enable production-grade observability. Phoenix works in conjunction with Arize to enable end-to-end model development and observability.\nWith Arize, you will additionally benefit from:\nBeing able to publish and observe your models in real-time as inferences are being served, and/or via direct connectors from your table/storage solution\nScalable compute to handle billions of predictions\nAbility to set up monitors & alerts\nProduction-grade observability\nIntegration with Phoenix for model iteration to observability\nEnterprise-grade RBAC and SSO\nExperiment with infinite permutations of model versions and filters\nCreate your \nfree account\n and see the full suite of \nArize\n features.\nWhere to go from here?\nRead more about Embeddings Analysis \nhere\nQuestions?\nJoin the \nPhoenix Slack community\n to ask questions, share findings, provide feedback, and connect with other developers.\nPrevious\nRetrieval Evals on Document Chunks\nNext\nHow-to: Inferences\nLast updated \n3 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can users visualize embeddings in Arize Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'Users can visualize embeddings in Arize Phoenix by projecting the embeddings into a lower dimensional space (3 dimensions) using a dimension reduction algorithm called UMAP (Uniform Manifold Approximation and Projection). This allows users to understand how their embeddings have encoded semantic meaning in a visually understandable way. Additionally, users can assign colors to the UMAP point-cloud by dimension, performance, and inference to explore the point-cloud from different perspectives.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/inferences/use-cases-inferences/embeddings-analysis\ntitle: Embeddings Analysis\n\nEmbeddings Analysis\nEmbedding Details\nFor each \nembedding\n described in the inference \nschema\n(s), Phoenix serves a embeddings troubleshooting view to help you identify areas of drift and performance degradation. Let's start with embedding drift.\nEmbedding Drift Over Time\nThe picture below shows a time series graph of the drift between two groups of vectors –- the primary (typically production) vectors and reference / baseline vectors. Phoenix uses euclidean distance as the primary measure of embedding drift and helps us identify times where your inference set is diverging from a given reference baseline. \nNote that when you are troubleshooting search and retrieval using \ncorpus\n inferences, the euclidean distance of your queries to your knowledge base vectors is presented as \nquery distance \nEuclidean distance over time\nMoments of high euclidean distance is an indication that the primary inference set is starting to drift from the reference inference set. As the primary inferences move further away from the reference (both in angle and in magnitude), the euclidean distance increases as well. For this reason times of high euclidean distance are a good starting point for trying to identify new anomalies and areas of drift.\nCentroids of the two inferences are used to calculate euclidean and cosine distance\nFor an in-depth guide of euclidean distance and embedding drift, check out\n Arze's ML course \nIn Phoenix, you can views the drift of a particular embedding in a time series graph at the top of the page. To diagnose the cause of the  drift, click on the graph at different times to view a breakdown of the embeddings at particular time.\nClick on a particular time to view why the inference embeddings are drifting\nClusters\nPhoenix automatically breaks up your embeddings into groups of inferences using a clustering algorithm called \nHDBSCAN\n. This is particularly useful if you are trying to identify areas of your embeddings that are drifting or performing badly.\nWhen twos are used to initialize phoenix, the clusters are automatically ordered by drift. This means that clusters that are suffering from the highest amount of under-sampling (more in the primary inferences than the reference) are bubbled to the top. You can click on these clusters to view the details of the points contained in each cluster. \nUMAP Point-Cloud\nPhoenix projects the embeddings you provided into lower dimensional space (3 dimensions) using a dimension reduction algorithm called \nUMAP\n (stands for Uniform Manifold Approximation and Projection).  This lets us understand how your \nembeddings have encoded semantic meaning\n in a visually understandable way.\n\nIn addition to the point-cloud, another dimension we have at our disposal is color (and in some cases shape). Out of the box phoenix let's you assign colors to the UMAP point-cloud by dimension (features, tags, predictions, actuals), performance (correctness which distinguishes true positives and true negatives from the incorrect predictions), and inference (to highlight areas of drift). This helps you explore your point-cloud from different perspectives depending on what you are looking for.\nPrevious\nUse-Cases: Inferences\nNext\nInferences and Schema\nLast updated \n4 months ago\n\nsource: https://docs.arize.com/phoenix/inferences/phoenix-inferences\ntitle: Quickstart: Inferences\n\n📺 To view the Phoenix app in a notebook, run `px.active_session().view()`\n\n\n📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\nYou are now ready to conduct comparative Root Cause Analysis!\n✅\n \nCheckpoint B.\nOptional - try the following exercises to familiarize yourself more with Phoenix:\nClick into \nimage_embedding\n under the Embeddings listing to enter the UMAP projector\nSelect a point on the time series where there is high drift (hint: as given by Euclidean Distance), and see the datapoints from the time selection being rendered below\nWhile colorizing the data by 'Dataset', select the datapoints with the lasso tool where there exists only production data (hint: this is a set of data that has emerged in prod, and is a cause for the increase in drift!)\nExport the selected cluster from Phoenix\nDescribe in words the process you went through to understand increased drift in your production data\nDiscuss your answers in our\n \ncommunity\n!\nStep 7 (Optional): Export data\nOnce you have identified datapoints of interest, you can export this data directly from the Phoenix app for further analysis, or to incorporate these into downstream model retraining and finetuning flows.\nSee more on exporting data \nhere\n.\nStep 8 (Optional): Enable production observability with Arize\nOnce your model is ready for production, you can add Arize to enable production-grade observability. Phoenix works in conjunction with Arize to enable end-to-end model development and observability.\nWith Arize, you will additionally benefit from:\nBeing able to publish and observe your models in real-time as inferences are being served, and/or via direct connectors from your table/storage solution\nScalable compute to handle billions of predictions\nAbility to set up monitors & alerts\nProduction-grade observability\nIntegration with Phoenix for model iteration to observability\nEnterprise-grade RBAC and SSO\nExperiment with infinite permutations of model versions and filters\nCreate your \nfree account\n and see the full suite of \nArize\n features.\nWhere to go from here?\nRead more about Embeddings Analysis \nhere\nQuestions?\nJoin the \nPhoenix Slack community\n to ask questions, share findings, provide feedback, and connect with other developers.\nPrevious\nRetrieval Evals on Document Chunks\nNext\nHow-to: Inferences\nLast updated \n3 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can users visualize embeddings in Arize Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'Users can visualize embeddings in Arize Phoenix by projecting the embeddings into a lower dimensional space (3 dimensions) using a dimension reduction algorithm called UMAP (Uniform Manifold Approximation and Projection). This allows users to understand how their embeddings have encoded semantic meaning in a visually understandable way. Additionally, users can assign colors to the UMAP point-cloud by dimension, performance, and inference to explore the point-cloud from different perspectives.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6Nw==,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n\nsource: https://docs.arize.com/phoenix/integrations/arize\ntitle: Arize\n\nArize\nPhoenix works hand-in-hand with Arize, it's enterprise counterpart\nShare Embeddings Data with Arize\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nPrevious\nModels\nNext\nExport Data from Arize to Phoenix\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the main objectives of Arize Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'The main objectives of Arize Phoenix are to provide an open-source observability library for AI Engineers and Data Scientists to visualize data, evaluate performance, track down issues, and export data to improve their AI models.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n\nsource: https://docs.arize.com/phoenix/integrations/arize\ntitle: Arize\n\nArize\nPhoenix works hand-in-hand with Arize, it's enterprise counterpart\nShare Embeddings Data with Arize\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nPrevious\nModels\nNext\nExport Data from Arize to Phoenix\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the main objectives of Arize Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'The main objectives of Arize Phoenix are to provide an open-source observability library for AI Engineers and Data Scientists to visualize data, evaluate performance, track down issues, and export data to improve their AI models.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6OA==,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/quickstart\ntitle: Quickstart\n\nQuickstart\nThe easiest way to run Phoenix is locally in your own computer. To launch Phoenix, use the following steps.\nInstall\nUsing pip\nUsing conda\nDocker\napp.phoenix.arize.com\nCopy\npip\n \ninstall\n \narize-phoenix\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix\nPhoenix server images are available via \nDocker Hub\n and can be used via \ndocker compose \nor if you simply want a long-running phoenix instance to share with your team.\nCopy\ndocker\n \npull\n \narizephoenix/phoenix:latest\nIf you don\'t want to host an instance of Phoenix yourself or use a notebook instance, you can use a persistent instance provided on our site. Sign up for an Arize Phoenix account at \nhttps://app.phoenix.arize.com/login\nFor more details, see \nHosted Phoenix\nLaunch Phoenix\nLaunching phoenix can be done in many ways depending on your use-case.\nCommand Line\nDocker\nNotebook\napp.phoenix.arize.com\nLaunch your local Phoenix instance using:\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \nserve\nFor details on customizing a local terminal deployment, see \nTerminal Setup\n.\nLaunch the phoenix docker image using:\nCopy\ndocker\n \nrun\n \n-p\n \n6006:6006\n \n-p\n \n4317:4317\n \narizephoenix/phoenix:latest\nThis will expose the Phoenix UI and REST API on \nlocalhost:6006\n and exposes the gRPC endpoint for spans on \nlocalhost:4317\nFor more details on customizing a docker deployment, see \nDocker\nWithin your notebook, launch Phoenix using:\nCopy\nimport\n phoenix \nas\n px\n\n\npx\n.\nlaunch_app\n()\nBy default, notebook instances do not have persistent storage, so your traces will disappear after the notebook is closed. See \nPersistence\n or use one of the other deployment options to retain traces.\nHosted Phoenix instances are always online. Nothing more to do here!\nConnect your App\nTo collect traces from your application, you must configure an OpenTelemetry TracerProvider to send traces to Phoenix. The \nregister\n utility from the \nphoenix.otel\n module streamlines this process.\nPython\nTypeScript\nNotebook\napp.phoenix.arize.com\nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your instance using:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n  \nYou do not have to use phoenix.otel to connect to your phoenix instance, you can use OpenTelemetry itself to initialize  your OTEL connection. See\nUsing OTEL Python Directly\nSee \nSetup Tracing: Python\n for more details on configuration and setup\nFor setting up tracing and OpenTelemetry with TypeScript, see \nSetup Tracing: TS\nConnect your notebook to Phoenix:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n \nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your cloud instance using:\nCopy\nimport\n os\n\n\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# Add Phoenix API Key for tracing\n\n\nos\n.\nenviron\n[\n""OTEL_EXPORTER_OTLP_HEADERS""\n]\n \n=\n f\n""api_key=\n{PHOENIX_API_KEY}\n""\n\n\n\n\n# configure the Phoenix tracer\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""https://app.phoenix.arize.com/v1/traces""\n,\n\n\n)\n \nYour Phoenix API key can be found on the Keys section of your dashboard.\nNext Steps\nTrace\n a running application\nRun \nevaluations\n on traces\nTest changes to you prompts, models, and application via \nexperiments\nPrevious\nArize Phoenix\nNext\nHosted Phoenix\nLast updated \n9 hours ago\n\nsource: https://docs.arize.com/phoenix/deployment/docker\ntitle: Docker\n\nDocker\nThis guide provides instructions for installing and setting up your environment to run Phoenix locally using Docker.\nPrerequisites\nEnsure Docker is installed and running on your system. You can verify this by running:\nCopy\ndocker info\nIf you don\'t see any server information in the output, make sure Docker is installed correctly and launch the Docker daemon.\nPhoenix Version\nOur Docker Compose files are pegged to the latest release of Phoenix. If you want to use a different version, you can specify it in the \ndocker-compose.yml\n file.\nPersistent Disc (optional)\nYou can configure external disc storage to store your data in a SQLite databse\nExternal Postgres(optional).\nyou will need to set the \nPHOENIX_SQL_DATABASE_URL\n environment variable to the connection string for your postgres instance.\nNote: We do only officially support Postgres versions >= 14.\nPostGreSQL\nYou can quickly launch Phoenix with a PostGreSQL backend using docker compose.\nFirst, ensure that Docker Compose is installed on your machine \nhttps://docs.docker.com/compose/install/\n.\nCopy the following YAML file into a new file called \ndocker-compose.yml\nCopy\n# docker-compose.yml\n\n\nservices\n:\n\n\n  \nphoenix\n:\n\n\n    \nimage\n:\n \narizephoenix/phoenix:latest\n \n# Must be greater than 4.0 version to work\n\n\n    \ndepends_on\n:\n\n\n      - \ndb\n\n\n    \nports\n:\n\n\n      - \n6006:6006\n  \n# PHOENIX_PORT\n\n\n      - \n4317:4317\n  \n# PHOENIX_GRPC_PORT\n\n\n      - \n9090:9090\n  \n# [Optional] PROMETHEUS PORT IF ENABLED\n\n\n    \nenvironment\n:\n\n\n      - \nPHOENIX_SQL_DATABASE_URL=postgresql://postgres:postgres@db:5432/postgres\n\n\n  \ndb\n:\n\n\n    \nimage\n:\n \npostgres\n\n\n    \nrestart\n:\n \nalways\n\n\n    \nenvironment\n:\n\n\n      - \nPOSTGRES_USER=postgres\n\n\n      - \nPOSTGRES_PASSWORD=postgres\n\n\n      - \nPOSTGRES_DB=postgres\n\n\n    \nports\n:\n\n\n      - \n5432\n\n\n    \nvolumes\n:\n\n\n      - \ndatabase_data:/var/lib/postgresql/data\n\n\nvolumes\n:\n\n\n  \ndatabase_data\n:\n\n\n    \ndriver\n:\n \nlocal\nRun docker compose to run phoenix with postgres\nCopy\ndocker compose up --build\nNote that the above setup is using your local disc as a volume mount to store the postgres data. For production deployments you will have to setup a persistent volume.\nSQLite\nYou can also run Phonix using SQLite with a persistent disc attached.\nCopy\n# docker-compose.yml\n\n\nservices\n:\n\n\n  \nphoenix\n:\n\n\n    \nimage\n:\n \narizephoenix/phoenix:latest\n \n# Must be greater than 4.0 version to work\n\n\n    \nports\n:\n\n\n      - \n6006:6006\n  \n# PHOENIX_PORT\n\n\n      - \n4317:4317\n  \n# PHOENIX_GRPC_PORT\n\n\n      - \n9090:9090\n  \n# [Optional] PROMETHEUS PORT IF ENABLED\n\n\n    \nenvironment\n:\n\n\n      - \nPHOENIX_WORKING_DIR=/mnt/data\n\n\n    \nvolumes\n:\n\n\n      - \nphoenix_data:/mnt/data\n   \n# PHOENIX_WORKING_DIR\n\n\nvolumes\n:\n\n\n  \nphoenix_data\n:\n\n\n    \ndriver\n:\n \nlocal\nPrevious\nKubernetes\nNext\nFAQs: Deployment\nLast updated \n3 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the steps to set up Arize Phoenix in a local environment?\nAnswer: '}, {'role': 'assistant', 'content': 'To set up Arize Phoenix in a local environment, you can follow these steps:\n1. Install Phoenix using pip or conda, or pull the Phoenix server image from Docker Hub.\n2. Launch Phoenix locally using command line, Docker, or within a notebook.\n3. Customize your deployment based on your use-case and preferences.\n4. Connect your application to Phoenix by configuring an OpenTelemetry TracerProvider.\n5. Optionally, set up persistent storage for your traces to retain them beyond the notebook session.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/quickstart\ntitle: Quickstart\n\nQuickstart\nThe easiest way to run Phoenix is locally in your own computer. To launch Phoenix, use the following steps.\nInstall\nUsing pip\nUsing conda\nDocker\napp.phoenix.arize.com\nCopy\npip\n \ninstall\n \narize-phoenix\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix\nPhoenix server images are available via \nDocker Hub\n and can be used via \ndocker compose \nor if you simply want a long-running phoenix instance to share with your team.\nCopy\ndocker\n \npull\n \narizephoenix/phoenix:latest\nIf you don\'t want to host an instance of Phoenix yourself or use a notebook instance, you can use a persistent instance provided on our site. Sign up for an Arize Phoenix account at \nhttps://app.phoenix.arize.com/login\nFor more details, see \nHosted Phoenix\nLaunch Phoenix\nLaunching phoenix can be done in many ways depending on your use-case.\nCommand Line\nDocker\nNotebook\napp.phoenix.arize.com\nLaunch your local Phoenix instance using:\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \nserve\nFor details on customizing a local terminal deployment, see \nTerminal Setup\n.\nLaunch the phoenix docker image using:\nCopy\ndocker\n \nrun\n \n-p\n \n6006:6006\n \n-p\n \n4317:4317\n \narizephoenix/phoenix:latest\nThis will expose the Phoenix UI and REST API on \nlocalhost:6006\n and exposes the gRPC endpoint for spans on \nlocalhost:4317\nFor more details on customizing a docker deployment, see \nDocker\nWithin your notebook, launch Phoenix using:\nCopy\nimport\n phoenix \nas\n px\n\n\npx\n.\nlaunch_app\n()\nBy default, notebook instances do not have persistent storage, so your traces will disappear after the notebook is closed. See \nPersistence\n or use one of the other deployment options to retain traces.\nHosted Phoenix instances are always online. Nothing more to do here!\nConnect your App\nTo collect traces from your application, you must configure an OpenTelemetry TracerProvider to send traces to Phoenix. The \nregister\n utility from the \nphoenix.otel\n module streamlines this process.\nPython\nTypeScript\nNotebook\napp.phoenix.arize.com\nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your instance using:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n  \nYou do not have to use phoenix.otel to connect to your phoenix instance, you can use OpenTelemetry itself to initialize  your OTEL connection. See\nUsing OTEL Python Directly\nSee \nSetup Tracing: Python\n for more details on configuration and setup\nFor setting up tracing and OpenTelemetry with TypeScript, see \nSetup Tracing: TS\nConnect your notebook to Phoenix:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n \nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your cloud instance using:\nCopy\nimport\n os\n\n\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# Add Phoenix API Key for tracing\n\n\nos\n.\nenviron\n[\n""OTEL_EXPORTER_OTLP_HEADERS""\n]\n \n=\n f\n""api_key=\n{PHOENIX_API_KEY}\n""\n\n\n\n\n# configure the Phoenix tracer\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""https://app.phoenix.arize.com/v1/traces""\n,\n\n\n)\n \nYour Phoenix API key can be found on the Keys section of your dashboard.\nNext Steps\nTrace\n a running application\nRun \nevaluations\n on traces\nTest changes to you prompts, models, and application via \nexperiments\nPrevious\nArize Phoenix\nNext\nHosted Phoenix\nLast updated \n9 hours ago\n\nsource: https://docs.arize.com/phoenix/deployment/docker\ntitle: Docker\n\nDocker\nThis guide provides instructions for installing and setting up your environment to run Phoenix locally using Docker.\nPrerequisites\nEnsure Docker is installed and running on your system. You can verify this by running:\nCopy\ndocker info\nIf you don\'t see any server information in the output, make sure Docker is installed correctly and launch the Docker daemon.\nPhoenix Version\nOur Docker Compose files are pegged to the latest release of Phoenix. If you want to use a different version, you can specify it in the \ndocker-compose.yml\n file.\nPersistent Disc (optional)\nYou can configure external disc storage to store your data in a SQLite databse\nExternal Postgres(optional).\nyou will need to set the \nPHOENIX_SQL_DATABASE_URL\n environment variable to the connection string for your postgres instance.\nNote: We do only officially support Postgres versions >= 14.\nPostGreSQL\nYou can quickly launch Phoenix with a PostGreSQL backend using docker compose.\nFirst, ensure that Docker Compose is installed on your machine \nhttps://docs.docker.com/compose/install/\n.\nCopy the following YAML file into a new file called \ndocker-compose.yml\nCopy\n# docker-compose.yml\n\n\nservices\n:\n\n\n  \nphoenix\n:\n\n\n    \nimage\n:\n \narizephoenix/phoenix:latest\n \n# Must be greater than 4.0 version to work\n\n\n    \ndepends_on\n:\n\n\n      - \ndb\n\n\n    \nports\n:\n\n\n      - \n6006:6006\n  \n# PHOENIX_PORT\n\n\n      - \n4317:4317\n  \n# PHOENIX_GRPC_PORT\n\n\n      - \n9090:9090\n  \n# [Optional] PROMETHEUS PORT IF ENABLED\n\n\n    \nenvironment\n:\n\n\n      - \nPHOENIX_SQL_DATABASE_URL=postgresql://postgres:postgres@db:5432/postgres\n\n\n  \ndb\n:\n\n\n    \nimage\n:\n \npostgres\n\n\n    \nrestart\n:\n \nalways\n\n\n    \nenvironment\n:\n\n\n      - \nPOSTGRES_USER=postgres\n\n\n      - \nPOSTGRES_PASSWORD=postgres\n\n\n      - \nPOSTGRES_DB=postgres\n\n\n    \nports\n:\n\n\n      - \n5432\n\n\n    \nvolumes\n:\n\n\n      - \ndatabase_data:/var/lib/postgresql/data\n\n\nvolumes\n:\n\n\n  \ndatabase_data\n:\n\n\n    \ndriver\n:\n \nlocal\nRun docker compose to run phoenix with postgres\nCopy\ndocker compose up --build\nNote that the above setup is using your local disc as a volume mount to store the postgres data. For production deployments you will have to setup a persistent volume.\nSQLite\nYou can also run Phonix using SQLite with a persistent disc attached.\nCopy\n# docker-compose.yml\n\n\nservices\n:\n\n\n  \nphoenix\n:\n\n\n    \nimage\n:\n \narizephoenix/phoenix:latest\n \n# Must be greater than 4.0 version to work\n\n\n    \nports\n:\n\n\n      - \n6006:6006\n  \n# PHOENIX_PORT\n\n\n      - \n4317:4317\n  \n# PHOENIX_GRPC_PORT\n\n\n      - \n9090:9090\n  \n# [Optional] PROMETHEUS PORT IF ENABLED\n\n\n    \nenvironment\n:\n\n\n      - \nPHOENIX_WORKING_DIR=/mnt/data\n\n\n    \nvolumes\n:\n\n\n      - \nphoenix_data:/mnt/data\n   \n# PHOENIX_WORKING_DIR\n\n\nvolumes\n:\n\n\n  \nphoenix_data\n:\n\n\n    \ndriver\n:\n \nlocal\nPrevious\nKubernetes\nNext\nFAQs: Deployment\nLast updated \n3 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the steps to set up Arize Phoenix in a local environment?\nAnswer: '}, {'role': 'assistant', 'content': 'To set up Arize Phoenix in a local environment, you can follow these steps:\n1. Install Phoenix using pip or conda, or pull the Phoenix server image from Docker Hub.\n2. Launch Phoenix locally using command line, Docker, or within a notebook.\n3. Customize your deployment based on your use-case and preferences.\n4. Connect your application to Phoenix by configuring an OpenTelemetry TracerProvider.\n5. Optionally, set up persistent storage for your traces to retain them beyond the notebook session.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6OQ==,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix\'s tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you\'ll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they\'re geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix\'s search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix\'s flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix\'s evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n\nsource: https://docs.arize.com/phoenix/tracing/llm-traces-1\ntitle: Quickstart: Tracing\n\nQuickstart: Tracing\nInspect the inner-workings of your LLM Application using OpenInference Traces\nOverview\nTracing is a powerful tool for understanding the behavior of your LLM application. Phoenix has best-in-class tracing, irregardless of what framework you use and has first-class instrumentation for a variety of frameworks ( \nLlamaIndex\n, \nLangChain\n,\n DSPy\n),  SDKs (\nOpenAI\n, \nBedrock\n, \nMistral\n, \nVertex\n), and Languages (Python, Javascript). You can also \nmanually instrument\n your application using the OpenTelemetry SDK.\nTo get started with traces, you will first want to start a local Phoenix app. Below we will explore how to use Phoenix in a notebook but you can \ndeploy phoenix \n once you are ready for a persistent observability platform.\nIn your Jupyter or Colab environment, run the following command to install.\nUsing pip\nUsing conda\nCopy\npip\n \ninstall\n \narize-phoenix\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix\nTo get started, launch the phoenix app.\nCopy\nimport\n phoenix \nas\n px\n\n\nsession \n=\n px\n.\nlaunch_app\n()\nThe above launches a Phoenix server that acts as a trace collector for any LLM application running locally in your jupyter notebook!\nCopy\n🌍 To view the Phoenix app in your browser, visit https://z8rwookkcle1-496ff2e9c6d22116-6060-colab.googleusercontent.com/\n\n\n📺 To view the Phoenix app in a notebook, run `px.active_session().view()`\n\n\n📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\nThe \nlaunch_app\n command will spit out a URL for you to view the Phoenix UI. You can access this url again at any time via the \nsession\n.\n\nNow that phoenix is up and running, you can setup tracing for your AI application so that you can debug your application as the traces stream in.\nLlamaIndex\nLangChain\nOpenAI\nTo use llama-index\'s one click, you must install the small integration first:\nCopy\npip\n \ninstall\n \n\'llama-index>=0.10.44\'\nCopy\nimport\n phoenix \nas\n px\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\nimport\n os\n\n\nfrom\n gcsfs \nimport\n GCSFileSystem\n\n\nfrom\n llama_index\n.\ncore \nimport\n (\n\n\n    Settings\n,\n\n\n    VectorStoreIndex\n,\n\n\n    StorageContext\n,\n\n\n    set_global_handler\n,\n\n\n    load_index_from_storage\n\n\n)\n\n\nfrom\n llama_index\n.\nembeddings\n.\nopenai \nimport\n OpenAIEmbedding\n\n\nfrom\n llama_index\n.\nllms\n.\nopenai \nimport\n OpenAI\n\n\nimport\n llama_index\n\n\n\n\n# To view traces in Phoenix, you will first have to start a Phoenix server. You can do this by running the following:\n\n\nsession \n=\n px\n.\nlaunch_app\n()\n\n\n\n\n# Initialize LlamaIndex auto-instrumentation\n\n\nLlamaIndexInstrumentor\n().\ninstrument\n()\n\n\n\n\nos\n.\nenviron\n[\n""OPENAI_API_KEY""\n]\n \n=\n \n""<ENTER_YOUR_OPENAI_API_KEY_HERE>""\n\n\n\n\n# LlamaIndex application initialization may vary\n\n\n# depending on your application\n\n\nSettings\n.\nllm \n=\n \nOpenAI\n(model\n=\n""gpt-4-turbo-preview""\n)\n\n\nSettings\n.\nembed_model \n=\n \nOpenAIEmbedding\n(model\n=\n""text-embedding-ada-002""\n)\n\n\n\n\n\n\n# Load your data and create an index. Here we\'ve provided an example of our documentation\n\n\nfile_system \n=\n \nGCSFileSystem\n(project\n=\n""public-assets-275721""\n)\n\n\nindex_path \n=\n \n""arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/index/""\n\n\nstorage_context \n=\n StorageContext\n.\nfrom_defaults\n(\n\n\n    fs\n=\nfile_system,\n\n\n    persist_dir\n=\nindex_path,\n\n\n)\n\n\n\n\nindex \n=\n \nload_index_from_storage\n(storage_context)\n\n\n\n\nquery_engine \n=\n index\n.\nas_query_engine\n()\n\n\n\n\n# Query your LlamaIndex application\n\n\nquery_engine\n.\nquery\n(\n""What is the meaning of life?""\n)\n\n\nquery_engine\n.\nquery\n(\n""How can I deploy Arize?""\n)\n\n\n\n\n# View the traces in the Phoenix UI\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What types of traces can be collected using Arize Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Tracing and span analysis capabilities can be used to collect traces for debugging in Phoenix.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix\'s tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you\'ll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they\'re geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix\'s search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix\'s flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix\'s evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n\nsource: https://docs.arize.com/phoenix/tracing/llm-traces-1\ntitle: Quickstart: Tracing\n\nQuickstart: Tracing\nInspect the inner-workings of your LLM Application using OpenInference Traces\nOverview\nTracing is a powerful tool for understanding the behavior of your LLM application. Phoenix has best-in-class tracing, irregardless of what framework you use and has first-class instrumentation for a variety of frameworks ( \nLlamaIndex\n, \nLangChain\n,\n DSPy\n),  SDKs (\nOpenAI\n, \nBedrock\n, \nMistral\n, \nVertex\n), and Languages (Python, Javascript). You can also \nmanually instrument\n your application using the OpenTelemetry SDK.\nTo get started with traces, you will first want to start a local Phoenix app. Below we will explore how to use Phoenix in a notebook but you can \ndeploy phoenix \n once you are ready for a persistent observability platform.\nIn your Jupyter or Colab environment, run the following command to install.\nUsing pip\nUsing conda\nCopy\npip\n \ninstall\n \narize-phoenix\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix\nTo get started, launch the phoenix app.\nCopy\nimport\n phoenix \nas\n px\n\n\nsession \n=\n px\n.\nlaunch_app\n()\nThe above launches a Phoenix server that acts as a trace collector for any LLM application running locally in your jupyter notebook!\nCopy\n🌍 To view the Phoenix app in your browser, visit https://z8rwookkcle1-496ff2e9c6d22116-6060-colab.googleusercontent.com/\n\n\n📺 To view the Phoenix app in a notebook, run `px.active_session().view()`\n\n\n📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\nThe \nlaunch_app\n command will spit out a URL for you to view the Phoenix UI. You can access this url again at any time via the \nsession\n.\n\nNow that phoenix is up and running, you can setup tracing for your AI application so that you can debug your application as the traces stream in.\nLlamaIndex\nLangChain\nOpenAI\nTo use llama-index\'s one click, you must install the small integration first:\nCopy\npip\n \ninstall\n \n\'llama-index>=0.10.44\'\nCopy\nimport\n phoenix \nas\n px\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\nimport\n os\n\n\nfrom\n gcsfs \nimport\n GCSFileSystem\n\n\nfrom\n llama_index\n.\ncore \nimport\n (\n\n\n    Settings\n,\n\n\n    VectorStoreIndex\n,\n\n\n    StorageContext\n,\n\n\n    set_global_handler\n,\n\n\n    load_index_from_storage\n\n\n)\n\n\nfrom\n llama_index\n.\nembeddings\n.\nopenai \nimport\n OpenAIEmbedding\n\n\nfrom\n llama_index\n.\nllms\n.\nopenai \nimport\n OpenAI\n\n\nimport\n llama_index\n\n\n\n\n# To view traces in Phoenix, you will first have to start a Phoenix server. You can do this by running the following:\n\n\nsession \n=\n px\n.\nlaunch_app\n()\n\n\n\n\n# Initialize LlamaIndex auto-instrumentation\n\n\nLlamaIndexInstrumentor\n().\ninstrument\n()\n\n\n\n\nos\n.\nenviron\n[\n""OPENAI_API_KEY""\n]\n \n=\n \n""<ENTER_YOUR_OPENAI_API_KEY_HERE>""\n\n\n\n\n# LlamaIndex application initialization may vary\n\n\n# depending on your application\n\n\nSettings\n.\nllm \n=\n \nOpenAI\n(model\n=\n""gpt-4-turbo-preview""\n)\n\n\nSettings\n.\nembed_model \n=\n \nOpenAIEmbedding\n(model\n=\n""text-embedding-ada-002""\n)\n\n\n\n\n\n\n# Load your data and create an index. Here we\'ve provided an example of our documentation\n\n\nfile_system \n=\n \nGCSFileSystem\n(project\n=\n""public-assets-275721""\n)\n\n\nindex_path \n=\n \n""arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/index/""\n\n\nstorage_context \n=\n StorageContext\n.\nfrom_defaults\n(\n\n\n    fs\n=\nfile_system,\n\n\n    persist_dir\n=\nindex_path,\n\n\n)\n\n\n\n\nindex \n=\n \nload_index_from_storage\n(storage_context)\n\n\n\n\nquery_engine \n=\n index\n.\nas_query_engine\n()\n\n\n\n\n# Query your LlamaIndex application\n\n\nquery_engine\n.\nquery\n(\n""What is the meaning of life?""\n)\n\n\nquery_engine\n.\nquery\n(\n""How can I deploy Arize?""\n)\n\n\n\n\n# View the traces in the Phoenix UI\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What types of traces can be collected using Arize Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Tracing and span analysis capabilities can be used to collect traces for debugging in Phoenix.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MTA=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix's tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you'll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they're geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix's search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix's flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix's evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What can I expect to see in an Arize Phoenix trace?\nAnswer: ""}, {'role': 'assistant', 'content': 'You can expect to see detailed insights into the execution flow, specific spans, performance metrics, and access to relevant logs and metadata in an Arize Phoenix trace.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix's tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you'll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they're geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix's search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix's flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix's evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What can I expect to see in an Arize Phoenix trace?\nAnswer: ""}, {'role': 'assistant', 'content': 'You can expect to see detailed insights into the execution flow, specific spans, performance metrics, and access to relevant logs and metadata in an Arize Phoenix trace.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MTE=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nNot more than 4-1/4 inches high, or more than 6 inches long, or greater than \\n0.016 inch thick.\\nd. Not more than 3.5 ounces (Charge flat-size prices for First-Class Mail \\ncard-type pieces over 3.5 ounces.)\\n\\n  Instruction: Based on the above documents, provide a detailed answer for the user question below.\\n  Answer \\""don\'t know\\"" if not present in the document.\\n  "",\n\n\n       \n""llm.input_messages.1.message.role""\n:\n \n""user""\n,\n\n\n       \n""llm.input_messages.1.message.content""\n:\n \n""Hello""\n,\n\n\n       \n""llm.model_name""\n:\n \n""gpt-4-turbo-preview""\n,\n\n\n       \n""llm.invocation_parameters""\n:\n \n""{\\""temperature\\"": 0.1, \\""model\\"": \\""gpt-4-turbo-preview\\""}""\n,\n\n\n       \n""output.value""\n:\n \n""How are you?""\n }\n,\n\n\n   \n""events""\n:\n []\n,\n\n\n   \n""links""\n:\n []\n,\n\n\n   \n""resource""\n:\n {\n\n\n       \n""attributes""\n:\n {}\n,\n\n\n       \n""schema_url""\n:\n \n""""\n\n\n   }\n\n\n}\nSpans can be nested, as is implied by the presence of a parent span ID: child spans represent sub-operations. This allows spans to more accurately capture the work done in an application.\nTraces\nA trace records the paths taken by requests (made by an application or end-user) as they propagate through multiple steps.\nWithout tracing, it is challenging to pinpoint the cause of performance problems in a system.\nIt improves the visibility of our application or system’s health and lets us debug behavior that is difficult to reproduce locally. Tracing is essential for LLM applications, which commonly have nondeterministic problems or are too complicated to reproduce locally.\nTracing makes debugging and understanding LLM applications less daunting by breaking down what happens within a request as it flows through a system.\nA trace is made of one or more spans. The first span represents the root span. Each root span represents a request from start to finish. The spans underneath the parent provide a more in-depth context of what occurs during a request (or what steps make up a request).\nProjects\nA \nproject\n is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces. Projects can be useful for various use-cases such as separating out environments, logging traces for evaluation runs, etc. To learn more about how to setup projects,  see the \nhow-to guide.\nSpan Kind\nWhen a span is created,  it is created as one of the following: Chain, Retriever, Reranker, LLM, Embedding, Agent, or Tool. \nCHAIN\nA Chain is a starting point or a link between different LLM application steps. For example, a Chain span could be used to represent the beginning of a request to an LLM application or the glue code that passes context from a retriever to and LLM call.\nRETRIEVER\nA Retriever is a span that represents a data retrieval step. For example, a Retriever span could be used to represent a call to a vector store or a database.\nRERANKER\nA Reranker is a span that represents the reranking of a set of input documents. For example, a cross-encoder may be used to compute the input documents\' relevance scores with respect to a user query, and the top K documents with the highest scores are then returned by the Reranker.\nLLM\nAn LLM is a span that represents a call to an LLM. For example, an LLM span could be used to represent a call to OpenAI or Llama.\nEMBEDDING\nAn Embedding is a span that represents a call to an LLM for an embedding. For example, an Embedding span could be used to represent a call OpenAI to get an ada-2 embedding for retrieval.\nTOOL\nA Tool is a span that represents a call to an external tool such as a calculator or a weather API.\nAGENT\nA span that encompasses calls to LLMs and Tools. An agent describes a reasoning block that acts on tools using the guidance of an LLM.\n\nSpan Attributes\nAttributes are key-value pairs that contain metadata that you can use to annotate a span to carry information about the operation it is tracking.\nFor example, if a span invokes an LLM, you can capture the model name, the invocation parameters, the token count, and so on.\nAttributes have the following rules:\nKeys must be non-null string values\nValues must be a non-null string, boolean, floating point value, integer, or an array of these values Additionally, there are Semantic Attributes, which are known naming conventions for metadata that is typically present in common operations. It\'s helpful to use semantic attribute naming wherever possible so that common kinds of metadata are standardized across systems. See \nsemantic conventions\n for more information.\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/manual-instrumentation/javascript\ntitle: Instrument: TS\n\n// ...\n\n\n\n\ntracer\n.startActiveSpan\n(\n\'app.doWork\'\n,\n (span) \n=>\n {\n\n\n  \nfor\n (\nlet\n i \n=\n \n0\n; i \n<=\n \nMath\n.floor\n(\nMath\n.random\n() \n*\n \n40000000\n); i \n+=\n \n1\n) {\n\n\n    \nif\n (i \n>\n \n10000\n) {\n\n\n      \nspan\n.setStatus\n({\n\n\n        code\n:\n \nSpanStatusCode\n.\nERROR\n,\n\n\n        message\n:\n \n\'Error\'\n,\n\n\n      });\n\n\n    }\n\n\n  }\n\n\n\n\n  \nspan\n.end\n();\n\n\n});\nRecording exceptions\nIt can be a good idea to record exceptions when they happen. It’s recommended to do this in conjunction with setting \nspan status\n.\nCopy\nimport\n opentelemetry\n,\n { SpanStatusCode } \nfrom\n \n\'@opentelemetry/api\'\n;\n\n\n\n\n// ...\n\n\n\n\ntry\n {\n\n\n  \ndoWork\n();\n\n\n} \ncatch\n (ex) {\n\n\n  \nspan\n.recordException\n(ex);\n\n\n  \nspan\n.setStatus\n({ code\n:\n \nSpanStatusCode\n.\nERROR\n });\n\n\n}\nUsing \nsdk-trace-base\n and manually propagating span context\nIn some cases, you may not be able to use either the Node.js SDK nor the Web SDK. The biggest difference, aside from initialization code, is that you’ll have to manually set spans as active in the current context to be able to create nested spans.\nInitializing tracing with \nsdk-trace-base\nInitializing tracing is similar to how you’d do it with Node.js or the Web SDK.\nCopy\nimport\n opentelemetry \nfrom\n \n\'@opentelemetry/api\'\n;\n\n\nimport\n {\n\n\n  BasicTracerProvider\n,\n\n\n  BatchSpanProcessor\n,\n\n\n  ConsoleSpanExporter\n,\n\n\n} \nfrom\n \n\'@opentelemetry/sdk-trace-base\'\n;\n\n\n\n\nconst\n \nprovider\n \n=\n \nnew\n \nBasicTracerProvider\n();\n\n\n\n\n// Configure span processor to send spans to the exporter\n\n\nprovider\n.addSpanProcessor\n(\nnew\n \nBatchSpanProcessor\n(\nnew\n \nConsoleSpanExporter\n()));\n\n\nprovider\n.register\n();\n\n\n\n\n// This is what we\'ll access in all instrumentation code\n\n\nconst\n \ntracer\n \n=\n \nopentelemetry\n.\ntrace\n.getTracer\n(\n\'example-basic-tracer-node\'\n);\nLike the other examples in this document, this exports a tracer you can use throughout the app.\nCreating nested spans with \nsdk-trace-base\nTo create nested spans, you need to set whatever the currently-created span is as the active span in the current context. Don’t bother using \nstartActiveSpan\n because it won’t do this for you.\nCopy\nconst\n \nmainWork\n \n=\n () \n=>\n {\n\n\n  \nconst\n \nparentSpan\n \n=\n \ntracer\n.startSpan\n(\n\'main\'\n);\n\n\n\n\n  \nfor\n (\nlet\n i \n=\n \n0\n; i \n<\n \n3\n; i \n+=\n \n1\n) {\n\n\n    \ndoWork\n(parentSpan\n,\n i);\n\n\n  }\n\n\n\n\n  \n// Be sure to end the parent span!\n\n\n  \nparentSpan\n.end\n();\n\n\n};\n\n\n\n\nconst\n \ndoWork\n \n=\n (parent\n,\n i) \n=>\n {\n\n\n  \n// To create a child span, we need to mark the current (parent) span as the active span\n\n\n  \n// in the context, then use the resulting context to create a child span.\n\n\n  \nconst\n \nctx\n \n=\n \nopentelemetry\n.\ntrace\n.setSpan\n(\n\n\n    \nopentelemetry\n.\ncontext\n.active\n()\n,\n\n\n    parent\n,\n\n\n  );\n\n\n  \nconst\n \nspan\n \n=\n \ntracer\n.startSpan\n(\n`doWork:\n${\ni\n}\n`\n,\n \nundefined\n,\n ctx);\n\n\n\n\n  \n// simulate some random work.\n\n\n  \nfor\n (\nlet\n i \n=\n \n0\n; i \n<=\n \nMath\n.floor\n(\nMath\n.random\n() \n*\n \n40000000\n); i \n+=\n \n1\n) {\n\n\n    \n// empty\n\n\n  }\n\n\n\n\n  \n// Make sure to end this child span! If you don\'t,\n\n\n  \n// it will continue to track work beyond \'doWork\'!\n\n\n  \nspan\n.end\n();\n\n\n};\nAll other APIs behave the same when you use \nsdk-trace-base\n compared with the Node.js SDKs.\nPrevious\nInstrumenting Span Types\nNext\nQuerying Spans\nLast updated \n1 month ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the different types of spans?\nAnswer: '}, {'role': 'assistant', 'content': 'The different types of spans mentioned in the provided context are Chain, Retriever, Reranker, LLM, Embedding, Tool, and Agent.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nNot more than 4-1/4 inches high, or more than 6 inches long, or greater than \\n0.016 inch thick.\\nd. Not more than 3.5 ounces (Charge flat-size prices for First-Class Mail \\ncard-type pieces over 3.5 ounces.)\\n\\n  Instruction: Based on the above documents, provide a detailed answer for the user question below.\\n  Answer \\""don\'t know\\"" if not present in the document.\\n  "",\n\n\n       \n""llm.input_messages.1.message.role""\n:\n \n""user""\n,\n\n\n       \n""llm.input_messages.1.message.content""\n:\n \n""Hello""\n,\n\n\n       \n""llm.model_name""\n:\n \n""gpt-4-turbo-preview""\n,\n\n\n       \n""llm.invocation_parameters""\n:\n \n""{\\""temperature\\"": 0.1, \\""model\\"": \\""gpt-4-turbo-preview\\""}""\n,\n\n\n       \n""output.value""\n:\n \n""How are you?""\n }\n,\n\n\n   \n""events""\n:\n []\n,\n\n\n   \n""links""\n:\n []\n,\n\n\n   \n""resource""\n:\n {\n\n\n       \n""attributes""\n:\n {}\n,\n\n\n       \n""schema_url""\n:\n \n""""\n\n\n   }\n\n\n}\nSpans can be nested, as is implied by the presence of a parent span ID: child spans represent sub-operations. This allows spans to more accurately capture the work done in an application.\nTraces\nA trace records the paths taken by requests (made by an application or end-user) as they propagate through multiple steps.\nWithout tracing, it is challenging to pinpoint the cause of performance problems in a system.\nIt improves the visibility of our application or system’s health and lets us debug behavior that is difficult to reproduce locally. Tracing is essential for LLM applications, which commonly have nondeterministic problems or are too complicated to reproduce locally.\nTracing makes debugging and understanding LLM applications less daunting by breaking down what happens within a request as it flows through a system.\nA trace is made of one or more spans. The first span represents the root span. Each root span represents a request from start to finish. The spans underneath the parent provide a more in-depth context of what occurs during a request (or what steps make up a request).\nProjects\nA \nproject\n is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces. Projects can be useful for various use-cases such as separating out environments, logging traces for evaluation runs, etc. To learn more about how to setup projects,  see the \nhow-to guide.\nSpan Kind\nWhen a span is created,  it is created as one of the following: Chain, Retriever, Reranker, LLM, Embedding, Agent, or Tool. \nCHAIN\nA Chain is a starting point or a link between different LLM application steps. For example, a Chain span could be used to represent the beginning of a request to an LLM application or the glue code that passes context from a retriever to and LLM call.\nRETRIEVER\nA Retriever is a span that represents a data retrieval step. For example, a Retriever span could be used to represent a call to a vector store or a database.\nRERANKER\nA Reranker is a span that represents the reranking of a set of input documents. For example, a cross-encoder may be used to compute the input documents\' relevance scores with respect to a user query, and the top K documents with the highest scores are then returned by the Reranker.\nLLM\nAn LLM is a span that represents a call to an LLM. For example, an LLM span could be used to represent a call to OpenAI or Llama.\nEMBEDDING\nAn Embedding is a span that represents a call to an LLM for an embedding. For example, an Embedding span could be used to represent a call OpenAI to get an ada-2 embedding for retrieval.\nTOOL\nA Tool is a span that represents a call to an external tool such as a calculator or a weather API.\nAGENT\nA span that encompasses calls to LLMs and Tools. An agent describes a reasoning block that acts on tools using the guidance of an LLM.\n\nSpan Attributes\nAttributes are key-value pairs that contain metadata that you can use to annotate a span to carry information about the operation it is tracking.\nFor example, if a span invokes an LLM, you can capture the model name, the invocation parameters, the token count, and so on.\nAttributes have the following rules:\nKeys must be non-null string values\nValues must be a non-null string, boolean, floating point value, integer, or an array of these values Additionally, there are Semantic Attributes, which are known naming conventions for metadata that is typically present in common operations. It\'s helpful to use semantic attribute naming wherever possible so that common kinds of metadata are standardized across systems. See \nsemantic conventions\n for more information.\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/manual-instrumentation/javascript\ntitle: Instrument: TS\n\n// ...\n\n\n\n\ntracer\n.startActiveSpan\n(\n\'app.doWork\'\n,\n (span) \n=>\n {\n\n\n  \nfor\n (\nlet\n i \n=\n \n0\n; i \n<=\n \nMath\n.floor\n(\nMath\n.random\n() \n*\n \n40000000\n); i \n+=\n \n1\n) {\n\n\n    \nif\n (i \n>\n \n10000\n) {\n\n\n      \nspan\n.setStatus\n({\n\n\n        code\n:\n \nSpanStatusCode\n.\nERROR\n,\n\n\n        message\n:\n \n\'Error\'\n,\n\n\n      });\n\n\n    }\n\n\n  }\n\n\n\n\n  \nspan\n.end\n();\n\n\n});\nRecording exceptions\nIt can be a good idea to record exceptions when they happen. It’s recommended to do this in conjunction with setting \nspan status\n.\nCopy\nimport\n opentelemetry\n,\n { SpanStatusCode } \nfrom\n \n\'@opentelemetry/api\'\n;\n\n\n\n\n// ...\n\n\n\n\ntry\n {\n\n\n  \ndoWork\n();\n\n\n} \ncatch\n (ex) {\n\n\n  \nspan\n.recordException\n(ex);\n\n\n  \nspan\n.setStatus\n({ code\n:\n \nSpanStatusCode\n.\nERROR\n });\n\n\n}\nUsing \nsdk-trace-base\n and manually propagating span context\nIn some cases, you may not be able to use either the Node.js SDK nor the Web SDK. The biggest difference, aside from initialization code, is that you’ll have to manually set spans as active in the current context to be able to create nested spans.\nInitializing tracing with \nsdk-trace-base\nInitializing tracing is similar to how you’d do it with Node.js or the Web SDK.\nCopy\nimport\n opentelemetry \nfrom\n \n\'@opentelemetry/api\'\n;\n\n\nimport\n {\n\n\n  BasicTracerProvider\n,\n\n\n  BatchSpanProcessor\n,\n\n\n  ConsoleSpanExporter\n,\n\n\n} \nfrom\n \n\'@opentelemetry/sdk-trace-base\'\n;\n\n\n\n\nconst\n \nprovider\n \n=\n \nnew\n \nBasicTracerProvider\n();\n\n\n\n\n// Configure span processor to send spans to the exporter\n\n\nprovider\n.addSpanProcessor\n(\nnew\n \nBatchSpanProcessor\n(\nnew\n \nConsoleSpanExporter\n()));\n\n\nprovider\n.register\n();\n\n\n\n\n// This is what we\'ll access in all instrumentation code\n\n\nconst\n \ntracer\n \n=\n \nopentelemetry\n.\ntrace\n.getTracer\n(\n\'example-basic-tracer-node\'\n);\nLike the other examples in this document, this exports a tracer you can use throughout the app.\nCreating nested spans with \nsdk-trace-base\nTo create nested spans, you need to set whatever the currently-created span is as the active span in the current context. Don’t bother using \nstartActiveSpan\n because it won’t do this for you.\nCopy\nconst\n \nmainWork\n \n=\n () \n=>\n {\n\n\n  \nconst\n \nparentSpan\n \n=\n \ntracer\n.startSpan\n(\n\'main\'\n);\n\n\n\n\n  \nfor\n (\nlet\n i \n=\n \n0\n; i \n<\n \n3\n; i \n+=\n \n1\n) {\n\n\n    \ndoWork\n(parentSpan\n,\n i);\n\n\n  }\n\n\n\n\n  \n// Be sure to end the parent span!\n\n\n  \nparentSpan\n.end\n();\n\n\n};\n\n\n\n\nconst\n \ndoWork\n \n=\n (parent\n,\n i) \n=>\n {\n\n\n  \n// To create a child span, we need to mark the current (parent) span as the active span\n\n\n  \n// in the context, then use the resulting context to create a child span.\n\n\n  \nconst\n \nctx\n \n=\n \nopentelemetry\n.\ntrace\n.setSpan\n(\n\n\n    \nopentelemetry\n.\ncontext\n.active\n()\n,\n\n\n    parent\n,\n\n\n  );\n\n\n  \nconst\n \nspan\n \n=\n \ntracer\n.startSpan\n(\n`doWork:\n${\ni\n}\n`\n,\n \nundefined\n,\n ctx);\n\n\n\n\n  \n// simulate some random work.\n\n\n  \nfor\n (\nlet\n i \n=\n \n0\n; i \n<=\n \nMath\n.floor\n(\nMath\n.random\n() \n*\n \n40000000\n); i \n+=\n \n1\n) {\n\n\n    \n// empty\n\n\n  }\n\n\n\n\n  \n// Make sure to end this child span! If you don\'t,\n\n\n  \n// it will continue to track work beyond \'doWork\'!\n\n\n  \nspan\n.end\n();\n\n\n};\nAll other APIs behave the same when you use \nsdk-trace-base\n compared with the Node.js SDKs.\nPrevious\nInstrumenting Span Types\nNext\nQuerying Spans\nLast updated \n1 month ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the different types of spans?\nAnswer: '}, {'role': 'assistant', 'content': 'The different types of spans mentioned in the provided context are Chain, Retriever, Reranker, LLM, Embedding, Tool, and Agent.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MTI=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/manual-instrumentation/javascript\ntitle: Instrument: TS\n\nimport\n { trace } \nfrom\n \n\'@opentelemetry/api\'\n;\n\n\nimport\n express\n,\n { Express } \nfrom\n \n\'express\'\n;\n\n\nimport\n { OpenAI } \nfrom\n \n""openai""\n;\n\n\n\n\nconst\n \ntracer\n \n=\n \ntrace\n.getTracer\n(\n\'llm-server\'\n,\n \n\'0.1.0\'\n);\n\n\n\n\nconst\n \nPORT\n:\n \nnumber\n \n=\n \nparseInt\n(\nprocess\n.\nenv\n.\nPORT\n \n||\n \n\'8080\'\n);\n\n\nconst\n \napp\n:\n \nExpress\n \n=\n \nexpress\n();\n\n\n\n\nconst\n \nopenai\n \n=\n \nnew\n \nOpenAI\n({\n\n\n  apiKey\n:\n \nprocess\n.\nenv\n.\nOPENAI_API_KEY\n,\n\n\n});\n\n\n\n\napp\n.get\n(\n\'/chat\'\n,\n (req\n,\n res) \n=>\n {\n\n\n  \nconst\n \nmessage\n \n=\n \nreq\n.\nquery\n.message\n\n\n  \nlet\n chatCompletion \n=\n \nawait\n \nopenai\n.\nchat\n.\ncompletions\n.create\n({\n\n\n    messages\n:\n [{ role\n:\n \n""user""\n,\n content\n:\n message }]\n,\n\n\n    model\n:\n \n""gpt-3.5-turbo""\n,\n\n\n  });\n\n\n  \nres\n.send\n(\nchatCompletion\n.choices[\n0\n].message);\n\n\n});\n\n\n\n\napp\n.listen\n(\nPORT\n,\n () \n=>\n {\n\n\n  \nconsole\n.log\n(\n`Listening for requests on http://localhost:\n${\nPORT\n}\n`\n);\n\n\n});\nCreate spans\nNow that you have \ntracers\n initialized, you can create \nspans\n.\nThe API of OpenTelemetry JavaScript exposes two methods that allow you to create spans:\ntracer.startSpan\n: Starts a new span without setting it on context.\ntracer.startActiveSpan\n: Starts a new span and calls the given callback function passing it the created span as first argument. The new span gets set in context and this context is activated for the duration of the function call.\nIn most cases you want to use the latter (\ntracer.startActiveSpan\n), as it takes care of setting the span and its context active.\nThe code below illustrates how to create an active span.\nCopy\nimport\n { trace\n,\n Span } \nfrom\n \n""@opentelemetry/api""\n;\n\n\nimport\n { SpanKind } \nfrom\n \n""@opentelemetry/api""\n;\n\n\nimport\n {\n\n\n    SemanticConventions\n,\n\n\n    OpenInferenceSpanKind\n,\n\n\n} \nfrom\n \n""@arizeai/openinference-semantic-conventions""\n;\n\n\n\n\nexport\n \nfunction\n \nchat\n(message\n:\n \nstring\n) {\n\n\n    \n// Create a span. A span must be closed.\n\n\n    \nreturn\n \ntracer\n.startActiveSpan\n(\n\n\n        \n""chat""\n,\n\n\n        (span\n:\n \nSpan\n) \n=>\n {\n\n\n            \nspan\n.setAttributes\n({\n\n\n                [\nSemanticConventions\n.\nOPENINFERENCE_SPAN_KIND\n]\n:\n \nOpenInferenceSpanKind\n.chain\n,\n\n\n                [\nSemanticConventions\n.\nINPUT_VALUE\n]\n:\n message\n,\n\n\n            });\n\n\n            \nlet\n chatCompletion \n=\n \nawait\n \nopenai\n.\nchat\n.\ncompletions\n.create\n({\n\n\n                messages\n:\n [{ role\n:\n \n""user""\n,\n content\n:\n message }]\n,\n\n\n                model\n:\n \n""gpt-3.5-turbo""\n,\n\n\n            });\n\n\n            \nspan\n.setAttributes\n({\n\n\n                attributes\n:\n {\n\n\n                    [\nSemanticConventions\n.\nOUTPUT_VALUE\n]\n:\n \nchatCompletion\n.choices[\n0\n].message\n,\n\n\n                }\n,\n\n\n            });\n\n\n            \n// Be sure to end the span!\n\n\n            \nspan\n.end\n();\n\n\n            \nreturn\n result;\n\n\n        }\n\n\n    );\n\n\n}\nThe above instrumented code can now be pasted in the \n/chat\n handler. You should now be able to see spans emitted from your app.\nStart your app as follows, and then send it requests by visiting \nhttp://localhost:8080/chat?message=""how long is a pencil""\n with your browser or \ncurl\n.\nCopy\nts-node\n \n--require\n \n./instrumentation.ts\n \napp.ts\nAfter a while, you should see the spans printed in the console by the \nConsoleSpanExporter\n, something like this:\nCopy\n{\n\n\n  \n""traceId""\n:\n \n""6cc927a05e7f573e63f806a2e9bb7da8""\n,\n\n\n  \n""parentId""\n:\n undefined\n,\n\n\n  \n""name""\n:\n \n""chat""\n,\n\n\n  \n""id""\n:\n \n""117d98e8add5dc80""\n,\n\n\n  \n""kind""\n:\n \n0\n,\n\n\n  \n""timestamp""\n:\n \n1688386291908349\n,\n\n\n  \n""duration""\n:\n \n501\n,\n\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nTraces\nA deep dive into the details of a trace\nSpans\nA span represents a unit of work or operation (think a \nspan\n of time). It tracks specific operations that a request makes, painting a picture of what happened during the time in which that operation was executed.\nA span contains name, time-related data, structured log messages, and other metadata (that is, Attributes) to provide information about the operation it tracks. A span for an LLM execution in JSON format is displayed below\nCopy\n{\n\n\n   \n""name""\n:\n \n""llm""\n,\n\n\n   \n""context""\n:\n {\n\n\n       \n""trace_id""\n:\n \n""0x6c80880dbeb609e2ed41e06a6397a0dd""\n,\n\n\n       \n""span_id""\n:\n \n""0xd9bdedf0df0b7208""\n,\n\n\n       \n""trace_state""\n:\n \n""[]""\n\n\n   }\n,\n\n\n   \n""kind""\n:\n \n""SpanKind.INTERNAL""\n,\n\n\n   \n""parent_id""\n:\n \n""0x7eb5df0046c77cd2""\n,\n\n\n   \n""start_time""\n:\n \n""2024-05-08T21:46:11.480777Z""\n,\n\n\n   \n""end_time""\n:\n \n""2024-05-08T21:46:35.368042Z""\n,\n\n\n   \n""status""\n:\n {\n\n\n       \n""status_code""\n:\n \n""OK""\n\n\n   }\n,\n\n\n   \n""attributes""\n:\n {\n\n\n       \n""openinference.span.kind""\n:\n \n""LLM""\n,\n\n\n       \n""llm.input_messages.0.message.role""\n:\n \n""system""\n,\n\n\n       ""llm.input_messages.0.message.content"": ""\\n  The following is a friendly conversation between a user and an AI assistant.\\n  The assistant is talkative and provides lots of specific details from its context.\\n  If the assistant does not know the answer to a question, it truthfully says it\\n  does not know.\\n\\n  Here are the relevant documents for the context:\\n\\n  page_label: 7\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.4Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\na. No piece may weigh more than 70 pounds.\\nb. The combined length and girth of a piece (the length of its longest side plus \\nthe distance around its thickest part) may not exceed 108 inches.\\nc. Lower size or weight standards apply to mail addressed to certain APOs and \\nFPOs, subject to 703.2.0  and 703.4.0  and for Department of State mail, \\nsubject to 703.3.0 .\\n\\npage_label: 6\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.2.10Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\na. The reply half of a double card must be used for reply only and may not be \\nused to convey a message to the original addressee or to send statements \\nof account. The reply half may be formatted for response purposes (e.g., contain blocks for completion by the addressee).\\nb. A double card must be folded before mailing and prepared so that the \\naddress on the reply half is on the inside when the double card is originally \\nmailed. The address side of the reply half may be prepared as Business \\nReply Mail, Courtesy Reply Mail, meter reply mail, or as a USPS Returns service label.\\nc. Plain stickers, seals, or a single wire stitch (staple) may be used to fasten the \\nopen edge at the top or bottom once the card is folded if affixed so that the \\ninner surfaces of the cards can be readily examined. Fasteners must be \\naffixed according to the applicable preparation requirements for the price claimed. Any sealing on the left and right sides of the cards, no matter the \\nsealing process used, is not permitted.\\nd. The first half of a double card must be detached when the reply half is \\nmailed for return. \\n6.2.10   Enclosures\\nEnclosures in double postcards are prohibited at card prices.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Why I am seeing X span in my trace?\nAnswer: '}, {'role': 'assistant', 'content': 'You are seeing a specific span in your trace because it represents a unit of work or operation that was executed during the time the request was made. Spans track specific operations, providing detailed information about what occurred during that operation. Each span contains attributes, time-related data, and other metadata to give insights into the operation it represents.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/manual-instrumentation/javascript\ntitle: Instrument: TS\n\nimport\n { trace } \nfrom\n \n\'@opentelemetry/api\'\n;\n\n\nimport\n express\n,\n { Express } \nfrom\n \n\'express\'\n;\n\n\nimport\n { OpenAI } \nfrom\n \n""openai""\n;\n\n\n\n\nconst\n \ntracer\n \n=\n \ntrace\n.getTracer\n(\n\'llm-server\'\n,\n \n\'0.1.0\'\n);\n\n\n\n\nconst\n \nPORT\n:\n \nnumber\n \n=\n \nparseInt\n(\nprocess\n.\nenv\n.\nPORT\n \n||\n \n\'8080\'\n);\n\n\nconst\n \napp\n:\n \nExpress\n \n=\n \nexpress\n();\n\n\n\n\nconst\n \nopenai\n \n=\n \nnew\n \nOpenAI\n({\n\n\n  apiKey\n:\n \nprocess\n.\nenv\n.\nOPENAI_API_KEY\n,\n\n\n});\n\n\n\n\napp\n.get\n(\n\'/chat\'\n,\n (req\n,\n res) \n=>\n {\n\n\n  \nconst\n \nmessage\n \n=\n \nreq\n.\nquery\n.message\n\n\n  \nlet\n chatCompletion \n=\n \nawait\n \nopenai\n.\nchat\n.\ncompletions\n.create\n({\n\n\n    messages\n:\n [{ role\n:\n \n""user""\n,\n content\n:\n message }]\n,\n\n\n    model\n:\n \n""gpt-3.5-turbo""\n,\n\n\n  });\n\n\n  \nres\n.send\n(\nchatCompletion\n.choices[\n0\n].message);\n\n\n});\n\n\n\n\napp\n.listen\n(\nPORT\n,\n () \n=>\n {\n\n\n  \nconsole\n.log\n(\n`Listening for requests on http://localhost:\n${\nPORT\n}\n`\n);\n\n\n});\nCreate spans\nNow that you have \ntracers\n initialized, you can create \nspans\n.\nThe API of OpenTelemetry JavaScript exposes two methods that allow you to create spans:\ntracer.startSpan\n: Starts a new span without setting it on context.\ntracer.startActiveSpan\n: Starts a new span and calls the given callback function passing it the created span as first argument. The new span gets set in context and this context is activated for the duration of the function call.\nIn most cases you want to use the latter (\ntracer.startActiveSpan\n), as it takes care of setting the span and its context active.\nThe code below illustrates how to create an active span.\nCopy\nimport\n { trace\n,\n Span } \nfrom\n \n""@opentelemetry/api""\n;\n\n\nimport\n { SpanKind } \nfrom\n \n""@opentelemetry/api""\n;\n\n\nimport\n {\n\n\n    SemanticConventions\n,\n\n\n    OpenInferenceSpanKind\n,\n\n\n} \nfrom\n \n""@arizeai/openinference-semantic-conventions""\n;\n\n\n\n\nexport\n \nfunction\n \nchat\n(message\n:\n \nstring\n) {\n\n\n    \n// Create a span. A span must be closed.\n\n\n    \nreturn\n \ntracer\n.startActiveSpan\n(\n\n\n        \n""chat""\n,\n\n\n        (span\n:\n \nSpan\n) \n=>\n {\n\n\n            \nspan\n.setAttributes\n({\n\n\n                [\nSemanticConventions\n.\nOPENINFERENCE_SPAN_KIND\n]\n:\n \nOpenInferenceSpanKind\n.chain\n,\n\n\n                [\nSemanticConventions\n.\nINPUT_VALUE\n]\n:\n message\n,\n\n\n            });\n\n\n            \nlet\n chatCompletion \n=\n \nawait\n \nopenai\n.\nchat\n.\ncompletions\n.create\n({\n\n\n                messages\n:\n [{ role\n:\n \n""user""\n,\n content\n:\n message }]\n,\n\n\n                model\n:\n \n""gpt-3.5-turbo""\n,\n\n\n            });\n\n\n            \nspan\n.setAttributes\n({\n\n\n                attributes\n:\n {\n\n\n                    [\nSemanticConventions\n.\nOUTPUT_VALUE\n]\n:\n \nchatCompletion\n.choices[\n0\n].message\n,\n\n\n                }\n,\n\n\n            });\n\n\n            \n// Be sure to end the span!\n\n\n            \nspan\n.end\n();\n\n\n            \nreturn\n result;\n\n\n        }\n\n\n    );\n\n\n}\nThe above instrumented code can now be pasted in the \n/chat\n handler. You should now be able to see spans emitted from your app.\nStart your app as follows, and then send it requests by visiting \nhttp://localhost:8080/chat?message=""how long is a pencil""\n with your browser or \ncurl\n.\nCopy\nts-node\n \n--require\n \n./instrumentation.ts\n \napp.ts\nAfter a while, you should see the spans printed in the console by the \nConsoleSpanExporter\n, something like this:\nCopy\n{\n\n\n  \n""traceId""\n:\n \n""6cc927a05e7f573e63f806a2e9bb7da8""\n,\n\n\n  \n""parentId""\n:\n undefined\n,\n\n\n  \n""name""\n:\n \n""chat""\n,\n\n\n  \n""id""\n:\n \n""117d98e8add5dc80""\n,\n\n\n  \n""kind""\n:\n \n0\n,\n\n\n  \n""timestamp""\n:\n \n1688386291908349\n,\n\n\n  \n""duration""\n:\n \n501\n,\n\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nTraces\nA deep dive into the details of a trace\nSpans\nA span represents a unit of work or operation (think a \nspan\n of time). It tracks specific operations that a request makes, painting a picture of what happened during the time in which that operation was executed.\nA span contains name, time-related data, structured log messages, and other metadata (that is, Attributes) to provide information about the operation it tracks. A span for an LLM execution in JSON format is displayed below\nCopy\n{\n\n\n   \n""name""\n:\n \n""llm""\n,\n\n\n   \n""context""\n:\n {\n\n\n       \n""trace_id""\n:\n \n""0x6c80880dbeb609e2ed41e06a6397a0dd""\n,\n\n\n       \n""span_id""\n:\n \n""0xd9bdedf0df0b7208""\n,\n\n\n       \n""trace_state""\n:\n \n""[]""\n\n\n   }\n,\n\n\n   \n""kind""\n:\n \n""SpanKind.INTERNAL""\n,\n\n\n   \n""parent_id""\n:\n \n""0x7eb5df0046c77cd2""\n,\n\n\n   \n""start_time""\n:\n \n""2024-05-08T21:46:11.480777Z""\n,\n\n\n   \n""end_time""\n:\n \n""2024-05-08T21:46:35.368042Z""\n,\n\n\n   \n""status""\n:\n {\n\n\n       \n""status_code""\n:\n \n""OK""\n\n\n   }\n,\n\n\n   \n""attributes""\n:\n {\n\n\n       \n""openinference.span.kind""\n:\n \n""LLM""\n,\n\n\n       \n""llm.input_messages.0.message.role""\n:\n \n""system""\n,\n\n\n       ""llm.input_messages.0.message.content"": ""\\n  The following is a friendly conversation between a user and an AI assistant.\\n  The assistant is talkative and provides lots of specific details from its context.\\n  If the assistant does not know the answer to a question, it truthfully says it\\n  does not know.\\n\\n  Here are the relevant documents for the context:\\n\\n  page_label: 7\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.4Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\na. No piece may weigh more than 70 pounds.\\nb. The combined length and girth of a piece (the length of its longest side plus \\nthe distance around its thickest part) may not exceed 108 inches.\\nc. Lower size or weight standards apply to mail addressed to certain APOs and \\nFPOs, subject to 703.2.0  and 703.4.0  and for Department of State mail, \\nsubject to 703.3.0 .\\n\\npage_label: 6\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.2.10Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\na. The reply half of a double card must be used for reply only and may not be \\nused to convey a message to the original addressee or to send statements \\nof account. The reply half may be formatted for response purposes (e.g., contain blocks for completion by the addressee).\\nb. A double card must be folded before mailing and prepared so that the \\naddress on the reply half is on the inside when the double card is originally \\nmailed. The address side of the reply half may be prepared as Business \\nReply Mail, Courtesy Reply Mail, meter reply mail, or as a USPS Returns service label.\\nc. Plain stickers, seals, or a single wire stitch (staple) may be used to fasten the \\nopen edge at the top or bottom once the card is folded if affixed so that the \\ninner surfaces of the cards can be readily examined. Fasteners must be \\naffixed according to the applicable preparation requirements for the price claimed. Any sealing on the left and right sides of the cards, no matter the \\nsealing process used, is not permitted.\\nd. The first half of a double card must be detached when the reply half is \\nmailed for return. \\n6.2.10   Enclosures\\nEnclosures in double postcards are prohibited at card prices.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Why I am seeing X span in my trace?\nAnswer: '}, {'role': 'assistant', 'content': 'You are seeing a specific span in your trace because it represents a unit of work or operation that was executed during the time the request was made. Spans track specific operations, providing detailed information about what occurred during that operation. Each span contains attributes, time-related data, and other metadata to give insights into the operation it represents.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MTM=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/llm-traces-1\ntitle: Quickstart: Tracing\n\n# Retrieve and generate using the relevant snippets of the blog.\n\n\nretriever \n=\n vectorstore\n.\nas_retriever\n()\n\n\nprompt \n=\n hub\n.\npull\n(\n""rlm/rag-prompt""\n)\n\n\n\n\ndef\n \nformat_docs\n(\ndocs\n):\n\n\n    \nreturn\n \n""\\n\\n""\n.\njoin\n(doc.page_content \nfor\n doc \nin\n docs)\n\n\n\n\n\n\nrag_chain \n=\n (\n\n\n    \n{\n""context""\n:\n retriever \n|\n format_docs\n,\n \n""question""\n:\n \nRunnablePassthrough\n()}\n\n\n    \n|\n prompt\n\n\n    \n|\n llm\n\n\n    \n|\n \nStrOutputParser\n()\n\n\n)\n\n\n\n\n# Execute the chain\n\n\nresponse \n=\n rag_chain\n.\ninvoke\n(\n""What is Task Decomposition?""\n)\nSee the \nintegration guide\n for details\nCopy\npip\n \ninstall\n \nopenai\nCopy\nimport\n phoenix \nas\n px\n\n\nfrom\n phoenix\n.\ntrace\n.\nopenai \nimport\n OpenAIInstrumentor\n\n\n\n\n# To view traces in Phoenix, you will first have to start a Phoenix server. You can do this by running the following:\n\n\nsession \n=\n px\n.\nlaunch_app\n()\n\n\n\n\n# Initialize OpenAI auto-instrumentation\n\n\nOpenAIInstrumentor\n().\ninstrument\n()\n\n\n\n\nimport\n os\n\n\nfrom\n openai \nimport\n OpenAI\n\n\n\n\n# Initialize an OpenAI client\n\n\nclient \n=\n \nOpenAI\n(api_key\n=\n\'\'\n)\n\n\n\n\n# Define a conversation with a user message\n\n\nconversation \n=\n [\n\n\n    \n{\n""role""\n:\n \n""system""\n,\n \n""content""\n:\n \n""You are a helpful assistant.""\n},\n\n\n    \n{\n""role""\n:\n \n""user""\n,\n \n""content""\n:\n \n""Hello, can you help me with something?""\n}\n\n\n]\n\n\n\n\n# Generate a response from the assistant\n\n\nresponse \n=\n client\n.\nchat\n.\ncompletions\n.\ncreate\n(\n\n\n    model\n=\n""gpt-3.5-turbo""\n,\n\n\n    messages\n=\nconversation,\n\n\n)\n\n\n\n\n# Extract and print the assistant\'s reply\n\n\n# The traces will be available in the Phoenix App for the above messsages\n\n\nassistant_reply \n=\n response\n.\nchoices\n[\n0\n].\nmessage\n.\ncontent\nOnce you\'ve executed a sufficient number of queries (or chats) to your application, you can view the details of the UI by refreshing the browser url\nA detailed view of a trace of a RAG application using LlamaIndex\nExporting Traces from Phoenix\nFrom the App\nCopy\n# You can export a dataframe from the session\n\n\ndf \n=\n px\n.\nClient\n().\nget_spans_dataframe\n()\n\n\n\n\n# Note that you can apply a filter if you would like to export only a sub-set of spans\n\n\ndf \n=\n px\n.\nClient\n().\nget_spans_dataframe\n(\n\'span_kind == ""RETRIEVER""\'\n)\nFor full details on how to export trace data, see \nthe detailed guide\nEvaluating Traces\nIn addition to launching phoenix on LlamaIndex and LangChain, teams can export trace data to a dataframe in order to run LLM Evals on the data.\nLearn more in the \nevals quickstart\n.\nConclusion\nLLM Traces\n are a powerful way to troubleshoot and understand your application and can be leveraged to \nevaluate\n the quality of your application (see also: this \ndeep dive on LLM evaluation\n). For a full list of notebooks that illustrate this in full-color, please check out the \nnotebooks section\n.\nPrevious\nOverview: Tracing\nNext\nIntegrations: Tracing\nLast updated \n6 hours ago\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/save-and-load-traces\ntitle: Save and Load Traces\n\nSave and Load Traces\nHow to manually save and load traces\nIn addition to persisting to a database, Phoenix allows you to save and load your trace data to and from external files.\nSaving Traces\nThe initial step involves saving the traces present in a Phoenix instance to a designated location.\nCopy\nmy_traces = px.Client().get_trace_dataset().save()\nLoading Traces\nTo facilitate the retrieval of these saved traces, one can execute the prescribed commands upon starting Phoenix.\nCopy\npx.launch_app(trace=px.TraceDataset.load(my_traces))\nNote the above will save to a default phoenix trace directory, to save in another directory, use the following example below.\nSaving Traces to a Specific Directory\nYou can specify the directory to save your traces by passing a\ndirectory\n argument to the \nsave\n method.\nCopy\nimport os\n\n\n\n\n# Specify and Create the Directory for Trace Dataset\n\n\ndirectory = \'/my_saved_traces\'\n\n\nos.makedirs(directory, exist_ok=True)\n\n\n\n\n# Save the Trace Dataset\n\n\ntrace_id = px.Client().get_trace_dataset().save(directory=directory)\nThis output the trace ID and prints the path of the saved file:\n💾 Trace dataset saved to under ID: f7733fda-6ad6-4427-a803-55ad2182b662 \n📂 Trace dataset path: /my_saved_traces/trace_dataset-f7733fda-6ad6-4427-a803-55ad2182b662.parquet\nLoading Traces from a Specific Directory\nTo load the previously saved trace dataset, use the trace ID and the specific directory path where the trace was stored.\nCopy\npx.launch_app(trace=px.TraceDataset.load(\'f7733fda-6ad6-4427-a803-55ad2182b662\', directory=""/my_saved_traces/""))\nPrevious\nImporting Existing Traces\nNext\nCapture Feedback\nLast updated \n2 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I get the last 7 days of traces?\nAnswer: '}, {'role': 'assistant', 'content': 'To get the last 7 days of traces, you can utilize the timestamp information associated with each trace. By filtering the traces based on their timestamp, you can extract the traces that fall within the last 7 days. This method allows you to specifically target and retrieve the traces that have been recorded within the desired timeframe.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/llm-traces-1\ntitle: Quickstart: Tracing\n\n# Retrieve and generate using the relevant snippets of the blog.\n\n\nretriever \n=\n vectorstore\n.\nas_retriever\n()\n\n\nprompt \n=\n hub\n.\npull\n(\n""rlm/rag-prompt""\n)\n\n\n\n\ndef\n \nformat_docs\n(\ndocs\n):\n\n\n    \nreturn\n \n""\\n\\n""\n.\njoin\n(doc.page_content \nfor\n doc \nin\n docs)\n\n\n\n\n\n\nrag_chain \n=\n (\n\n\n    \n{\n""context""\n:\n retriever \n|\n format_docs\n,\n \n""question""\n:\n \nRunnablePassthrough\n()}\n\n\n    \n|\n prompt\n\n\n    \n|\n llm\n\n\n    \n|\n \nStrOutputParser\n()\n\n\n)\n\n\n\n\n# Execute the chain\n\n\nresponse \n=\n rag_chain\n.\ninvoke\n(\n""What is Task Decomposition?""\n)\nSee the \nintegration guide\n for details\nCopy\npip\n \ninstall\n \nopenai\nCopy\nimport\n phoenix \nas\n px\n\n\nfrom\n phoenix\n.\ntrace\n.\nopenai \nimport\n OpenAIInstrumentor\n\n\n\n\n# To view traces in Phoenix, you will first have to start a Phoenix server. You can do this by running the following:\n\n\nsession \n=\n px\n.\nlaunch_app\n()\n\n\n\n\n# Initialize OpenAI auto-instrumentation\n\n\nOpenAIInstrumentor\n().\ninstrument\n()\n\n\n\n\nimport\n os\n\n\nfrom\n openai \nimport\n OpenAI\n\n\n\n\n# Initialize an OpenAI client\n\n\nclient \n=\n \nOpenAI\n(api_key\n=\n\'\'\n)\n\n\n\n\n# Define a conversation with a user message\n\n\nconversation \n=\n [\n\n\n    \n{\n""role""\n:\n \n""system""\n,\n \n""content""\n:\n \n""You are a helpful assistant.""\n},\n\n\n    \n{\n""role""\n:\n \n""user""\n,\n \n""content""\n:\n \n""Hello, can you help me with something?""\n}\n\n\n]\n\n\n\n\n# Generate a response from the assistant\n\n\nresponse \n=\n client\n.\nchat\n.\ncompletions\n.\ncreate\n(\n\n\n    model\n=\n""gpt-3.5-turbo""\n,\n\n\n    messages\n=\nconversation,\n\n\n)\n\n\n\n\n# Extract and print the assistant\'s reply\n\n\n# The traces will be available in the Phoenix App for the above messsages\n\n\nassistant_reply \n=\n response\n.\nchoices\n[\n0\n].\nmessage\n.\ncontent\nOnce you\'ve executed a sufficient number of queries (or chats) to your application, you can view the details of the UI by refreshing the browser url\nA detailed view of a trace of a RAG application using LlamaIndex\nExporting Traces from Phoenix\nFrom the App\nCopy\n# You can export a dataframe from the session\n\n\ndf \n=\n px\n.\nClient\n().\nget_spans_dataframe\n()\n\n\n\n\n# Note that you can apply a filter if you would like to export only a sub-set of spans\n\n\ndf \n=\n px\n.\nClient\n().\nget_spans_dataframe\n(\n\'span_kind == ""RETRIEVER""\'\n)\nFor full details on how to export trace data, see \nthe detailed guide\nEvaluating Traces\nIn addition to launching phoenix on LlamaIndex and LangChain, teams can export trace data to a dataframe in order to run LLM Evals on the data.\nLearn more in the \nevals quickstart\n.\nConclusion\nLLM Traces\n are a powerful way to troubleshoot and understand your application and can be leveraged to \nevaluate\n the quality of your application (see also: this \ndeep dive on LLM evaluation\n). For a full list of notebooks that illustrate this in full-color, please check out the \nnotebooks section\n.\nPrevious\nOverview: Tracing\nNext\nIntegrations: Tracing\nLast updated \n6 hours ago\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/save-and-load-traces\ntitle: Save and Load Traces\n\nSave and Load Traces\nHow to manually save and load traces\nIn addition to persisting to a database, Phoenix allows you to save and load your trace data to and from external files.\nSaving Traces\nThe initial step involves saving the traces present in a Phoenix instance to a designated location.\nCopy\nmy_traces = px.Client().get_trace_dataset().save()\nLoading Traces\nTo facilitate the retrieval of these saved traces, one can execute the prescribed commands upon starting Phoenix.\nCopy\npx.launch_app(trace=px.TraceDataset.load(my_traces))\nNote the above will save to a default phoenix trace directory, to save in another directory, use the following example below.\nSaving Traces to a Specific Directory\nYou can specify the directory to save your traces by passing a\ndirectory\n argument to the \nsave\n method.\nCopy\nimport os\n\n\n\n\n# Specify and Create the Directory for Trace Dataset\n\n\ndirectory = \'/my_saved_traces\'\n\n\nos.makedirs(directory, exist_ok=True)\n\n\n\n\n# Save the Trace Dataset\n\n\ntrace_id = px.Client().get_trace_dataset().save(directory=directory)\nThis output the trace ID and prints the path of the saved file:\n💾 Trace dataset saved to under ID: f7733fda-6ad6-4427-a803-55ad2182b662 \n📂 Trace dataset path: /my_saved_traces/trace_dataset-f7733fda-6ad6-4427-a803-55ad2182b662.parquet\nLoading Traces from a Specific Directory\nTo load the previously saved trace dataset, use the trace ID and the specific directory path where the trace was stored.\nCopy\npx.launch_app(trace=px.TraceDataset.load(\'f7733fda-6ad6-4427-a803-55ad2182b662\', directory=""/my_saved_traces/""))\nPrevious\nImporting Existing Traces\nNext\nCapture Feedback\nLast updated \n2 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I get the last 7 days of traces?\nAnswer: '}, {'role': 'assistant', 'content': 'To get the last 7 days of traces, you can utilize the timestamp information associated with each trace. By filtering the traces based on their timestamp, you can extract the traces that fall within the last 7 days. This method allows you to specifically target and retrieve the traces that have been recorded within the desired timeframe.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MTQ=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/extract-data-from-spans\ntitle: Querying Spans\n\n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n    score\n=\n""document.score""\n,\n\n\n)\nHow to Apply Filters\nThe \n.where()\n method accepts a string of valid Python boolean expression. The expression can be arbitrarily complex, but restrictions apply, e.g. making function calls are generally disallowed. Below is a conjunction filtering also on whether the input value contains the string \n\'programming\'\n.\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""span_kind == \'RETRIEVER\' and \'programming\' in input.value""\n\n\n)\nFiltering Spans by Evaluation Results\nFiltering spans by evaluation results, e.g. \nscore\n or \nlabel\n, can be done via a special syntax. The name of the evaluation is specified as an indexer on the special keyword \nevals\n. The example below filters for spans with the \nincorrect\n label on their \ncorrectness\n evaluations. (See \nhere\n for how to compute evaluations for traces, and \nhere\n for how to ingest those results back to Phoenix.)\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""evals[\'correctness\'].label == \'incorrect\'""\n\n\n)\nFiltering on Metadata\nmetadata\n is an attribute that is a dictionary and it can be filtered like a dictionary.\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""metadata[""\ntopic\n""] == \'programming\'""\n\n\n)\nFiltering for Substring\nNote that Python strings do not have a \ncontain\n method, and substring search is done with the \nin\n operator.\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""\'programming\' in metadata[""\ntopic\n""]""\n\n\n)\nFiltering for No Evaluations\nGet spans that do not have an evaluation attached yet\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""evals[\'correctness\'].label is None""\n\n\n)\n\n\n# correctness is whatever you named your evaluation metric\nHow to Extract Attributes\nSpan attributes can be selected by simply listing them inside \n.select()\n method.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n\n\n    \n""input.value""\n,\n\n\n    \n""output.value,\n\n\n)\nRenaming Output Columns\nKeyword-argument style can be used to rename the columns in the dataframe. The example below returns two columns named \ninput\n and \noutput\n instead of the original names of the attributes.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n\n\n    input\n=\n""input.value""\n,\n\n\n    output\n=\n""output.value,\n\n\n)\nArbitrary Output Column Names \nIf arbitrary output names are desired, e.g. names with spaces and symbols, we can leverage Python\'s double-asterisk idiom for unpacking a dictionary, as shown below.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n**\n{\n\n\n    \n""Value (Input)""\n: \n""input.value""\n,\n\n\n    \n""Value (Output)""\n: \n""output.value,\n\n\n})\nAdvanced Usage\nConcatenating\nThe document contents can also be concatenated together. The query below concatenates the list of \ndocument.content\n with \n\\n\\n\n (double newlines), which is the default separator. Keyword arguments are necessary to name the output columns, and in this example we name the output column as \nreference\n. (Python\'s double-asterisk unpacking idiom can be used to specify arbitrary output names containing spaces or symbols. See \nhere\n for an example.)\nCopy\nquery \n=\n \nSpanQuery\n().\nconcat\n(\n\n\n    \n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n)\nSpecial Separators\nIf a different separator is desired, say \n\\n************\\n\n, it can be specified as follows.\nCopy\nquery \n=\n \nSpanQuery\n().\nconcat\n(\n\n\n    \n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n).\nwith_concat_separator\n(\n\n\n    separator\n=\n""\\n************\\n""\n,\n\n\n)\nUsing Parent ID as Index\nThis is useful for joining a span to its parent span. To do that we would first index the child span by selecting its parent ID and renaming it as \nspan_id\n. This works because \nspan_id\n is a special column name: whichever column having that name will become the index of the output DataFrame.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n\n\n    span_id\n=\n""parent_id""\n,\n\n\n    output\n=\n""output.value,\n\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals/online-evals\ntitle: Online Evals\n\nOnline Evals\nYou can use cron to run evals client-side as your traces and spans are generated, augmenting your dataset with evaluations in an online manner. View the \nexample in Github\n.\nThis example:\nContinuously queries a LangChain application to send new traces and spans to your Phoenix session\nQueries new spans once per minute and runs evals, including:\nHallucination\nQ&A Correctness\nRelevance\nLogs evaluations back to Phoenix so they appear in the UI\nThe evaluation script is run as a cron job, enabling you to adjust the frequency of the evaluation job:\nCopy\n* * * * * /path/to/python /path/to/run_evals.py\nThe above script can be run periodically to augment Evals in Phoenix.\nPrevious\nBring Your Own Evaluator\nNext\nEvaluation Models\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I get spans with no evaluations?\nAnswer: '}, {'role': 'assistant', 'content': 'To get spans with no evaluations, you can apply a filter in your query using the syntax ""evals[\'correctness\'].label is None"". This filter will help you retrieve spans that do not have any evaluation attached yet.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/extract-data-from-spans\ntitle: Querying Spans\n\n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n    score\n=\n""document.score""\n,\n\n\n)\nHow to Apply Filters\nThe \n.where()\n method accepts a string of valid Python boolean expression. The expression can be arbitrarily complex, but restrictions apply, e.g. making function calls are generally disallowed. Below is a conjunction filtering also on whether the input value contains the string \n\'programming\'\n.\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""span_kind == \'RETRIEVER\' and \'programming\' in input.value""\n\n\n)\nFiltering Spans by Evaluation Results\nFiltering spans by evaluation results, e.g. \nscore\n or \nlabel\n, can be done via a special syntax. The name of the evaluation is specified as an indexer on the special keyword \nevals\n. The example below filters for spans with the \nincorrect\n label on their \ncorrectness\n evaluations. (See \nhere\n for how to compute evaluations for traces, and \nhere\n for how to ingest those results back to Phoenix.)\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""evals[\'correctness\'].label == \'incorrect\'""\n\n\n)\nFiltering on Metadata\nmetadata\n is an attribute that is a dictionary and it can be filtered like a dictionary.\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""metadata[""\ntopic\n""] == \'programming\'""\n\n\n)\nFiltering for Substring\nNote that Python strings do not have a \ncontain\n method, and substring search is done with the \nin\n operator.\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""\'programming\' in metadata[""\ntopic\n""]""\n\n\n)\nFiltering for No Evaluations\nGet spans that do not have an evaluation attached yet\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""evals[\'correctness\'].label is None""\n\n\n)\n\n\n# correctness is whatever you named your evaluation metric\nHow to Extract Attributes\nSpan attributes can be selected by simply listing them inside \n.select()\n method.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n\n\n    \n""input.value""\n,\n\n\n    \n""output.value,\n\n\n)\nRenaming Output Columns\nKeyword-argument style can be used to rename the columns in the dataframe. The example below returns two columns named \ninput\n and \noutput\n instead of the original names of the attributes.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n\n\n    input\n=\n""input.value""\n,\n\n\n    output\n=\n""output.value,\n\n\n)\nArbitrary Output Column Names \nIf arbitrary output names are desired, e.g. names with spaces and symbols, we can leverage Python\'s double-asterisk idiom for unpacking a dictionary, as shown below.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n**\n{\n\n\n    \n""Value (Input)""\n: \n""input.value""\n,\n\n\n    \n""Value (Output)""\n: \n""output.value,\n\n\n})\nAdvanced Usage\nConcatenating\nThe document contents can also be concatenated together. The query below concatenates the list of \ndocument.content\n with \n\\n\\n\n (double newlines), which is the default separator. Keyword arguments are necessary to name the output columns, and in this example we name the output column as \nreference\n. (Python\'s double-asterisk unpacking idiom can be used to specify arbitrary output names containing spaces or symbols. See \nhere\n for an example.)\nCopy\nquery \n=\n \nSpanQuery\n().\nconcat\n(\n\n\n    \n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n)\nSpecial Separators\nIf a different separator is desired, say \n\\n************\\n\n, it can be specified as follows.\nCopy\nquery \n=\n \nSpanQuery\n().\nconcat\n(\n\n\n    \n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n).\nwith_concat_separator\n(\n\n\n    separator\n=\n""\\n************\\n""\n,\n\n\n)\nUsing Parent ID as Index\nThis is useful for joining a span to its parent span. To do that we would first index the child span by selecting its parent ID and renaming it as \nspan_id\n. This works because \nspan_id\n is a special column name: whichever column having that name will become the index of the output DataFrame.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n\n\n    span_id\n=\n""parent_id""\n,\n\n\n    output\n=\n""output.value,\n\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals/online-evals\ntitle: Online Evals\n\nOnline Evals\nYou can use cron to run evals client-side as your traces and spans are generated, augmenting your dataset with evaluations in an online manner. View the \nexample in Github\n.\nThis example:\nContinuously queries a LangChain application to send new traces and spans to your Phoenix session\nQueries new spans once per minute and runs evals, including:\nHallucination\nQ&A Correctness\nRelevance\nLogs evaluations back to Phoenix so they appear in the UI\nThe evaluation script is run as a cron job, enabling you to adjust the frequency of the evaluation job:\nCopy\n* * * * * /path/to/python /path/to/run_evals.py\nThe above script can be run periodically to augment Evals in Phoenix.\nPrevious\nBring Your Own Evaluator\nNext\nEvaluation Models\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I get spans with no evaluations?\nAnswer: '}, {'role': 'assistant', 'content': 'To get spans with no evaluations, you can apply a filter in your query using the syntax ""evals[\'correctness\'].label is None"". This filter will help you retrieve spans that do not have any evaluation attached yet.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MTU=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals/online-evals\ntitle: Online Evals\n\nOnline Evals\nYou can use cron to run evals client-side as your traces and spans are generated, augmenting your dataset with evaluations in an online manner. View the \nexample in Github\n.\nThis example:\nContinuously queries a LangChain application to send new traces and spans to your Phoenix session\nQueries new spans once per minute and runs evals, including:\nHallucination\nQ&A Correctness\nRelevance\nLogs evaluations back to Phoenix so they appear in the UI\nThe evaluation script is run as a cron job, enabling you to adjust the frequency of the evaluation job:\nCopy\n* * * * * /path/to/python /path/to/run_evals.py\nThe above script can be run periodically to augment Evals in Phoenix.\nPrevious\nBring Your Own Evaluator\nNext\nEvaluation Models\nLast updated \n5 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals\ntitle: How to: Evals\n\nHow to: Evals\nPhoenix Evaluators\nHallucinations\nQ&A on Retrieved Data\nRetrieval (RAG) Relevance\nSummarization \nCode Generation\nToxicity\n \nAI vs Human \nReference (Citation) Eval\nBring Your Own Evaluator\nCategorical evaluator\n (llm_classify)\nNumeric evaluator\n (llm_generate)\nOnline Evals\nRun evaluations via a job to visualize in the UI as traces stream in.\nPrevious\nCustom Task Evaluation\nNext\nUse Phoenix Evaluators\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I run an evaluation?\nAnswer: '}, {'role': 'assistant', 'content': 'You can run evaluations by using cron to execute the evaluation script as a job periodically. This script will augment the evaluations in Phoenix by continuously querying a LangChain application to send new traces and spans to your Phoenix session, running evaluations such as Hallucination, Q&A Correctness, and Relevance, and logging the evaluations back to Phoenix for visualization in the UI.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals/online-evals\ntitle: Online Evals\n\nOnline Evals\nYou can use cron to run evals client-side as your traces and spans are generated, augmenting your dataset with evaluations in an online manner. View the \nexample in Github\n.\nThis example:\nContinuously queries a LangChain application to send new traces and spans to your Phoenix session\nQueries new spans once per minute and runs evals, including:\nHallucination\nQ&A Correctness\nRelevance\nLogs evaluations back to Phoenix so they appear in the UI\nThe evaluation script is run as a cron job, enabling you to adjust the frequency of the evaluation job:\nCopy\n* * * * * /path/to/python /path/to/run_evals.py\nThe above script can be run periodically to augment Evals in Phoenix.\nPrevious\nBring Your Own Evaluator\nNext\nEvaluation Models\nLast updated \n5 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals\ntitle: How to: Evals\n\nHow to: Evals\nPhoenix Evaluators\nHallucinations\nQ&A on Retrieved Data\nRetrieval (RAG) Relevance\nSummarization \nCode Generation\nToxicity\n \nAI vs Human \nReference (Citation) Eval\nBring Your Own Evaluator\nCategorical evaluator\n (llm_classify)\nNumeric evaluator\n (llm_generate)\nOnline Evals\nRun evaluations via a job to visualize in the UI as traces stream in.\nPrevious\nCustom Task Evaluation\nNext\nUse Phoenix Evaluators\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I run an evaluation?\nAnswer: '}, {'role': 'assistant', 'content': 'You can run evaluations by using cron to execute the evaluation script as a job periodically. This script will augment the evaluations in Phoenix by continuously querying a LangChain application to send new traces and spans to your Phoenix session, running evaluations such as Hallucination, Q&A Correctness, and Relevance, and logging the evaluations back to Phoenix for visualization in the UI.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MTY=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/overview-datasets\ntitle: Overview: Datasets\n\nOverview: Datasets\nThe velocity of AI application development is bottlenecked by quality evaluations because AI engineers are often faced with hard tradeoffs: which prompt or LLM best balances performance, latency, and cost. High quality evaluations are critical as they can help developers answer these types of questions with greater confidence.\nDatasets\nDatasets are integral to evaluation. They are collections of examples that provide the \ninputs\n and, optionally, expected \nreference\n outputs for assessing your application.  Datasets allow you to collect data from production, staging, evaluations, and even manually. The examples collected are used to run experiments and evaluations to track improvements to your prompt, LLM, or other parts of your LLM application.\nExperiments\nIn AI development, it\'s hard to understand how a change will affect performance. This breaks the dev flow, making iteration more guesswork than engineering.\nExperiments and evaluations solve this, helping distill the indeterminism of LLMs into tangible feedback that helps you ship more reliable product.\nSpecifically, good evals help you:\nUnderstand whether an update is an improvement or a regression\nDrill down into good / bad examples\nCompare specific examples vs. prior runs\nAvoid guesswork\n\n\nPrevious\nEvaluation Models\nNext\nQuickstart: Datasets\nLast updated \n22 days ago\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/evaluation\ntitle: Evaluators\n\nEvaluators\nEvaluation and benchmarking are crucial concepts in LLM development. To improve the performance of an LLM app (RAG, agents), you must have a way to measure it.\nPhoenix offers key modules to measure the quality of generated results as well as modules to measure retrieval quality.\nResponse Evaluation\n: Does the response match the retrieved context? Does it also match the query?\nRetrieval Evaluation\n: Are the retrieved sources relevant to the query?\nResponse Evaluation\nEvaluation of generated results can be challenging. Unlike traditional ML, the predicted results are not numeric or categorical, making it hard to define quantitative metrics for this problem.\nPhoenix offers \nLLM Evaluations\n, a module designed to measure the quality of results. This module uses a ""gold"" LLM (e.g. GPT-4) to decide whether the generated answer is correct in a variety of ways.\n\nNote that many of these evaluation criteria DO NOT require ground-truth labels. Evaluation can be done simply with a combination of the \ninput\n (query), \noutput\n (response), and \ncontext\n.\nLLM Evals supports the following response evaluation criteria:\nQA Correctness\n - Whether a question was correctly answered by the system based on the retrieved data. In contrast to retrieval Evals that are checks on chunks of data returned, this check is a system level check of a correct Q&A.\nHallucinations\n \n-\n Designed to detect LLM hallucinations relative to retrieved context\nToxicity\n - Identify if the AI response is racist, biased, or toxic\nResponse evaluations are a critical first step to figuring out whether your LLM App is running correctly. Response evaluations can pinpoint specific executions (a.k.a. traces) that are performing badly and can be aggregated up so that you can track how your application is running as a whole.\nEvaluations can be aggregated across executions to be used as KPIs\nRetrieval Evaluation\nPhoenix also provides evaluation of retrieval independently.\nThe concept of retrieval evaluation is not new; given a set of relevance scores for a set of retrieved documents, we can evaluate retrievers using retrieval metrics like \nprecision\n, \nNDCG\n, \nhit rate\n and more.\nLLM Evals supports the following retrieval evaluation criteria:\nRelevance\n - Evaluates whether a retrieved document chunk contains an answer to the query.\nRetrieval Evaluations can be run directly on application traces\nRetrieval is possibly the most important step in any LLM application as poor and/or incorrect retrieval can be the cause of bad response generation. If your application uses RAG to power an LLM, retrieval evals can help you identify the cause of hallucinations and incorrect answers.\nEvaluations\nInferences that contain generative records can be fed into evals to produce evaluations for analysis\nWith Phoenix\'s LLM Evals, evaluation results (or just \nEvaluations\n for short) is data consisting of 3 main columns:\nlabel\n: str [optional] - a classification label for the evaluation (e.g. ""hallucinated"" vs ""factual""). Can be used to calculate percentages (e.g. percent hallucinated) and can be used to filter down your data (e.g. \nEvals[""Hallucinations""].label == ""hallucinated""\n)\nscore\n: number [optional] - a numeric score for the evaluation (e.g. 1 for good, 0 for bad). Scores are great way to sort your data to surface poorly performing examples and can be used to filter your data by a threshold.\nexplanation\n: str [optional] - the reasoning for why the evaluation label or score was given. In the case of LLM evals, this is the evaluation model\'s reasoning. While explanations are optional, they can be extremely useful when trying to understand problematic areas of your application.\nLet\'s take a look at an example list of \nQ&A relevance\n evaluations:\nlabel\nexplanation\nscore\ncorrect\nThe reference text explains that YC was not or...\n1\ncorrect\nTo determine if the answer is correct, we need...\n1\nincorrect\nTo determine if the answer is correct, we must...\n0\ncorrect\nTo determine if the answer is correct, we need...\n1\nThese three columns combined can drive any type of evaluation you can imagine. \nlabel\n provides a way to classify responses, \nscore\n provides a way to assign a numeric assessment, and \nexplanation\n gives you a way to get qualitative feedback.\nEvaluating Traces\nAdding evaluations on traces can highlight problematic areas that require further analysis\nWith Phoenix, evaluations can be ""attached"" to the \nspans\n and \ndocuments\n collected. In order to facilitate this, Phoenix supports the following steps.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is an evaluation?\nAnswer: '}, {'role': 'assistant', 'content': 'An evaluation is a process of assessing the quality of generated results or retrieval in LLM development. It involves measuring whether the response matches the retrieved context or query, evaluating the relevance of retrieved sources, and checking for criteria such as QA correctness, hallucinations, and toxicity. Evaluations can be done based on the input (query), output (response), and context, and they help developers understand if updates are improvements or regressions, identify specific examples, and avoid guesswork in determining the performance of their LLM application.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/overview-datasets\ntitle: Overview: Datasets\n\nOverview: Datasets\nThe velocity of AI application development is bottlenecked by quality evaluations because AI engineers are often faced with hard tradeoffs: which prompt or LLM best balances performance, latency, and cost. High quality evaluations are critical as they can help developers answer these types of questions with greater confidence.\nDatasets\nDatasets are integral to evaluation. They are collections of examples that provide the \ninputs\n and, optionally, expected \nreference\n outputs for assessing your application.  Datasets allow you to collect data from production, staging, evaluations, and even manually. The examples collected are used to run experiments and evaluations to track improvements to your prompt, LLM, or other parts of your LLM application.\nExperiments\nIn AI development, it\'s hard to understand how a change will affect performance. This breaks the dev flow, making iteration more guesswork than engineering.\nExperiments and evaluations solve this, helping distill the indeterminism of LLMs into tangible feedback that helps you ship more reliable product.\nSpecifically, good evals help you:\nUnderstand whether an update is an improvement or a regression\nDrill down into good / bad examples\nCompare specific examples vs. prior runs\nAvoid guesswork\n\n\nPrevious\nEvaluation Models\nNext\nQuickstart: Datasets\nLast updated \n22 days ago\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/evaluation\ntitle: Evaluators\n\nEvaluators\nEvaluation and benchmarking are crucial concepts in LLM development. To improve the performance of an LLM app (RAG, agents), you must have a way to measure it.\nPhoenix offers key modules to measure the quality of generated results as well as modules to measure retrieval quality.\nResponse Evaluation\n: Does the response match the retrieved context? Does it also match the query?\nRetrieval Evaluation\n: Are the retrieved sources relevant to the query?\nResponse Evaluation\nEvaluation of generated results can be challenging. Unlike traditional ML, the predicted results are not numeric or categorical, making it hard to define quantitative metrics for this problem.\nPhoenix offers \nLLM Evaluations\n, a module designed to measure the quality of results. This module uses a ""gold"" LLM (e.g. GPT-4) to decide whether the generated answer is correct in a variety of ways.\n\nNote that many of these evaluation criteria DO NOT require ground-truth labels. Evaluation can be done simply with a combination of the \ninput\n (query), \noutput\n (response), and \ncontext\n.\nLLM Evals supports the following response evaluation criteria:\nQA Correctness\n - Whether a question was correctly answered by the system based on the retrieved data. In contrast to retrieval Evals that are checks on chunks of data returned, this check is a system level check of a correct Q&A.\nHallucinations\n \n-\n Designed to detect LLM hallucinations relative to retrieved context\nToxicity\n - Identify if the AI response is racist, biased, or toxic\nResponse evaluations are a critical first step to figuring out whether your LLM App is running correctly. Response evaluations can pinpoint specific executions (a.k.a. traces) that are performing badly and can be aggregated up so that you can track how your application is running as a whole.\nEvaluations can be aggregated across executions to be used as KPIs\nRetrieval Evaluation\nPhoenix also provides evaluation of retrieval independently.\nThe concept of retrieval evaluation is not new; given a set of relevance scores for a set of retrieved documents, we can evaluate retrievers using retrieval metrics like \nprecision\n, \nNDCG\n, \nhit rate\n and more.\nLLM Evals supports the following retrieval evaluation criteria:\nRelevance\n - Evaluates whether a retrieved document chunk contains an answer to the query.\nRetrieval Evaluations can be run directly on application traces\nRetrieval is possibly the most important step in any LLM application as poor and/or incorrect retrieval can be the cause of bad response generation. If your application uses RAG to power an LLM, retrieval evals can help you identify the cause of hallucinations and incorrect answers.\nEvaluations\nInferences that contain generative records can be fed into evals to produce evaluations for analysis\nWith Phoenix\'s LLM Evals, evaluation results (or just \nEvaluations\n for short) is data consisting of 3 main columns:\nlabel\n: str [optional] - a classification label for the evaluation (e.g. ""hallucinated"" vs ""factual""). Can be used to calculate percentages (e.g. percent hallucinated) and can be used to filter down your data (e.g. \nEvals[""Hallucinations""].label == ""hallucinated""\n)\nscore\n: number [optional] - a numeric score for the evaluation (e.g. 1 for good, 0 for bad). Scores are great way to sort your data to surface poorly performing examples and can be used to filter your data by a threshold.\nexplanation\n: str [optional] - the reasoning for why the evaluation label or score was given. In the case of LLM evals, this is the evaluation model\'s reasoning. While explanations are optional, they can be extremely useful when trying to understand problematic areas of your application.\nLet\'s take a look at an example list of \nQ&A relevance\n evaluations:\nlabel\nexplanation\nscore\ncorrect\nThe reference text explains that YC was not or...\n1\ncorrect\nTo determine if the answer is correct, we need...\n1\nincorrect\nTo determine if the answer is correct, we must...\n0\ncorrect\nTo determine if the answer is correct, we need...\n1\nThese three columns combined can drive any type of evaluation you can imagine. \nlabel\n provides a way to classify responses, \nscore\n provides a way to assign a numeric assessment, and \nexplanation\n gives you a way to get qualitative feedback.\nEvaluating Traces\nAdding evaluations on traces can highlight problematic areas that require further analysis\nWith Phoenix, evaluations can be ""attached"" to the \nspans\n and \ndocuments\n collected. In order to facilitate this, Phoenix supports the following steps.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is an evaluation?\nAnswer: '}, {'role': 'assistant', 'content': 'An evaluation is a process of assessing the quality of generated results or retrieval in LLM development. It involves measuring whether the response matches the retrieved context or query, evaluating the relevance of retrieved sources, and checking for criteria such as QA correctness, hallucinations, and toxicity. Evaluations can be done based on the input (query), output (response), and context, and they help developers understand if updates are improvements or regressions, identify specific examples, and avoid guesswork in determining the performance of their LLM application.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MTc=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nNot more than 4-1/4 inches high, or more than 6 inches long, or greater than \\n0.016 inch thick.\\nd. Not more than 3.5 ounces (Charge flat-size prices for First-Class Mail \\ncard-type pieces over 3.5 ounces.)\\n\\n  Instruction: Based on the above documents, provide a detailed answer for the user question below.\\n  Answer \\""don\'t know\\"" if not present in the document.\\n  "",\n\n\n       \n""llm.input_messages.1.message.role""\n:\n \n""user""\n,\n\n\n       \n""llm.input_messages.1.message.content""\n:\n \n""Hello""\n,\n\n\n       \n""llm.model_name""\n:\n \n""gpt-4-turbo-preview""\n,\n\n\n       \n""llm.invocation_parameters""\n:\n \n""{\\""temperature\\"": 0.1, \\""model\\"": \\""gpt-4-turbo-preview\\""}""\n,\n\n\n       \n""output.value""\n:\n \n""How are you?""\n }\n,\n\n\n   \n""events""\n:\n []\n,\n\n\n   \n""links""\n:\n []\n,\n\n\n   \n""resource""\n:\n {\n\n\n       \n""attributes""\n:\n {}\n,\n\n\n       \n""schema_url""\n:\n \n""""\n\n\n   }\n\n\n}\nSpans can be nested, as is implied by the presence of a parent span ID: child spans represent sub-operations. This allows spans to more accurately capture the work done in an application.\nTraces\nA trace records the paths taken by requests (made by an application or end-user) as they propagate through multiple steps.\nWithout tracing, it is challenging to pinpoint the cause of performance problems in a system.\nIt improves the visibility of our application or system’s health and lets us debug behavior that is difficult to reproduce locally. Tracing is essential for LLM applications, which commonly have nondeterministic problems or are too complicated to reproduce locally.\nTracing makes debugging and understanding LLM applications less daunting by breaking down what happens within a request as it flows through a system.\nA trace is made of one or more spans. The first span represents the root span. Each root span represents a request from start to finish. The spans underneath the parent provide a more in-depth context of what occurs during a request (or what steps make up a request).\nProjects\nA \nproject\n is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces. Projects can be useful for various use-cases such as separating out environments, logging traces for evaluation runs, etc. To learn more about how to setup projects,  see the \nhow-to guide.\nSpan Kind\nWhen a span is created,  it is created as one of the following: Chain, Retriever, Reranker, LLM, Embedding, Agent, or Tool. \nCHAIN\nA Chain is a starting point or a link between different LLM application steps. For example, a Chain span could be used to represent the beginning of a request to an LLM application or the glue code that passes context from a retriever to and LLM call.\nRETRIEVER\nA Retriever is a span that represents a data retrieval step. For example, a Retriever span could be used to represent a call to a vector store or a database.\nRERANKER\nA Reranker is a span that represents the reranking of a set of input documents. For example, a cross-encoder may be used to compute the input documents\' relevance scores with respect to a user query, and the top K documents with the highest scores are then returned by the Reranker.\nLLM\nAn LLM is a span that represents a call to an LLM. For example, an LLM span could be used to represent a call to OpenAI or Llama.\nEMBEDDING\nAn Embedding is a span that represents a call to an LLM for an embedding. For example, an Embedding span could be used to represent a call OpenAI to get an ada-2 embedding for retrieval.\nTOOL\nA Tool is a span that represents a call to an external tool such as a calculator or a weather API.\nAGENT\nA span that encompasses calls to LLMs and Tools. An agent describes a reasoning block that acts on tools using the guidance of an LLM.\n\nSpan Attributes\nAttributes are key-value pairs that contain metadata that you can use to annotate a span to carry information about the operation it is tracking.\nFor example, if a span invokes an LLM, you can capture the model name, the invocation parameters, the token count, and so on.\nAttributes have the following rules:\nKeys must be non-null string values\nValues must be a non-null string, boolean, floating point value, integer, or an array of these values Additionally, there are Semantic Attributes, which are known naming conventions for metadata that is typically present in common operations. It\'s helpful to use semantic attribute naming wherever possible so that common kinds of metadata are standardized across systems. See \nsemantic conventions\n for more information.\n\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nTraces\nA deep dive into the details of a trace\nSpans\nA span represents a unit of work or operation (think a \nspan\n of time). It tracks specific operations that a request makes, painting a picture of what happened during the time in which that operation was executed.\nA span contains name, time-related data, structured log messages, and other metadata (that is, Attributes) to provide information about the operation it tracks. A span for an LLM execution in JSON format is displayed below\nCopy\n{\n\n\n   \n""name""\n:\n \n""llm""\n,\n\n\n   \n""context""\n:\n {\n\n\n       \n""trace_id""\n:\n \n""0x6c80880dbeb609e2ed41e06a6397a0dd""\n,\n\n\n       \n""span_id""\n:\n \n""0xd9bdedf0df0b7208""\n,\n\n\n       \n""trace_state""\n:\n \n""[]""\n\n\n   }\n,\n\n\n   \n""kind""\n:\n \n""SpanKind.INTERNAL""\n,\n\n\n   \n""parent_id""\n:\n \n""0x7eb5df0046c77cd2""\n,\n\n\n   \n""start_time""\n:\n \n""2024-05-08T21:46:11.480777Z""\n,\n\n\n   \n""end_time""\n:\n \n""2024-05-08T21:46:35.368042Z""\n,\n\n\n   \n""status""\n:\n {\n\n\n       \n""status_code""\n:\n \n""OK""\n\n\n   }\n,\n\n\n   \n""attributes""\n:\n {\n\n\n       \n""openinference.span.kind""\n:\n \n""LLM""\n,\n\n\n       \n""llm.input_messages.0.message.role""\n:\n \n""system""\n,\n\n\n       ""llm.input_messages.0.message.content"": ""\\n  The following is a friendly conversation between a user and an AI assistant.\\n  The assistant is talkative and provides lots of specific details from its context.\\n  If the assistant does not know the answer to a question, it truthfully says it\\n  does not know.\\n\\n  Here are the relevant documents for the context:\\n\\n  page_label: 7\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.4Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\na. No piece may weigh more than 70 pounds.\\nb. The combined length and girth of a piece (the length of its longest side plus \\nthe distance around its thickest part) may not exceed 108 inches.\\nc. Lower size or weight standards apply to mail addressed to certain APOs and \\nFPOs, subject to 703.2.0  and 703.4.0  and for Department of State mail, \\nsubject to 703.3.0 .\\n\\npage_label: 6\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.2.10Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\na. The reply half of a double card must be used for reply only and may not be \\nused to convey a message to the original addressee or to send statements \\nof account. The reply half may be formatted for response purposes (e.g., contain blocks for completion by the addressee).\\nb. A double card must be folded before mailing and prepared so that the \\naddress on the reply half is on the inside when the double card is originally \\nmailed. The address side of the reply half may be prepared as Business \\nReply Mail, Courtesy Reply Mail, meter reply mail, or as a USPS Returns service label.\\nc. Plain stickers, seals, or a single wire stitch (staple) may be used to fasten the \\nopen edge at the top or bottom once the card is folded if affixed so that the \\ninner surfaces of the cards can be readily examined. Fasteners must be \\naffixed according to the applicable preparation requirements for the price claimed. Any sealing on the left and right sides of the cards, no matter the \\nsealing process used, is not permitted.\\nd. The first half of a double card must be detached when the reply half is \\nmailed for return. \\n6.2.10   Enclosures\\nEnclosures in double postcards are prohibited at card prices.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is a trace?\nAnswer: '}, {'role': 'assistant', 'content': 'A trace records the paths taken by requests as they move through multiple steps, providing a detailed account of the journey of a request from start to finish.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nNot more than 4-1/4 inches high, or more than 6 inches long, or greater than \\n0.016 inch thick.\\nd. Not more than 3.5 ounces (Charge flat-size prices for First-Class Mail \\ncard-type pieces over 3.5 ounces.)\\n\\n  Instruction: Based on the above documents, provide a detailed answer for the user question below.\\n  Answer \\""don\'t know\\"" if not present in the document.\\n  "",\n\n\n       \n""llm.input_messages.1.message.role""\n:\n \n""user""\n,\n\n\n       \n""llm.input_messages.1.message.content""\n:\n \n""Hello""\n,\n\n\n       \n""llm.model_name""\n:\n \n""gpt-4-turbo-preview""\n,\n\n\n       \n""llm.invocation_parameters""\n:\n \n""{\\""temperature\\"": 0.1, \\""model\\"": \\""gpt-4-turbo-preview\\""}""\n,\n\n\n       \n""output.value""\n:\n \n""How are you?""\n }\n,\n\n\n   \n""events""\n:\n []\n,\n\n\n   \n""links""\n:\n []\n,\n\n\n   \n""resource""\n:\n {\n\n\n       \n""attributes""\n:\n {}\n,\n\n\n       \n""schema_url""\n:\n \n""""\n\n\n   }\n\n\n}\nSpans can be nested, as is implied by the presence of a parent span ID: child spans represent sub-operations. This allows spans to more accurately capture the work done in an application.\nTraces\nA trace records the paths taken by requests (made by an application or end-user) as they propagate through multiple steps.\nWithout tracing, it is challenging to pinpoint the cause of performance problems in a system.\nIt improves the visibility of our application or system’s health and lets us debug behavior that is difficult to reproduce locally. Tracing is essential for LLM applications, which commonly have nondeterministic problems or are too complicated to reproduce locally.\nTracing makes debugging and understanding LLM applications less daunting by breaking down what happens within a request as it flows through a system.\nA trace is made of one or more spans. The first span represents the root span. Each root span represents a request from start to finish. The spans underneath the parent provide a more in-depth context of what occurs during a request (or what steps make up a request).\nProjects\nA \nproject\n is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces. Projects can be useful for various use-cases such as separating out environments, logging traces for evaluation runs, etc. To learn more about how to setup projects,  see the \nhow-to guide.\nSpan Kind\nWhen a span is created,  it is created as one of the following: Chain, Retriever, Reranker, LLM, Embedding, Agent, or Tool. \nCHAIN\nA Chain is a starting point or a link between different LLM application steps. For example, a Chain span could be used to represent the beginning of a request to an LLM application or the glue code that passes context from a retriever to and LLM call.\nRETRIEVER\nA Retriever is a span that represents a data retrieval step. For example, a Retriever span could be used to represent a call to a vector store or a database.\nRERANKER\nA Reranker is a span that represents the reranking of a set of input documents. For example, a cross-encoder may be used to compute the input documents\' relevance scores with respect to a user query, and the top K documents with the highest scores are then returned by the Reranker.\nLLM\nAn LLM is a span that represents a call to an LLM. For example, an LLM span could be used to represent a call to OpenAI or Llama.\nEMBEDDING\nAn Embedding is a span that represents a call to an LLM for an embedding. For example, an Embedding span could be used to represent a call OpenAI to get an ada-2 embedding for retrieval.\nTOOL\nA Tool is a span that represents a call to an external tool such as a calculator or a weather API.\nAGENT\nA span that encompasses calls to LLMs and Tools. An agent describes a reasoning block that acts on tools using the guidance of an LLM.\n\nSpan Attributes\nAttributes are key-value pairs that contain metadata that you can use to annotate a span to carry information about the operation it is tracking.\nFor example, if a span invokes an LLM, you can capture the model name, the invocation parameters, the token count, and so on.\nAttributes have the following rules:\nKeys must be non-null string values\nValues must be a non-null string, boolean, floating point value, integer, or an array of these values Additionally, there are Semantic Attributes, which are known naming conventions for metadata that is typically present in common operations. It\'s helpful to use semantic attribute naming wherever possible so that common kinds of metadata are standardized across systems. See \nsemantic conventions\n for more information.\n\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nTraces\nA deep dive into the details of a trace\nSpans\nA span represents a unit of work or operation (think a \nspan\n of time). It tracks specific operations that a request makes, painting a picture of what happened during the time in which that operation was executed.\nA span contains name, time-related data, structured log messages, and other metadata (that is, Attributes) to provide information about the operation it tracks. A span for an LLM execution in JSON format is displayed below\nCopy\n{\n\n\n   \n""name""\n:\n \n""llm""\n,\n\n\n   \n""context""\n:\n {\n\n\n       \n""trace_id""\n:\n \n""0x6c80880dbeb609e2ed41e06a6397a0dd""\n,\n\n\n       \n""span_id""\n:\n \n""0xd9bdedf0df0b7208""\n,\n\n\n       \n""trace_state""\n:\n \n""[]""\n\n\n   }\n,\n\n\n   \n""kind""\n:\n \n""SpanKind.INTERNAL""\n,\n\n\n   \n""parent_id""\n:\n \n""0x7eb5df0046c77cd2""\n,\n\n\n   \n""start_time""\n:\n \n""2024-05-08T21:46:11.480777Z""\n,\n\n\n   \n""end_time""\n:\n \n""2024-05-08T21:46:35.368042Z""\n,\n\n\n   \n""status""\n:\n {\n\n\n       \n""status_code""\n:\n \n""OK""\n\n\n   }\n,\n\n\n   \n""attributes""\n:\n {\n\n\n       \n""openinference.span.kind""\n:\n \n""LLM""\n,\n\n\n       \n""llm.input_messages.0.message.role""\n:\n \n""system""\n,\n\n\n       ""llm.input_messages.0.message.content"": ""\\n  The following is a friendly conversation between a user and an AI assistant.\\n  The assistant is talkative and provides lots of specific details from its context.\\n  If the assistant does not know the answer to a question, it truthfully says it\\n  does not know.\\n\\n  Here are the relevant documents for the context:\\n\\n  page_label: 7\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.4Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\na. No piece may weigh more than 70 pounds.\\nb. The combined length and girth of a piece (the length of its longest side plus \\nthe distance around its thickest part) may not exceed 108 inches.\\nc. Lower size or weight standards apply to mail addressed to certain APOs and \\nFPOs, subject to 703.2.0  and 703.4.0  and for Department of State mail, \\nsubject to 703.3.0 .\\n\\npage_label: 6\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.2.10Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\na. The reply half of a double card must be used for reply only and may not be \\nused to convey a message to the original addressee or to send statements \\nof account. The reply half may be formatted for response purposes (e.g., contain blocks for completion by the addressee).\\nb. A double card must be folded before mailing and prepared so that the \\naddress on the reply half is on the inside when the double card is originally \\nmailed. The address side of the reply half may be prepared as Business \\nReply Mail, Courtesy Reply Mail, meter reply mail, or as a USPS Returns service label.\\nc. Plain stickers, seals, or a single wire stitch (staple) may be used to fasten the \\nopen edge at the top or bottom once the card is folded if affixed so that the \\ninner surfaces of the cards can be readily examined. Fasteners must be \\naffixed according to the applicable preparation requirements for the price claimed. Any sealing on the left and right sides of the cards, no matter the \\nsealing process used, is not permitted.\\nd. The first half of a double card must be detached when the reply half is \\nmailed for return. \\n6.2.10   Enclosures\\nEnclosures in double postcards are prohibited at card prices.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is a trace?\nAnswer: '}, {'role': 'assistant', 'content': 'A trace records the paths taken by requests as they move through multiple steps, providing a detailed account of the journey of a request from start to finish.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MTg=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/customize-spans/instrumenting-prompt-templates-and-prompt-variables\ntitle: Instrumenting Prompt Templates and Prompt Variables\n\nInstrumenting Prompt Templates and Prompt Variables\nInstrumenting prompt templates and variables allows you to track and visualize prompt changes. These can also be combined with \nExperiments\n to measure the performance changes driven by each of your prompts.\nPython\nWe provide a \nusing_prompt_template\n context manager to add a prompt template (including its version and variables) to the current OpenTelemetry Context. OpenInference \nauto-instrumentors\n will read this Context and pass the prompt template fields as span attributes, following the OpenInference \nsemantic conventions\n. Its inputs must be of the following type:\nTemplate: non-empty string.\nVersion: non-empty string.\nVariables: a dictionary with string keys. This dictionary will be serialized to JSON when saved to the OTEL Context and remain a JSON string when sent as a span attribute.\nCopy\nfrom\n openinference\n.\ninstrumentation \nimport\n using_prompt_template\n\n\n\n\nprompt_template \n=\n \n""Please describe the weather forecast for \n{city}\n on \n{date}\n""\n\n\nprompt_template_variables \n=\n \n{\n""city""\n:\n \n""Johannesburg""\n,\n \n""date""\n:\n""July 11""\n}\n\n\nwith\n \nusing_prompt_template\n(\n\n\n    template\n=\nprompt_template,\n\n\n    variables\n=\nprompt_template_variables,\n\n\n    version\n=\n""v1.0""\n,\n\n\n    ):\n\n\n    \n# Commonly preceeds a chat completion to append templates to auto instrumentation\n\n\n    \n# response = client.chat.completions.create()\n\n\n    \n# Calls within this block will generate spans with the attributes:\n\n\n    \n# ""llm.prompt_template.template"" = ""Please describe the weather forecast for {city} on {date}""\n\n\n    \n# ""llm.prompt_template.version"" = ""v1.0""\n\n\n    \n# ""llm.prompt_template.variables"" = ""{\\""city\\"": \\""Johannesburg\\"", \\""date\\"": \\""July 11\\""}"" # JSON serialized\n\n\n    ...\nIt can also be used as a decorator:\nCopy\n@using_prompt_template\n(\n\n\n    template\n=\nprompt_template,\n\n\n    variables\n=\nprompt_template_variables,\n\n\n    version\n=\n""v1.0""\n,\n\n\n)\n\n\ndef\n \ncall_fn\n(\n*\nargs\n,\n \n**\nkwargs\n):\n\n\n    \n# Calls within this function will generate spans with the attributes:\n\n\n    \n# ""llm.prompt_template.template"" = ""Please describe the weather forecast for {city} on {date}""\n\n\n    \n# ""llm.prompt_template.version"" = ""v1.0""\n\n\n    \n# ""llm.prompt_template.variables"" = ""{\\""city\\"": \\""Johannesburg\\"", \\""date\\"": \\""July 11\\""}"" # JSON serialized\n\n\n    ...\nPrevious\nCustomize Spans\nNext\nAuto Instrumentation\nLast updated \n22 days ago\n\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing/prompt-flow\ntitle: Prompt flow\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n()\n\n\nspan_exporter \n=\n \nOTLPSpanExporter\n(\n""http://localhost:6006/v1/traces""\n)\n\n\nspan_processor \n=\n \nSimpleSpanProcessor\n(span_exporter)\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor)\n\n\ntrace_api\n.\nset_tracer_provider\n(tracer_provider)\nFor more info on using Phoenix with Docker, see \nDocker\nIf you don\'t want to host an instance of Phoenix yourself or use a notebook instance, you can use a persistent instance provided on our site. Sign up for an Arize Phoenix account at\nhttps://app.phoenix.arize.com/login\nInstall packages:\nCopy\npip\n \ninstall\n \nopentelemetry-sdk\n \nopentelemetry-exporter-otlp\nConnect your application to your cloud instance:\nCopy\nimport\n os\n\n\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\nresources \nimport\n Resource\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\ngrpc\n.\ntrace_exporter \nimport\n (\n\n\n    OTLPSpanExporter \nas\n GRPCSpanExporter\n,\n\n\n)\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n (\n\n\n    OTLPSpanExporter \nas\n HTTPSpanExporter\n,\n\n\n)\n\n\n\n\n# Add Phoenix API Key for tracing\n\n\nos\n.\nenviron\n[\n""OTEL_EXPORTER_OTLP_HEADERS""\n]\n \n=\n f\n""api_key=\n{PHOENIX_API_KEY}\n""\n\n\n\n\n# Add Phoenix\n\n\nspan_phoenix_processor \n=\n \nSimpleSpanProcessor\n(\nHTTPSpanExporter\n(endpoint\n=\n""https://app.phoenix.arize.com/v1/traces""\n))\n\n\n\n\n# Add them to the tracer\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n()\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor\n=\nspan_phoenix_processor)\n\n\ntrace_api\n.\nset_tracer_provider\n(tracer_provider\n=\ntracer_provider)\nYour \nPhoenix API key\n can be found on the Keys section of your \ndashboard\n.\nInstall\nCopy\npip\n \ninstall\n \npromptflow\nSetup\nSet up the OpenTelemetry endpoint to point to Phoenix and use Prompt flow\'s \nsetup_exporter_from_environ\n to start tracing any further flows and LLM calls.\nCopy\nimport\n os\n\n\nfrom\n opentelemetry\n.\nsdk\n.\nenvironment_variables \nimport\n OTEL_EXPORTER_OTLP_ENDPOINT\n\n\nfrom\n promptflow\n.\ntracing\n.\n_start_trace \nimport\n setup_exporter_from_environ\n\n\n\n\nendpoint \n=\n f\n""http://127.0.0.1:6006/v1/traces""\n \n# replace with your Phoenix endpoint if self-hosting\n\n\nos\n.\nenviron\n[\nOTEL_EXPORTER_OTLP_ENDPOINT\n]\n \n=\n endpoint\n\n\nsetup_exporter_from_environ\n()\nRun PromptFlow\nProceed with creating Prompt flow flows as usual. See this \nexample notebook\n for inspiration.\nObserve\nYou should see the spans render in Phoenix as shown in the below screenshots.\nResources\nExample Notebook\nPrevious\nAutoGen\nNext\nInstructor\nLast updated \n15 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are prompts?\nAnswer: '}, {'role': 'assistant', 'content': 'Prompts are templates that can be instrumented with variables to track and visualize changes. They are used to measure performance changes driven by each prompt and can be combined with experiments for analysis. Prompts consist of a template (a non-empty string), a version (a non-empty string), and variables (a dictionary with string keys). These prompts are added to the OpenTelemetry Context using a context manager or decorator, allowing for the generation of spans with specific attributes related to the prompt template, version, and variables.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/customize-spans/instrumenting-prompt-templates-and-prompt-variables\ntitle: Instrumenting Prompt Templates and Prompt Variables\n\nInstrumenting Prompt Templates and Prompt Variables\nInstrumenting prompt templates and variables allows you to track and visualize prompt changes. These can also be combined with \nExperiments\n to measure the performance changes driven by each of your prompts.\nPython\nWe provide a \nusing_prompt_template\n context manager to add a prompt template (including its version and variables) to the current OpenTelemetry Context. OpenInference \nauto-instrumentors\n will read this Context and pass the prompt template fields as span attributes, following the OpenInference \nsemantic conventions\n. Its inputs must be of the following type:\nTemplate: non-empty string.\nVersion: non-empty string.\nVariables: a dictionary with string keys. This dictionary will be serialized to JSON when saved to the OTEL Context and remain a JSON string when sent as a span attribute.\nCopy\nfrom\n openinference\n.\ninstrumentation \nimport\n using_prompt_template\n\n\n\n\nprompt_template \n=\n \n""Please describe the weather forecast for \n{city}\n on \n{date}\n""\n\n\nprompt_template_variables \n=\n \n{\n""city""\n:\n \n""Johannesburg""\n,\n \n""date""\n:\n""July 11""\n}\n\n\nwith\n \nusing_prompt_template\n(\n\n\n    template\n=\nprompt_template,\n\n\n    variables\n=\nprompt_template_variables,\n\n\n    version\n=\n""v1.0""\n,\n\n\n    ):\n\n\n    \n# Commonly preceeds a chat completion to append templates to auto instrumentation\n\n\n    \n# response = client.chat.completions.create()\n\n\n    \n# Calls within this block will generate spans with the attributes:\n\n\n    \n# ""llm.prompt_template.template"" = ""Please describe the weather forecast for {city} on {date}""\n\n\n    \n# ""llm.prompt_template.version"" = ""v1.0""\n\n\n    \n# ""llm.prompt_template.variables"" = ""{\\""city\\"": \\""Johannesburg\\"", \\""date\\"": \\""July 11\\""}"" # JSON serialized\n\n\n    ...\nIt can also be used as a decorator:\nCopy\n@using_prompt_template\n(\n\n\n    template\n=\nprompt_template,\n\n\n    variables\n=\nprompt_template_variables,\n\n\n    version\n=\n""v1.0""\n,\n\n\n)\n\n\ndef\n \ncall_fn\n(\n*\nargs\n,\n \n**\nkwargs\n):\n\n\n    \n# Calls within this function will generate spans with the attributes:\n\n\n    \n# ""llm.prompt_template.template"" = ""Please describe the weather forecast for {city} on {date}""\n\n\n    \n# ""llm.prompt_template.version"" = ""v1.0""\n\n\n    \n# ""llm.prompt_template.variables"" = ""{\\""city\\"": \\""Johannesburg\\"", \\""date\\"": \\""July 11\\""}"" # JSON serialized\n\n\n    ...\nPrevious\nCustomize Spans\nNext\nAuto Instrumentation\nLast updated \n22 days ago\n\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing/prompt-flow\ntitle: Prompt flow\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n()\n\n\nspan_exporter \n=\n \nOTLPSpanExporter\n(\n""http://localhost:6006/v1/traces""\n)\n\n\nspan_processor \n=\n \nSimpleSpanProcessor\n(span_exporter)\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor)\n\n\ntrace_api\n.\nset_tracer_provider\n(tracer_provider)\nFor more info on using Phoenix with Docker, see \nDocker\nIf you don\'t want to host an instance of Phoenix yourself or use a notebook instance, you can use a persistent instance provided on our site. Sign up for an Arize Phoenix account at\nhttps://app.phoenix.arize.com/login\nInstall packages:\nCopy\npip\n \ninstall\n \nopentelemetry-sdk\n \nopentelemetry-exporter-otlp\nConnect your application to your cloud instance:\nCopy\nimport\n os\n\n\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\nresources \nimport\n Resource\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\ngrpc\n.\ntrace_exporter \nimport\n (\n\n\n    OTLPSpanExporter \nas\n GRPCSpanExporter\n,\n\n\n)\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n (\n\n\n    OTLPSpanExporter \nas\n HTTPSpanExporter\n,\n\n\n)\n\n\n\n\n# Add Phoenix API Key for tracing\n\n\nos\n.\nenviron\n[\n""OTEL_EXPORTER_OTLP_HEADERS""\n]\n \n=\n f\n""api_key=\n{PHOENIX_API_KEY}\n""\n\n\n\n\n# Add Phoenix\n\n\nspan_phoenix_processor \n=\n \nSimpleSpanProcessor\n(\nHTTPSpanExporter\n(endpoint\n=\n""https://app.phoenix.arize.com/v1/traces""\n))\n\n\n\n\n# Add them to the tracer\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n()\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor\n=\nspan_phoenix_processor)\n\n\ntrace_api\n.\nset_tracer_provider\n(tracer_provider\n=\ntracer_provider)\nYour \nPhoenix API key\n can be found on the Keys section of your \ndashboard\n.\nInstall\nCopy\npip\n \ninstall\n \npromptflow\nSetup\nSet up the OpenTelemetry endpoint to point to Phoenix and use Prompt flow\'s \nsetup_exporter_from_environ\n to start tracing any further flows and LLM calls.\nCopy\nimport\n os\n\n\nfrom\n opentelemetry\n.\nsdk\n.\nenvironment_variables \nimport\n OTEL_EXPORTER_OTLP_ENDPOINT\n\n\nfrom\n promptflow\n.\ntracing\n.\n_start_trace \nimport\n setup_exporter_from_environ\n\n\n\n\nendpoint \n=\n f\n""http://127.0.0.1:6006/v1/traces""\n \n# replace with your Phoenix endpoint if self-hosting\n\n\nos\n.\nenviron\n[\nOTEL_EXPORTER_OTLP_ENDPOINT\n]\n \n=\n endpoint\n\n\nsetup_exporter_from_environ\n()\nRun PromptFlow\nProceed with creating Prompt flow flows as usual. See this \nexample notebook\n for inspiration.\nObserve\nYou should see the spans render in Phoenix as shown in the below screenshots.\nResources\nExample Notebook\nPrevious\nAutoGen\nNext\nInstructor\nLast updated \n15 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are prompts?\nAnswer: '}, {'role': 'assistant', 'content': 'Prompts are templates that can be instrumented with variables to track and visualize changes. They are used to measure performance changes driven by each prompt and can be combined with experiments for analysis. Prompts consist of a template (a non-empty string), a version (a non-empty string), and variables (a dictionary with string keys). These prompts are added to the OpenTelemetry Context using a context manager or decorator, allowing for the generation of spans with specific attributes related to the prompt template, version, and variables.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MTk=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/customize-spans/instrumenting-prompt-templates-and-prompt-variables\ntitle: Instrumenting Prompt Templates and Prompt Variables\n\nInstrumenting Prompt Templates and Prompt Variables\nInstrumenting prompt templates and variables allows you to track and visualize prompt changes. These can also be combined with \nExperiments\n to measure the performance changes driven by each of your prompts.\nPython\nWe provide a \nusing_prompt_template\n context manager to add a prompt template (including its version and variables) to the current OpenTelemetry Context. OpenInference \nauto-instrumentors\n will read this Context and pass the prompt template fields as span attributes, following the OpenInference \nsemantic conventions\n. Its inputs must be of the following type:\nTemplate: non-empty string.\nVersion: non-empty string.\nVariables: a dictionary with string keys. This dictionary will be serialized to JSON when saved to the OTEL Context and remain a JSON string when sent as a span attribute.\nCopy\nfrom\n openinference\n.\ninstrumentation \nimport\n using_prompt_template\n\n\n\n\nprompt_template \n=\n \n""Please describe the weather forecast for \n{city}\n on \n{date}\n""\n\n\nprompt_template_variables \n=\n \n{\n""city""\n:\n \n""Johannesburg""\n,\n \n""date""\n:\n""July 11""\n}\n\n\nwith\n \nusing_prompt_template\n(\n\n\n    template\n=\nprompt_template,\n\n\n    variables\n=\nprompt_template_variables,\n\n\n    version\n=\n""v1.0""\n,\n\n\n    ):\n\n\n    \n# Commonly preceeds a chat completion to append templates to auto instrumentation\n\n\n    \n# response = client.chat.completions.create()\n\n\n    \n# Calls within this block will generate spans with the attributes:\n\n\n    \n# ""llm.prompt_template.template"" = ""Please describe the weather forecast for {city} on {date}""\n\n\n    \n# ""llm.prompt_template.version"" = ""v1.0""\n\n\n    \n# ""llm.prompt_template.variables"" = ""{\\""city\\"": \\""Johannesburg\\"", \\""date\\"": \\""July 11\\""}"" # JSON serialized\n\n\n    ...\nIt can also be used as a decorator:\nCopy\n@using_prompt_template\n(\n\n\n    template\n=\nprompt_template,\n\n\n    variables\n=\nprompt_template_variables,\n\n\n    version\n=\n""v1.0""\n,\n\n\n)\n\n\ndef\n \ncall_fn\n(\n*\nargs\n,\n \n**\nkwargs\n):\n\n\n    \n# Calls within this function will generate spans with the attributes:\n\n\n    \n# ""llm.prompt_template.template"" = ""Please describe the weather forecast for {city} on {date}""\n\n\n    \n# ""llm.prompt_template.version"" = ""v1.0""\n\n\n    \n# ""llm.prompt_template.variables"" = ""{\\""city\\"": \\""Johannesburg\\"", \\""date\\"": \\""July 11\\""}"" # JSON serialized\n\n\n    ...\nPrevious\nCustomize Spans\nNext\nAuto Instrumentation\nLast updated \n22 days ago\n\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals/hallucinations\ntitle: Hallucinations\n\nHallucinations\nWhen To Use Hallucination Eval Template\nThis LLM Eval detects if the output of a model is a hallucination based on contextual data.\nThis Eval is specifically designed to detect hallucinations in generated answers from private or retrieved data. The Eval detects if an AI answer to a question is a hallucination based on the reference data used to generate the answer.\nThis Eval is designed to check for hallucinations on private data, on data that is fed into the context window from retrieval.\nIt is NOT designed to check hallucinations on what the LLM was trained on. It is not useful for random public fact hallucinations ""What was Michael Jordan\'s birthday?""\nIt is useful for hallucinations in RAG systems\nHallucination Eval Template\nCopy\nIn this task, you will be presented with a query, a reference text and an answer. The answer is\n\n\ngenerated to the question based on the reference text. The answer may contain false information. You\n\n\nmust use the reference text to determine if the answer to the question contains false information,\n\n\nif the answer is a hallucination of facts. Your objective is to determine whether the answer text\n\n\ncontains factual information and is not a hallucination. A \'hallucination\' refers to\n\n\nan answer that is not based on the reference text or assumes information that is not available in\n\n\nthe reference text. Your response should be a single word: either ""factual"" or ""hallucinated"", and\n\n\nit should not include any other text or characters. ""hallucinated"" indicates that the answer\n\n\nprovides factually inaccurate information to the query based on the reference text. ""factual""\n\n\nindicates that the answer to the question is correct relative to the reference text, and does not\n\n\ncontain made up information. Please read the query and reference text carefully before determining\n\n\nyour response.\n\n\n\n\n    # Query: {query}\n\n\n    # Reference text: {reference}\n\n\n    # Answer: {response}\n\n\n    Is the answer above factual or hallucinated based on the query and reference text?\nWe are continually iterating our templates, view the most up-to-date template on GitHub. Last updated on 10/12/2023\nBenchmark Results\nGPT-4 Results\nGPT-3.5 Results\nClaud v2 Results\nGPT-4 Turbo\nHow To Run the Eval\nCopy\nfrom\n phoenix\n.\nevals \nimport\n (\n\n\n    HALLUCINATION_PROMPT_RAILS_MAP\n,\n\n\n    HALLUCINATION_PROMPT_TEMPLATE\n,\n\n\n    OpenAIModel\n,\n\n\n    download_benchmark_dataset\n,\n\n\n    llm_classify\n,\n\n\n)\n\n\n\n\nmodel \n=\n \nOpenAIModel\n(\n\n\n    model_name\n=\n""gpt-4""\n,\n\n\n    temperature\n=\n0.0\n,\n\n\n)\n\n\n\n\n#The rails is used to hold the output to specific values based on the template\n\n\n#It will remove text such as "",,,"" or ""...""\n\n\n#Will ensure the binary value expected from the template is returned \n\n\nrails \n=\n \nlist\n(HALLUCINATION_PROMPT_RAILS_MAP.\nvalues\n())\n\n\nhallucination_classifications \n=\n \nllm_classify\n(\n\n\n    dataframe\n=\ndf, \n\n\n    template\n=\nHALLUCINATION_PROMPT_TEMPLATE, \n\n\n    model\n=\nmodel, \n\n\n    rails\n=\nrails,\n\n\n    provide_explanation\n=\nTrue\n, \n#optional to generate explanations for the value produced by the eval LLM\n\n\n)\n\n\nThe above Eval shows how to the the hallucination template for Eval detection.\nEval\nGPT-4\nGPT-4 Turbo\nGemini Pro\nGPT-3.5\nGPT-3.5-turbo-instruct\nPalm 2 (Text Bison)\nClaude V2\nPrecision\n0.93\n0.97\n0.89\n0.89\n0.89\n1\n0.80\nRecall\n0.72\n0.70\n0.53\n0.65\n0.80\n0.44\n0.95\nF1\n0.82\n0.81\n0.67\n0.75\n0.84\n0.61\n0.87\nThroughput\nGPT-4\nGPT-4 Turbo\nGPT-3.5\n100 Samples\n105 sec\n58 Sec\n52 Sec\nPrevious\nUse Phoenix Evaluators\nNext\nQ&A on Retrieved Data\nLast updated \n3 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are prompt templates?\nAnswer: '}, {'role': 'assistant', 'content': 'Prompt templates are used to track and visualize prompt changes. They can be combined with Experiments to measure the performance changes driven by each prompt. Prompt templates consist of a template (non-empty string), version (non-empty string), and variables (a dictionary with string keys). These templates are serialized to JSON when saved to the OTEL Context and remain a JSON string when sent as a span attribute.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/customize-spans/instrumenting-prompt-templates-and-prompt-variables\ntitle: Instrumenting Prompt Templates and Prompt Variables\n\nInstrumenting Prompt Templates and Prompt Variables\nInstrumenting prompt templates and variables allows you to track and visualize prompt changes. These can also be combined with \nExperiments\n to measure the performance changes driven by each of your prompts.\nPython\nWe provide a \nusing_prompt_template\n context manager to add a prompt template (including its version and variables) to the current OpenTelemetry Context. OpenInference \nauto-instrumentors\n will read this Context and pass the prompt template fields as span attributes, following the OpenInference \nsemantic conventions\n. Its inputs must be of the following type:\nTemplate: non-empty string.\nVersion: non-empty string.\nVariables: a dictionary with string keys. This dictionary will be serialized to JSON when saved to the OTEL Context and remain a JSON string when sent as a span attribute.\nCopy\nfrom\n openinference\n.\ninstrumentation \nimport\n using_prompt_template\n\n\n\n\nprompt_template \n=\n \n""Please describe the weather forecast for \n{city}\n on \n{date}\n""\n\n\nprompt_template_variables \n=\n \n{\n""city""\n:\n \n""Johannesburg""\n,\n \n""date""\n:\n""July 11""\n}\n\n\nwith\n \nusing_prompt_template\n(\n\n\n    template\n=\nprompt_template,\n\n\n    variables\n=\nprompt_template_variables,\n\n\n    version\n=\n""v1.0""\n,\n\n\n    ):\n\n\n    \n# Commonly preceeds a chat completion to append templates to auto instrumentation\n\n\n    \n# response = client.chat.completions.create()\n\n\n    \n# Calls within this block will generate spans with the attributes:\n\n\n    \n# ""llm.prompt_template.template"" = ""Please describe the weather forecast for {city} on {date}""\n\n\n    \n# ""llm.prompt_template.version"" = ""v1.0""\n\n\n    \n# ""llm.prompt_template.variables"" = ""{\\""city\\"": \\""Johannesburg\\"", \\""date\\"": \\""July 11\\""}"" # JSON serialized\n\n\n    ...\nIt can also be used as a decorator:\nCopy\n@using_prompt_template\n(\n\n\n    template\n=\nprompt_template,\n\n\n    variables\n=\nprompt_template_variables,\n\n\n    version\n=\n""v1.0""\n,\n\n\n)\n\n\ndef\n \ncall_fn\n(\n*\nargs\n,\n \n**\nkwargs\n):\n\n\n    \n# Calls within this function will generate spans with the attributes:\n\n\n    \n# ""llm.prompt_template.template"" = ""Please describe the weather forecast for {city} on {date}""\n\n\n    \n# ""llm.prompt_template.version"" = ""v1.0""\n\n\n    \n# ""llm.prompt_template.variables"" = ""{\\""city\\"": \\""Johannesburg\\"", \\""date\\"": \\""July 11\\""}"" # JSON serialized\n\n\n    ...\nPrevious\nCustomize Spans\nNext\nAuto Instrumentation\nLast updated \n22 days ago\n\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals/hallucinations\ntitle: Hallucinations\n\nHallucinations\nWhen To Use Hallucination Eval Template\nThis LLM Eval detects if the output of a model is a hallucination based on contextual data.\nThis Eval is specifically designed to detect hallucinations in generated answers from private or retrieved data. The Eval detects if an AI answer to a question is a hallucination based on the reference data used to generate the answer.\nThis Eval is designed to check for hallucinations on private data, on data that is fed into the context window from retrieval.\nIt is NOT designed to check hallucinations on what the LLM was trained on. It is not useful for random public fact hallucinations ""What was Michael Jordan\'s birthday?""\nIt is useful for hallucinations in RAG systems\nHallucination Eval Template\nCopy\nIn this task, you will be presented with a query, a reference text and an answer. The answer is\n\n\ngenerated to the question based on the reference text. The answer may contain false information. You\n\n\nmust use the reference text to determine if the answer to the question contains false information,\n\n\nif the answer is a hallucination of facts. Your objective is to determine whether the answer text\n\n\ncontains factual information and is not a hallucination. A \'hallucination\' refers to\n\n\nan answer that is not based on the reference text or assumes information that is not available in\n\n\nthe reference text. Your response should be a single word: either ""factual"" or ""hallucinated"", and\n\n\nit should not include any other text or characters. ""hallucinated"" indicates that the answer\n\n\nprovides factually inaccurate information to the query based on the reference text. ""factual""\n\n\nindicates that the answer to the question is correct relative to the reference text, and does not\n\n\ncontain made up information. Please read the query and reference text carefully before determining\n\n\nyour response.\n\n\n\n\n    # Query: {query}\n\n\n    # Reference text: {reference}\n\n\n    # Answer: {response}\n\n\n    Is the answer above factual or hallucinated based on the query and reference text?\nWe are continually iterating our templates, view the most up-to-date template on GitHub. Last updated on 10/12/2023\nBenchmark Results\nGPT-4 Results\nGPT-3.5 Results\nClaud v2 Results\nGPT-4 Turbo\nHow To Run the Eval\nCopy\nfrom\n phoenix\n.\nevals \nimport\n (\n\n\n    HALLUCINATION_PROMPT_RAILS_MAP\n,\n\n\n    HALLUCINATION_PROMPT_TEMPLATE\n,\n\n\n    OpenAIModel\n,\n\n\n    download_benchmark_dataset\n,\n\n\n    llm_classify\n,\n\n\n)\n\n\n\n\nmodel \n=\n \nOpenAIModel\n(\n\n\n    model_name\n=\n""gpt-4""\n,\n\n\n    temperature\n=\n0.0\n,\n\n\n)\n\n\n\n\n#The rails is used to hold the output to specific values based on the template\n\n\n#It will remove text such as "",,,"" or ""...""\n\n\n#Will ensure the binary value expected from the template is returned \n\n\nrails \n=\n \nlist\n(HALLUCINATION_PROMPT_RAILS_MAP.\nvalues\n())\n\n\nhallucination_classifications \n=\n \nllm_classify\n(\n\n\n    dataframe\n=\ndf, \n\n\n    template\n=\nHALLUCINATION_PROMPT_TEMPLATE, \n\n\n    model\n=\nmodel, \n\n\n    rails\n=\nrails,\n\n\n    provide_explanation\n=\nTrue\n, \n#optional to generate explanations for the value produced by the eval LLM\n\n\n)\n\n\nThe above Eval shows how to the the hallucination template for Eval detection.\nEval\nGPT-4\nGPT-4 Turbo\nGemini Pro\nGPT-3.5\nGPT-3.5-turbo-instruct\nPalm 2 (Text Bison)\nClaude V2\nPrecision\n0.93\n0.97\n0.89\n0.89\n0.89\n1\n0.80\nRecall\n0.72\n0.70\n0.53\n0.65\n0.80\n0.44\n0.95\nF1\n0.82\n0.81\n0.67\n0.75\n0.84\n0.61\n0.87\nThroughput\nGPT-4\nGPT-4 Turbo\nGPT-3.5\n100 Samples\n105 sec\n58 Sec\n52 Sec\nPrevious\nUse Phoenix Evaluators\nNext\nQ&A on Retrieved Data\nLast updated \n3 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are prompt templates?\nAnswer: '}, {'role': 'assistant', 'content': 'Prompt templates are used to track and visualize prompt changes. They can be combined with Experiments to measure the performance changes driven by each prompt. Prompt templates consist of a template (non-empty string), version (non-empty string), and variables (a dictionary with string keys). These templates are serialized to JSON when saved to the OTEL Context and remain a JSON string when sent as a span attribute.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MjA=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/customize-spans/instrumenting-prompt-templates-and-prompt-variables\ntitle: Instrumenting Prompt Templates and Prompt Variables\n\nInstrumenting Prompt Templates and Prompt Variables\nInstrumenting prompt templates and variables allows you to track and visualize prompt changes. These can also be combined with \nExperiments\n to measure the performance changes driven by each of your prompts.\nPython\nWe provide a \nusing_prompt_template\n context manager to add a prompt template (including its version and variables) to the current OpenTelemetry Context. OpenInference \nauto-instrumentors\n will read this Context and pass the prompt template fields as span attributes, following the OpenInference \nsemantic conventions\n. Its inputs must be of the following type:\nTemplate: non-empty string.\nVersion: non-empty string.\nVariables: a dictionary with string keys. This dictionary will be serialized to JSON when saved to the OTEL Context and remain a JSON string when sent as a span attribute.\nCopy\nfrom\n openinference\n.\ninstrumentation \nimport\n using_prompt_template\n\n\n\n\nprompt_template \n=\n \n""Please describe the weather forecast for \n{city}\n on \n{date}\n""\n\n\nprompt_template_variables \n=\n \n{\n""city""\n:\n \n""Johannesburg""\n,\n \n""date""\n:\n""July 11""\n}\n\n\nwith\n \nusing_prompt_template\n(\n\n\n    template\n=\nprompt_template,\n\n\n    variables\n=\nprompt_template_variables,\n\n\n    version\n=\n""v1.0""\n,\n\n\n    ):\n\n\n    \n# Commonly preceeds a chat completion to append templates to auto instrumentation\n\n\n    \n# response = client.chat.completions.create()\n\n\n    \n# Calls within this block will generate spans with the attributes:\n\n\n    \n# ""llm.prompt_template.template"" = ""Please describe the weather forecast for {city} on {date}""\n\n\n    \n# ""llm.prompt_template.version"" = ""v1.0""\n\n\n    \n# ""llm.prompt_template.variables"" = ""{\\""city\\"": \\""Johannesburg\\"", \\""date\\"": \\""July 11\\""}"" # JSON serialized\n\n\n    ...\nIt can also be used as a decorator:\nCopy\n@using_prompt_template\n(\n\n\n    template\n=\nprompt_template,\n\n\n    variables\n=\nprompt_template_variables,\n\n\n    version\n=\n""v1.0""\n,\n\n\n)\n\n\ndef\n \ncall_fn\n(\n*\nargs\n,\n \n**\nkwargs\n):\n\n\n    \n# Calls within this function will generate spans with the attributes:\n\n\n    \n# ""llm.prompt_template.template"" = ""Please describe the weather forecast for {city} on {date}""\n\n\n    \n# ""llm.prompt_template.version"" = ""v1.0""\n\n\n    \n# ""llm.prompt_template.variables"" = ""{\\""city\\"": \\""Johannesburg\\"", \\""date\\"": \\""July 11\\""}"" # JSON serialized\n\n\n    ...\nPrevious\nCustomize Spans\nNext\nAuto Instrumentation\nLast updated \n22 days ago\n\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix\'s tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you\'ll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they\'re geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix\'s search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix\'s flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix\'s evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is a prompt variable?\nAnswer: '}, {'role': 'assistant', 'content': 'A prompt variable is a key-value pair used to define specific values within a prompt template.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/customize-spans/instrumenting-prompt-templates-and-prompt-variables\ntitle: Instrumenting Prompt Templates and Prompt Variables\n\nInstrumenting Prompt Templates and Prompt Variables\nInstrumenting prompt templates and variables allows you to track and visualize prompt changes. These can also be combined with \nExperiments\n to measure the performance changes driven by each of your prompts.\nPython\nWe provide a \nusing_prompt_template\n context manager to add a prompt template (including its version and variables) to the current OpenTelemetry Context. OpenInference \nauto-instrumentors\n will read this Context and pass the prompt template fields as span attributes, following the OpenInference \nsemantic conventions\n. Its inputs must be of the following type:\nTemplate: non-empty string.\nVersion: non-empty string.\nVariables: a dictionary with string keys. This dictionary will be serialized to JSON when saved to the OTEL Context and remain a JSON string when sent as a span attribute.\nCopy\nfrom\n openinference\n.\ninstrumentation \nimport\n using_prompt_template\n\n\n\n\nprompt_template \n=\n \n""Please describe the weather forecast for \n{city}\n on \n{date}\n""\n\n\nprompt_template_variables \n=\n \n{\n""city""\n:\n \n""Johannesburg""\n,\n \n""date""\n:\n""July 11""\n}\n\n\nwith\n \nusing_prompt_template\n(\n\n\n    template\n=\nprompt_template,\n\n\n    variables\n=\nprompt_template_variables,\n\n\n    version\n=\n""v1.0""\n,\n\n\n    ):\n\n\n    \n# Commonly preceeds a chat completion to append templates to auto instrumentation\n\n\n    \n# response = client.chat.completions.create()\n\n\n    \n# Calls within this block will generate spans with the attributes:\n\n\n    \n# ""llm.prompt_template.template"" = ""Please describe the weather forecast for {city} on {date}""\n\n\n    \n# ""llm.prompt_template.version"" = ""v1.0""\n\n\n    \n# ""llm.prompt_template.variables"" = ""{\\""city\\"": \\""Johannesburg\\"", \\""date\\"": \\""July 11\\""}"" # JSON serialized\n\n\n    ...\nIt can also be used as a decorator:\nCopy\n@using_prompt_template\n(\n\n\n    template\n=\nprompt_template,\n\n\n    variables\n=\nprompt_template_variables,\n\n\n    version\n=\n""v1.0""\n,\n\n\n)\n\n\ndef\n \ncall_fn\n(\n*\nargs\n,\n \n**\nkwargs\n):\n\n\n    \n# Calls within this function will generate spans with the attributes:\n\n\n    \n# ""llm.prompt_template.template"" = ""Please describe the weather forecast for {city} on {date}""\n\n\n    \n# ""llm.prompt_template.version"" = ""v1.0""\n\n\n    \n# ""llm.prompt_template.variables"" = ""{\\""city\\"": \\""Johannesburg\\"", \\""date\\"": \\""July 11\\""}"" # JSON serialized\n\n\n    ...\nPrevious\nCustomize Spans\nNext\nAuto Instrumentation\nLast updated \n22 days ago\n\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix\'s tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you\'ll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they\'re geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix\'s search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix\'s flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix\'s evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is a prompt variable?\nAnswer: '}, {'role': 'assistant', 'content': 'A prompt variable is a key-value pair used to define specific values within a prompt template.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MjE=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix's tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you'll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they're geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix's search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix's flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix's evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is Phoenix for?\nAnswer: ""}, {'role': 'assistant', 'content': 'Phoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix's tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you'll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they're geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix's search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix's flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix's evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is Phoenix for?\nAnswer: ""}, {'role': 'assistant', 'content': 'Phoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MjI=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix's tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you'll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they're geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix's search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix's flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix's evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How is Phoenix different than other observability platforms?\nAnswer: ""}, {'role': 'assistant', 'content': 'Phoenix offers a comprehensive observability platform designed specifically for AI Engineers and Data Scientists to visualize data, evaluate performance, track down issues, and export data to improve. It provides essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval, making it stand out from other observability platforms by focusing on AI-specific needs and workflows.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix's tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you'll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they're geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix's search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix's flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix's evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How is Phoenix different than other observability platforms?\nAnswer: ""}, {'role': 'assistant', 'content': 'Phoenix offers a comprehensive observability platform designed specifically for AI Engineers and Data Scientists to visualize data, evaluate performance, track down issues, and export data to improve. It provides essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval, making it stand out from other observability platforms by focusing on AI-specific needs and workflows.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MjM=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n\nsource: https://docs.arize.com/phoenix/integrations/arize\ntitle: Arize\n\nArize\nPhoenix works hand-in-hand with Arize, it's enterprise counterpart\nShare Embeddings Data with Arize\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nPrevious\nModels\nNext\nExport Data from Arize to Phoenix\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: When do I use Phoenix vs. Arize?\nAnswer: ""}, {'role': 'assistant', 'content': 'You use Phoenix when you need an open-source observability library for experimentation, evaluation, and troubleshooting in AI projects. On the other hand, you use Arize, the enterprise counterpart of Phoenix, when you want to share embeddings data easily with your data science team for further investigation or to initiate retraining workflows.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n\nsource: https://docs.arize.com/phoenix/integrations/arize\ntitle: Arize\n\nArize\nPhoenix works hand-in-hand with Arize, it's enterprise counterpart\nShare Embeddings Data with Arize\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nPrevious\nModels\nNext\nExport Data from Arize to Phoenix\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: When do I use Phoenix vs. Arize?\nAnswer: ""}, {'role': 'assistant', 'content': 'You use Phoenix when you need an open-source observability library for experimentation, evaluation, and troubleshooting in AI projects. On the other hand, you use Arize, the enterprise counterpart of Phoenix, when you want to share embeddings data easily with your data science team for further investigation or to initiate retraining workflows.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MjQ=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/integrations/arize\ntitle: Arize\n\nArize\nPhoenix works hand-in-hand with Arize, it's enterprise counterpart\nShare Embeddings Data with Arize\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nPrevious\nModels\nNext\nExport Data from Arize to Phoenix\nLast updated \n5 months ago\n\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What can I do with the Phoenix API?\nAnswer: ""}, {'role': 'assistant', 'content': 'With the Phoenix API, you can visualize data, evaluate performance, track down issues, and export data for further analysis and improvement.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/integrations/arize\ntitle: Arize\n\nArize\nPhoenix works hand-in-hand with Arize, it's enterprise counterpart\nShare Embeddings Data with Arize\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nPrevious\nModels\nNext\nExport Data from Arize to Phoenix\nLast updated \n5 months ago\n\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What can I do with the Phoenix API?\nAnswer: ""}, {'role': 'assistant', 'content': 'With the Phoenix API, you can visualize data, evaluate performance, track down issues, and export data for further analysis and improvement.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MjU=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/api/evals\ntitle: Evals\n\n""name""\n:\n \n""Peter""\n,\n\n\n    \n""age""\n:\n \n20\n,\n\n\n    \n""location""\n:\n \n""Queens""\n\n\n}\n\n\nprint\n(prompt_template.\nformat\n(value_dict))\n\n\n# Output: My name is Peter. I am 20 years old and I am from Queens\nNote that once you initialize the \nPromptTemplate\n class, you don\'t need to worry about delimiters anymore, it will be handled for you.\nphoenix.evals.llm_classify\nCopy\ndef\n \nllm_classify\n(\n\n\n    \ndataframe\n:\n pd\n.\nDataFrame\n,\n\n\n    \nmodel\n:\n BaseEvalModel\n,\n\n\n    \ntemplate\n:\n Union\n[\nClassificationTemplate\n,\n PromptTemplate\n,\n \nstr\n],\n\n\n    \nrails\n:\n List\n[\nstr\n],\n\n\n    \nsystem_instruction\n:\n Optional\n[\nstr\n]\n \n=\n \nNone\n,\n\n\n    \nverbose\n:\n \nbool\n \n=\n \nFalse\n,\n\n\n    \nuse_function_calling_if_available\n:\n \nbool\n \n=\n \nTrue\n,\n\n\n    \nprovide_explanation\n:\n \nbool\n \n=\n \nFalse\n,\n\n\n) \n->\n pd\n.\nDataFrame\nClassifies each input row of the \ndataframe\n using an LLM. Returns a \npandas.DataFrame\n where the first column is named \nlabel\n and contains the classification labels. An optional column named \nexplanation\n is added when \nprovide_explanation=True\n.\nParameters\ndataframe (pandas.DataFrame)\n: A pandas dataframe in which each row represents a record to be classified. All template variable names must appear as column names in the dataframe (extra columns unrelated to the template are permitted).\ntemplate (ClassificationTemplate, or str):\n The prompt template as either an instance of PromptTemplate or a string. If the latter, the variable names should be surrounded by curly braces so that a call to \n.format\n can be made to substitute variable values.\nmodel (BaseEvalModel):\n An LLM model class instance\nrails (List[str]):\n A list of strings representing the possible output classes of the model\'s predictions.\nsystem_instruction (Optional[str]):\n An optional system message for modals that support it\nverbose (bool, optional):\n If \nTrue\n, prints detailed info to stdout such as model invocation parameters and details about retries and snapping to rails. Default \nFalse\n.\nuse_function_calling_if_available (bool, default=True):\n If \nTrue\n, use function calling (if available) as a means to constrain the LLM outputs. With function calling, the LLM is instructed to provide its response as a structured JSON object, which is easier to parse.\nprovide_explanation (bool, default=False):\n If \nTrue\n, provides an explanation for each classification label. A column named \nexplanation\n is added to the output dataframe. Note that this will default to using function calling if available. If the model supplied does not support function calling, \nllm_classify\n will need a prompt template that prompts for an explanation. For phoenix\'s pre-tested eval templates, the template is swapped out for a \nchain-of-thought\n based template that prompts for an explanation.\nReturns\npandas.DataFrame:\n A dataframe where the \nlabel\n column (at column position 0) contains the classification labels. If \nprovide_explanation=True\n, then an additional column named \nexplanation\n is added to contain the explanation for each label. The dataframe has the same length and index as the input dataframe. The classification label values are from the entries in the rails argument or ""NOT_PARSABLE"" if the model\'s output could not be parsed.\nphoenix.evals.llm_generate\nCopy\ndef\n \nllm_generate\n(\n\n\n    \ndataframe\n:\n pd\n.\nDataFrame\n,\n\n\n    \ntemplate\n:\n Union\n[\nPromptTemplate\n,\n \nstr\n],\n\n\n    \nmodel\n:\n Optional\n[\nBaseEvalModel\n]\n \n=\n \nNone\n,\n\n\n    \nsystem_instruction\n:\n Optional\n[\nstr\n]\n \n=\n \nNone\n,\n\n\n    \noutput_parser\n:\n Optional\n[\nCallable\n[\n[\nstr\n,\n \nint\n]\n,\n Dict\n[\nstr\n,\n Any\n]]]\n \n=\n \nNone\n,\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/evals-with-explanations\ntitle: Evals With Explanations\n\nEvals With Explanations\nIt can be hard to understand in many cases why an LLM responds in a specific way. The explanation feature of Phoneix allows you to get a Eval output and an explanation from the LLM at the same time. We have found this incredibly useful for debugging LLM Evals.\nCopy\nfrom\n phoenix\n.\nevals \nimport\n (\n\n\n    RAG_RELEVANCY_PROMPT_RAILS_MAP\n,\n\n\n    RAG_RELEVANCY_PROMPT_TEMPLATE\n,\n\n\n    OpenAIModel\n,\n\n\n    download_benchmark_dataset\n,\n\n\n    llm_classify\n,\n\n\n)\n\n\n\n\nmodel \n=\n \nOpenAIModel\n(\n\n\n    model_name\n=\n""gpt-4""\n,\n\n\n    temperature\n=\n0.0\n,\n\n\n)\n\n\n\n\n#The rails is used to hold the output to specific values based on the template\n\n\n#It will remove text such as "",,,"" or ""...""\n\n\n#Will ensure the binary value expected from the template is returned\n\n\nrails \n=\n \nlist\n(RAG_RELEVANCY_PROMPT_RAILS_MAP.\nvalues\n())\n\n\nrelevance_classifications \n=\n \nllm_classify\n(\n\n\n    dataframe\n=\ndf,\n\n\n    template\n=\nRAG_RELEVANCY_PROMPT_TEMPLATE,\n\n\n    model\n=\nmodel,\n\n\n    rails\n=\nrails,\n\n\n    \nprovide_explanation\n=\nTrue\n\n\n)\n\n\n#relevance_classifications is a Dataframe with columns \'label\' and \'explanation\'\nThe flag above can be set with any of the templates or your own custom templates. The example below is from a relevance Evaluation.\nPrevious\nEval Data Types\nNext\nEvaluators\nLast updated \n3 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Can you explain llm_classify?\nAnswer: '}, {'role': 'assistant', 'content': 'llm_classify is a function that classifies each input row of a pandas DataFrame using a Language Model (LLM). It returns a pandas DataFrame where the first column is named ""label"" and contains the classification labels. Additionally, an optional column named ""explanation"" is added when the parameter provide_explanation is set to True. The function requires parameters such as the DataFrame to be classified, the template for classification, the LLM model, a list of possible output classes, and optional parameters like system instruction, verbosity, and whether to provide explanations for the classifications. The output DataFrame will contain the classification labels in the ""label"" column and explanations in the ""explanation"" column if provide_explanation is set to True.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/api/evals\ntitle: Evals\n\n""name""\n:\n \n""Peter""\n,\n\n\n    \n""age""\n:\n \n20\n,\n\n\n    \n""location""\n:\n \n""Queens""\n\n\n}\n\n\nprint\n(prompt_template.\nformat\n(value_dict))\n\n\n# Output: My name is Peter. I am 20 years old and I am from Queens\nNote that once you initialize the \nPromptTemplate\n class, you don\'t need to worry about delimiters anymore, it will be handled for you.\nphoenix.evals.llm_classify\nCopy\ndef\n \nllm_classify\n(\n\n\n    \ndataframe\n:\n pd\n.\nDataFrame\n,\n\n\n    \nmodel\n:\n BaseEvalModel\n,\n\n\n    \ntemplate\n:\n Union\n[\nClassificationTemplate\n,\n PromptTemplate\n,\n \nstr\n],\n\n\n    \nrails\n:\n List\n[\nstr\n],\n\n\n    \nsystem_instruction\n:\n Optional\n[\nstr\n]\n \n=\n \nNone\n,\n\n\n    \nverbose\n:\n \nbool\n \n=\n \nFalse\n,\n\n\n    \nuse_function_calling_if_available\n:\n \nbool\n \n=\n \nTrue\n,\n\n\n    \nprovide_explanation\n:\n \nbool\n \n=\n \nFalse\n,\n\n\n) \n->\n pd\n.\nDataFrame\nClassifies each input row of the \ndataframe\n using an LLM. Returns a \npandas.DataFrame\n where the first column is named \nlabel\n and contains the classification labels. An optional column named \nexplanation\n is added when \nprovide_explanation=True\n.\nParameters\ndataframe (pandas.DataFrame)\n: A pandas dataframe in which each row represents a record to be classified. All template variable names must appear as column names in the dataframe (extra columns unrelated to the template are permitted).\ntemplate (ClassificationTemplate, or str):\n The prompt template as either an instance of PromptTemplate or a string. If the latter, the variable names should be surrounded by curly braces so that a call to \n.format\n can be made to substitute variable values.\nmodel (BaseEvalModel):\n An LLM model class instance\nrails (List[str]):\n A list of strings representing the possible output classes of the model\'s predictions.\nsystem_instruction (Optional[str]):\n An optional system message for modals that support it\nverbose (bool, optional):\n If \nTrue\n, prints detailed info to stdout such as model invocation parameters and details about retries and snapping to rails. Default \nFalse\n.\nuse_function_calling_if_available (bool, default=True):\n If \nTrue\n, use function calling (if available) as a means to constrain the LLM outputs. With function calling, the LLM is instructed to provide its response as a structured JSON object, which is easier to parse.\nprovide_explanation (bool, default=False):\n If \nTrue\n, provides an explanation for each classification label. A column named \nexplanation\n is added to the output dataframe. Note that this will default to using function calling if available. If the model supplied does not support function calling, \nllm_classify\n will need a prompt template that prompts for an explanation. For phoenix\'s pre-tested eval templates, the template is swapped out for a \nchain-of-thought\n based template that prompts for an explanation.\nReturns\npandas.DataFrame:\n A dataframe where the \nlabel\n column (at column position 0) contains the classification labels. If \nprovide_explanation=True\n, then an additional column named \nexplanation\n is added to contain the explanation for each label. The dataframe has the same length and index as the input dataframe. The classification label values are from the entries in the rails argument or ""NOT_PARSABLE"" if the model\'s output could not be parsed.\nphoenix.evals.llm_generate\nCopy\ndef\n \nllm_generate\n(\n\n\n    \ndataframe\n:\n pd\n.\nDataFrame\n,\n\n\n    \ntemplate\n:\n Union\n[\nPromptTemplate\n,\n \nstr\n],\n\n\n    \nmodel\n:\n Optional\n[\nBaseEvalModel\n]\n \n=\n \nNone\n,\n\n\n    \nsystem_instruction\n:\n Optional\n[\nstr\n]\n \n=\n \nNone\n,\n\n\n    \noutput_parser\n:\n Optional\n[\nCallable\n[\n[\nstr\n,\n \nint\n]\n,\n Dict\n[\nstr\n,\n Any\n]]]\n \n=\n \nNone\n,\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/evals-with-explanations\ntitle: Evals With Explanations\n\nEvals With Explanations\nIt can be hard to understand in many cases why an LLM responds in a specific way. The explanation feature of Phoneix allows you to get a Eval output and an explanation from the LLM at the same time. We have found this incredibly useful for debugging LLM Evals.\nCopy\nfrom\n phoenix\n.\nevals \nimport\n (\n\n\n    RAG_RELEVANCY_PROMPT_RAILS_MAP\n,\n\n\n    RAG_RELEVANCY_PROMPT_TEMPLATE\n,\n\n\n    OpenAIModel\n,\n\n\n    download_benchmark_dataset\n,\n\n\n    llm_classify\n,\n\n\n)\n\n\n\n\nmodel \n=\n \nOpenAIModel\n(\n\n\n    model_name\n=\n""gpt-4""\n,\n\n\n    temperature\n=\n0.0\n,\n\n\n)\n\n\n\n\n#The rails is used to hold the output to specific values based on the template\n\n\n#It will remove text such as "",,,"" or ""...""\n\n\n#Will ensure the binary value expected from the template is returned\n\n\nrails \n=\n \nlist\n(RAG_RELEVANCY_PROMPT_RAILS_MAP.\nvalues\n())\n\n\nrelevance_classifications \n=\n \nllm_classify\n(\n\n\n    dataframe\n=\ndf,\n\n\n    template\n=\nRAG_RELEVANCY_PROMPT_TEMPLATE,\n\n\n    model\n=\nmodel,\n\n\n    rails\n=\nrails,\n\n\n    \nprovide_explanation\n=\nTrue\n\n\n)\n\n\n#relevance_classifications is a Dataframe with columns \'label\' and \'explanation\'\nThe flag above can be set with any of the templates or your own custom templates. The example below is from a relevance Evaluation.\nPrevious\nEval Data Types\nNext\nEvaluators\nLast updated \n3 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Can you explain llm_classify?\nAnswer: '}, {'role': 'assistant', 'content': 'llm_classify is a function that classifies each input row of a pandas DataFrame using a Language Model (LLM). It returns a pandas DataFrame where the first column is named ""label"" and contains the classification labels. Additionally, an optional column named ""explanation"" is added when the parameter provide_explanation is set to True. The function requires parameters such as the DataFrame to be classified, the template for classification, the LLM model, a list of possible output classes, and optional parameters like system instruction, verbosity, and whether to provide explanations for the classifications. The output DataFrame will contain the classification labels in the ""label"" column and explanations in the ""explanation"" column if provide_explanation is set to True.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MjY=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/api/evals\ntitle: Evals\n\nEvals\nEvals are LLM-powered functions that you can use to evaluate the output of your LLM or generative application\nphoenix.evals.run_evals\nCopy\ndef\n \nrun_evals\n(\n\n\n    \ndataframe\n:\n pd\n.\nDataFrame\n,\n\n\n    \nevaluators\n:\n List\n[\nLLMEvaluator\n],\n\n\n    \nprovide_explanation\n:\n \nbool\n \n=\n \nFalse\n,\n\n\n    \nuse_function_calling_if_available\n:\n \nbool\n \n=\n \nTrue\n,\n\n\n    \nverbose\n:\n \nbool\n \n=\n \nFalse\n,\n\n\n    \nconcurrency\n:\n \nint\n \n=\n \n20\n,\n\n\n) \n->\n List\n[\npd\n.\nDataFrame\n]\nEvaluates a pandas dataframe using a set of user-specified evaluators that assess each row for relevance of retrieved documents, hallucinations, toxicity, etc. Outputs a list of dataframes, one for each evaluator, that contain the labels, scores, and optional explanations from the corresponding evaluator applied to the input dataframe.\nParameters\ndataframe\n (pandas.DataFrame): A pandas dataframe in which each row represents an individual record to be evaluated. Each evaluator uses an LLM and an evaluation prompt template to assess the rows of the dataframe, and those template variables must appear as column names in the dataframe.\nevaluators\n (List[LLMEvaluator]): A list of evaluators to apply to the input dataframe. Each evaluator class accepts a \nmodel\n as input, which is used in conjunction with an evaluation prompt template to evaluate the rows of the input dataframe and to output labels, scores, and optional explanations. Currently supported evaluators include:\nHallucinationEvaluator:\n Evaluates whether a response (stored under an ""output"" column) is a hallucination given a query (stored under an ""input"" column) and one or more retrieved documents (stored under a ""reference"" column).\nRelevanceEvaluator:\n Evaluates whether a retrieved document (stored under a ""reference"" column) is relevant or irrelevant to the corresponding query (stored under an ""input"" column).\nToxicityEvaluator:\n Evaluates whether a string (stored under an ""input"" column) contains racist, sexist, chauvinistic, biased, or otherwise toxic content.\nQAEvaluator:\n Evaluates whether a response (stored under an ""output"" column) is correct or incorrect given a query (stored under an ""input"" column) and one or more retrieved documents (stored under a ""reference"" column).\nSummarizationEvaluator:\n Evaluates whether a summary (stored under an ""output"" column) provides an accurate synopsis of an input document (stored under an ""input"" column).\nSQLEvaluator:\n Evaluates whether a generated SQL query (stored under the ""query_gen"" column) and a response (stored under the ""response"" column) appropriately answer a question (stored under the ""question"" column).\nprovide_explanation\n (bool, optional): If true, each output dataframe will contain an explanation column containing the LLM\'s reasoning for each evaluation.\nuse_function_calling_if_available\n (bool, optional): If true, function calling is used (if available) as a means to constrain the LLM outputs. With function calling, the LLM is instructed to provide its response as a structured JSON object, which is easier to parse.\nverbose\n (bool, optional): If true, prints detailed information such as model invocation parameters, retries on failed requests, etc.\nconcurrency\n (int, optional): The number of concurrent workers if async submission is possible. If not provided, a recommended default concurrency is set on a per-model basis.\nReturns\nList[pandas.DataFrame]\n: A list of dataframes, one for each evaluator, all of which have the same number of rows as the input dataframe.\nUsage\nTo use \nrun_evals\n, you must first wrangle your LLM application data into a pandas dataframe either manually or by querying and exporting the spans collected by your Phoenix session. Once your dataframe is wrangled into the appropriate format, you can instantiate your evaluators by passing the model to be used during evaluation.\nThis example uses \nOpenAIModel\n, but you can use any of our \nsupported evaluation models\n.\nCopy\nfrom\n phoenix\n.\nevals \nimport\n (\n\n\n    OpenAIModel\n,\n\n\n    HallucinationEvaluator\n,\n\n\n    QAEvaluator\n,\n\n\n    run_evals\n,\n\n\n)\n\n\n\n\napi_key \n=\n \nNone\n  \n# set your api key here or with the OPENAI_API_KEY environment variable\n\n\neval_model \n=\n \nOpenAIModel\n(model_name\n=\n""gpt-4-turbo-preview""\n, api_key\n=\napi_key)\n\n\n\n\nhallucination_evaluator \n=\n \nHallucinationEvaluator\n(eval_model)\n\nsource: https://docs.arize.com/phoenix/api/evals\ntitle: Evals\n\nOpenAIModel\n,\n\n\n    HallucinationEvaluator\n,\n\n\n    QAEvaluator\n,\n\n\n    run_evals\n,\n\n\n)\n\n\n\n\napi_key \n=\n \nNone\n  \n# set your api key here or with the OPENAI_API_KEY environment variable\n\n\neval_model \n=\n \nOpenAIModel\n(model_name\n=\n""gpt-4-turbo-preview""\n, api_key\n=\napi_key)\n\n\n\n\nhallucination_evaluator \n=\n \nHallucinationEvaluator\n(eval_model)\n\n\nqa_correctness_evaluator \n=\n \nQAEvaluator\n(eval_model)\nRun your evaluations by passing your \ndataframe\n and your list of desired evaluators.\nCopy\nhallucination_eval_df, qa_correctness_eval_df = run_evals(\n\n\n    dataframe=dataframe,\n\n\n    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n\n\n    provide_explanation=True,\n\n\n)\nAssuming your \ndataframe\n contains the ""input"", ""reference"", and ""output"" columns required by \nHallucinationEvaluator\n and \nQAEvaluator\n, your output dataframes should contain the results of the corresponding evaluator applied to the input dataframe, including columns for labels (e.g., ""factual"" or ""hallucinated""), scores (e.g., 0 for factual labels, 1 for hallucinated labels), and explanations. If your dataframe was exported from your Phoenix session, you can then ingest the evaluations using \nphoenix.log_evaluations\n so that the evals will be visible as annotations inside Phoenix.\nFor an end-to-end example, see the \nevals quickstart\n.\nphoenix.evals.PromptTemplate\nCopy\nclass\n \nPromptTemplate\n(\n\n\n    \ntext\n:\n \nstr\n\n\n    \ndelimiters\n:\n List\n[\nstr\n]\n\n\n)\nClass used to store and format prompt templates.\nParameters\ntext\n (str): The raw prompt text used as a template.\ndelimiters\n (List[str]): List of characters used to locate the variables within the prompt template \ntext\n. Defaults to \n[""{"", ""}""]\n.\nAttributes\ntext\n (str): The raw prompt text used as a template.\nvariables\n (List[str]): The names of the variables that, once their values are substituted into the template, create the prompt text. These variable names are automatically detected from the template \ntext\n using the \ndelimiters\n passed when initializing the class (see Usage section below).\nUsage\nDefine a \nPromptTemplate\n by passing a \ntext\n string and the \ndelimiters\n to use to locate the \nvariables\n. The default delimiters are \n{\n and \n}\n.\nCopy\nfrom\n phoenix\n.\nevals \nimport\n PromptTemplate\n\n\n\n\ntemplate_text \n=\n \n""My name is \n{name}\n. I am \n{age}\n years old and I am from \n{location}\n.""\n\n\nprompt_template \n=\n \nPromptTemplate\n(text\n=\ntemplate_text)\nIf the prompt template variables have been correctly located, you can access them as follows:\nCopy\nprint\n(prompt_template.variables)\n\n\n# Output: [\'name\', \'age\', \'location\']\nThe \nPromptTemplate\n class can also understand any combination of delimiters. Following the example above, but getting creative with our delimiters:\nCopy\ntemplate_text \n=\n \n""My name is :/name-!). I am :/age-!) years old and I am from :/location-!).""\n\n\nprompt_template \n=\n \nPromptTemplate\n(text\n=\ntemplate_text, delimiters\n=\n[\n"":/""\n, \n""-!)""\n])\n\n\nprint\n(prompt_template.variables)\n\n\n# Output: [\'name\', \'age\', \'location\']\nOnce you have a \nPromptTemplate\n class instantiated, you can make use of its \nformat\n method to construct the prompt text resulting from substituting values into the \nvariables\n. To do so, a dictionary mapping the variable names to the values is passed:\nCopy\nvalue_dict \n=\n \n{\n\n\n    \n""name""\n:\n \n""Peter""\n,\n\n\n    \n""age""\n:\n \n20\n,\n\n\n    \n""location""\n:\n \n""Queens""\n\n\n}\n\n\nprint\n(prompt_template.\nformat\n(value_dict))\n\n\n# Output: My name is Peter. I am 20 years old and I am from Queens\nNote that once you initialize the \nPromptTemplate\n class, you don\'t need to worry about delimiters anymore, it will be handled for you.\nphoenix.evals.llm_classify\nCopy\ndef\n \nllm_classify\n(\n\n\n    \ndataframe\n:\n pd\n.\nDataFrame\n,\n\n\n    \nmodel\n:\n BaseEvalModel\n,\n\n\n    \ntemplate\n:\n Union\n[\nClassificationTemplate\n,\n PromptTemplate\n,\n \nstr\n],\n\n\n    \nrails\n:\n List\n[\nstr\n],\n\n\n    \nsystem_instruction\n:\n Optional\n[\nstr\n]\n \n=\n \nNone\n,\n\n\n    \nverbose\n:\n \nbool\n \n=\n \nFalse\n,\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is run_evals?\nAnswer: '}, {'role': 'assistant', 'content': 'run_evals is a function that evaluates a pandas dataframe using a set of user-specified evaluators that assess each row for relevance of retrieved documents, hallucinations, toxicity, etc. It outputs a list of dataframes, one for each evaluator, that contain the labels, scores, and optional explanations from the corresponding evaluator applied to the input dataframe.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/api/evals\ntitle: Evals\n\nEvals\nEvals are LLM-powered functions that you can use to evaluate the output of your LLM or generative application\nphoenix.evals.run_evals\nCopy\ndef\n \nrun_evals\n(\n\n\n    \ndataframe\n:\n pd\n.\nDataFrame\n,\n\n\n    \nevaluators\n:\n List\n[\nLLMEvaluator\n],\n\n\n    \nprovide_explanation\n:\n \nbool\n \n=\n \nFalse\n,\n\n\n    \nuse_function_calling_if_available\n:\n \nbool\n \n=\n \nTrue\n,\n\n\n    \nverbose\n:\n \nbool\n \n=\n \nFalse\n,\n\n\n    \nconcurrency\n:\n \nint\n \n=\n \n20\n,\n\n\n) \n->\n List\n[\npd\n.\nDataFrame\n]\nEvaluates a pandas dataframe using a set of user-specified evaluators that assess each row for relevance of retrieved documents, hallucinations, toxicity, etc. Outputs a list of dataframes, one for each evaluator, that contain the labels, scores, and optional explanations from the corresponding evaluator applied to the input dataframe.\nParameters\ndataframe\n (pandas.DataFrame): A pandas dataframe in which each row represents an individual record to be evaluated. Each evaluator uses an LLM and an evaluation prompt template to assess the rows of the dataframe, and those template variables must appear as column names in the dataframe.\nevaluators\n (List[LLMEvaluator]): A list of evaluators to apply to the input dataframe. Each evaluator class accepts a \nmodel\n as input, which is used in conjunction with an evaluation prompt template to evaluate the rows of the input dataframe and to output labels, scores, and optional explanations. Currently supported evaluators include:\nHallucinationEvaluator:\n Evaluates whether a response (stored under an ""output"" column) is a hallucination given a query (stored under an ""input"" column) and one or more retrieved documents (stored under a ""reference"" column).\nRelevanceEvaluator:\n Evaluates whether a retrieved document (stored under a ""reference"" column) is relevant or irrelevant to the corresponding query (stored under an ""input"" column).\nToxicityEvaluator:\n Evaluates whether a string (stored under an ""input"" column) contains racist, sexist, chauvinistic, biased, or otherwise toxic content.\nQAEvaluator:\n Evaluates whether a response (stored under an ""output"" column) is correct or incorrect given a query (stored under an ""input"" column) and one or more retrieved documents (stored under a ""reference"" column).\nSummarizationEvaluator:\n Evaluates whether a summary (stored under an ""output"" column) provides an accurate synopsis of an input document (stored under an ""input"" column).\nSQLEvaluator:\n Evaluates whether a generated SQL query (stored under the ""query_gen"" column) and a response (stored under the ""response"" column) appropriately answer a question (stored under the ""question"" column).\nprovide_explanation\n (bool, optional): If true, each output dataframe will contain an explanation column containing the LLM\'s reasoning for each evaluation.\nuse_function_calling_if_available\n (bool, optional): If true, function calling is used (if available) as a means to constrain the LLM outputs. With function calling, the LLM is instructed to provide its response as a structured JSON object, which is easier to parse.\nverbose\n (bool, optional): If true, prints detailed information such as model invocation parameters, retries on failed requests, etc.\nconcurrency\n (int, optional): The number of concurrent workers if async submission is possible. If not provided, a recommended default concurrency is set on a per-model basis.\nReturns\nList[pandas.DataFrame]\n: A list of dataframes, one for each evaluator, all of which have the same number of rows as the input dataframe.\nUsage\nTo use \nrun_evals\n, you must first wrangle your LLM application data into a pandas dataframe either manually or by querying and exporting the spans collected by your Phoenix session. Once your dataframe is wrangled into the appropriate format, you can instantiate your evaluators by passing the model to be used during evaluation.\nThis example uses \nOpenAIModel\n, but you can use any of our \nsupported evaluation models\n.\nCopy\nfrom\n phoenix\n.\nevals \nimport\n (\n\n\n    OpenAIModel\n,\n\n\n    HallucinationEvaluator\n,\n\n\n    QAEvaluator\n,\n\n\n    run_evals\n,\n\n\n)\n\n\n\n\napi_key \n=\n \nNone\n  \n# set your api key here or with the OPENAI_API_KEY environment variable\n\n\neval_model \n=\n \nOpenAIModel\n(model_name\n=\n""gpt-4-turbo-preview""\n, api_key\n=\napi_key)\n\n\n\n\nhallucination_evaluator \n=\n \nHallucinationEvaluator\n(eval_model)\n\nsource: https://docs.arize.com/phoenix/api/evals\ntitle: Evals\n\nOpenAIModel\n,\n\n\n    HallucinationEvaluator\n,\n\n\n    QAEvaluator\n,\n\n\n    run_evals\n,\n\n\n)\n\n\n\n\napi_key \n=\n \nNone\n  \n# set your api key here or with the OPENAI_API_KEY environment variable\n\n\neval_model \n=\n \nOpenAIModel\n(model_name\n=\n""gpt-4-turbo-preview""\n, api_key\n=\napi_key)\n\n\n\n\nhallucination_evaluator \n=\n \nHallucinationEvaluator\n(eval_model)\n\n\nqa_correctness_evaluator \n=\n \nQAEvaluator\n(eval_model)\nRun your evaluations by passing your \ndataframe\n and your list of desired evaluators.\nCopy\nhallucination_eval_df, qa_correctness_eval_df = run_evals(\n\n\n    dataframe=dataframe,\n\n\n    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n\n\n    provide_explanation=True,\n\n\n)\nAssuming your \ndataframe\n contains the ""input"", ""reference"", and ""output"" columns required by \nHallucinationEvaluator\n and \nQAEvaluator\n, your output dataframes should contain the results of the corresponding evaluator applied to the input dataframe, including columns for labels (e.g., ""factual"" or ""hallucinated""), scores (e.g., 0 for factual labels, 1 for hallucinated labels), and explanations. If your dataframe was exported from your Phoenix session, you can then ingest the evaluations using \nphoenix.log_evaluations\n so that the evals will be visible as annotations inside Phoenix.\nFor an end-to-end example, see the \nevals quickstart\n.\nphoenix.evals.PromptTemplate\nCopy\nclass\n \nPromptTemplate\n(\n\n\n    \ntext\n:\n \nstr\n\n\n    \ndelimiters\n:\n List\n[\nstr\n]\n\n\n)\nClass used to store and format prompt templates.\nParameters\ntext\n (str): The raw prompt text used as a template.\ndelimiters\n (List[str]): List of characters used to locate the variables within the prompt template \ntext\n. Defaults to \n[""{"", ""}""]\n.\nAttributes\ntext\n (str): The raw prompt text used as a template.\nvariables\n (List[str]): The names of the variables that, once their values are substituted into the template, create the prompt text. These variable names are automatically detected from the template \ntext\n using the \ndelimiters\n passed when initializing the class (see Usage section below).\nUsage\nDefine a \nPromptTemplate\n by passing a \ntext\n string and the \ndelimiters\n to use to locate the \nvariables\n. The default delimiters are \n{\n and \n}\n.\nCopy\nfrom\n phoenix\n.\nevals \nimport\n PromptTemplate\n\n\n\n\ntemplate_text \n=\n \n""My name is \n{name}\n. I am \n{age}\n years old and I am from \n{location}\n.""\n\n\nprompt_template \n=\n \nPromptTemplate\n(text\n=\ntemplate_text)\nIf the prompt template variables have been correctly located, you can access them as follows:\nCopy\nprint\n(prompt_template.variables)\n\n\n# Output: [\'name\', \'age\', \'location\']\nThe \nPromptTemplate\n class can also understand any combination of delimiters. Following the example above, but getting creative with our delimiters:\nCopy\ntemplate_text \n=\n \n""My name is :/name-!). I am :/age-!) years old and I am from :/location-!).""\n\n\nprompt_template \n=\n \nPromptTemplate\n(text\n=\ntemplate_text, delimiters\n=\n[\n"":/""\n, \n""-!)""\n])\n\n\nprint\n(prompt_template.variables)\n\n\n# Output: [\'name\', \'age\', \'location\']\nOnce you have a \nPromptTemplate\n class instantiated, you can make use of its \nformat\n method to construct the prompt text resulting from substituting values into the \nvariables\n. To do so, a dictionary mapping the variable names to the values is passed:\nCopy\nvalue_dict \n=\n \n{\n\n\n    \n""name""\n:\n \n""Peter""\n,\n\n\n    \n""age""\n:\n \n20\n,\n\n\n    \n""location""\n:\n \n""Queens""\n\n\n}\n\n\nprint\n(prompt_template.\nformat\n(value_dict))\n\n\n# Output: My name is Peter. I am 20 years old and I am from Queens\nNote that once you initialize the \nPromptTemplate\n class, you don\'t need to worry about delimiters anymore, it will be handled for you.\nphoenix.evals.llm_classify\nCopy\ndef\n \nllm_classify\n(\n\n\n    \ndataframe\n:\n pd\n.\nDataFrame\n,\n\n\n    \nmodel\n:\n BaseEvalModel\n,\n\n\n    \ntemplate\n:\n Union\n[\nClassificationTemplate\n,\n PromptTemplate\n,\n \nstr\n],\n\n\n    \nrails\n:\n List\n[\nstr\n],\n\n\n    \nsystem_instruction\n:\n Optional\n[\nstr\n]\n \n=\n \nNone\n,\n\n\n    \nverbose\n:\n \nbool\n \n=\n \nFalse\n,\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is run_evals?\nAnswer: '}, {'role': 'assistant', 'content': 'run_evals is a function that evaluates a pandas dataframe using a set of user-specified evaluators that assess each row for relevance of retrieved documents, hallucinations, toxicity, etc. It outputs a list of dataframes, one for each evaluator, that contain the labels, scores, and optional explanations from the corresponding evaluator applied to the input dataframe.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6Mjc=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals\ntitle: Use Phoenix Evaluators\n\nUse Phoenix Evaluators\nThe following are simple functions on top of the LLM Evals building blocks that are pre-tested with benchmark data.\nAll evals templates are tested against golden data that are available as part of the LLM eval library\'s \nbenchmarked data\n and target precision at 70-90% and F1 at 70-85%.\nRetrieval Eval\nRAG individual retrieval\nTested on:\nMS Marco, WikiQA\nHallucination Eval\nHallucinations on answers to public and private data\nTested on:\nHallucination QA Dataset, Hallucination RAG Dataset\nToxicity Eval\nIs the AI response racist, biased or toxic\nT\nested on:\nWikiToxic\nQ&A Eval\nPrivate data Q&A Eval\nTested on:\nWikiQA\nSummarization Eval\nSummarization performance\nTested on:\nGigaWorld, CNNDM, Xsum\nCode Generation Eval\nCode writing correctness and readability\nTested on:\nWikiSQL, HumanEval, CodeXGlu\nSupported Models.\nThe models are instantiated and usable in the LLM Eval function. The models are also directly callable with strings.\nCopy\nmodel \n=\n \nOpenAIModel\n(model_name\n=\n""gpt-4""\n,temperature\n=\n0.6\n)\n\n\nmodel\n(\n""What is the largest costal city in France?""\n)\nWe currently support a growing set of models for LLM Evals, please check out the \nAPI section for usage\n.\nModel\nSupport\nGPT-4\n✔\nGPT-3.5 Turbo\n✔\nGPT-3.5 Instruct\n✔\nAzure Hosted Open AI\n✔\nPalm 2 Vertex\n✔\nAWS Bedrock\n✔\nLitellm\n✔\nHuggingface Llama7B\n(use litellm)\nAnthropic\n✔\nCohere\n(use litellm)\nHow we benchmark pre-tested evals\nThe above diagram shows examples of different environments the Eval harness is desinged to run. The benchmarking environment is designed to enable the testing of the Eval model & Eval template performance against a designed set of data.\nThe above approach allows us to compare models easily in an understandable format:\nHallucination Eval\nGPT-4\nGPT-3.5\nPrecision\n0.94\n0.94\nRecall\n0.75\n0.71\nF1\n0.83\n0.81\nPrevious\nHow to: Evals\nNext\nHallucinations\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/evaluation\ntitle: Evaluators\n\nEvaluators\nEvaluation and benchmarking are crucial concepts in LLM development. To improve the performance of an LLM app (RAG, agents), you must have a way to measure it.\nPhoenix offers key modules to measure the quality of generated results as well as modules to measure retrieval quality.\nResponse Evaluation\n: Does the response match the retrieved context? Does it also match the query?\nRetrieval Evaluation\n: Are the retrieved sources relevant to the query?\nResponse Evaluation\nEvaluation of generated results can be challenging. Unlike traditional ML, the predicted results are not numeric or categorical, making it hard to define quantitative metrics for this problem.\nPhoenix offers \nLLM Evaluations\n, a module designed to measure the quality of results. This module uses a ""gold"" LLM (e.g. GPT-4) to decide whether the generated answer is correct in a variety of ways.\n\nNote that many of these evaluation criteria DO NOT require ground-truth labels. Evaluation can be done simply with a combination of the \ninput\n (query), \noutput\n (response), and \ncontext\n.\nLLM Evals supports the following response evaluation criteria:\nQA Correctness\n - Whether a question was correctly answered by the system based on the retrieved data. In contrast to retrieval Evals that are checks on chunks of data returned, this check is a system level check of a correct Q&A.\nHallucinations\n \n-\n Designed to detect LLM hallucinations relative to retrieved context\nToxicity\n - Identify if the AI response is racist, biased, or toxic\nResponse evaluations are a critical first step to figuring out whether your LLM App is running correctly. Response evaluations can pinpoint specific executions (a.k.a. traces) that are performing badly and can be aggregated up so that you can track how your application is running as a whole.\nEvaluations can be aggregated across executions to be used as KPIs\nRetrieval Evaluation\nPhoenix also provides evaluation of retrieval independently.\nThe concept of retrieval evaluation is not new; given a set of relevance scores for a set of retrieved documents, we can evaluate retrievers using retrieval metrics like \nprecision\n, \nNDCG\n, \nhit rate\n and more.\nLLM Evals supports the following retrieval evaluation criteria:\nRelevance\n - Evaluates whether a retrieved document chunk contains an answer to the query.\nRetrieval Evaluations can be run directly on application traces\nRetrieval is possibly the most important step in any LLM application as poor and/or incorrect retrieval can be the cause of bad response generation. If your application uses RAG to power an LLM, retrieval evals can help you identify the cause of hallucinations and incorrect answers.\nEvaluations\nInferences that contain generative records can be fed into evals to produce evaluations for analysis\nWith Phoenix\'s LLM Evals, evaluation results (or just \nEvaluations\n for short) is data consisting of 3 main columns:\nlabel\n: str [optional] - a classification label for the evaluation (e.g. ""hallucinated"" vs ""factual""). Can be used to calculate percentages (e.g. percent hallucinated) and can be used to filter down your data (e.g. \nEvals[""Hallucinations""].label == ""hallucinated""\n)\nscore\n: number [optional] - a numeric score for the evaluation (e.g. 1 for good, 0 for bad). Scores are great way to sort your data to surface poorly performing examples and can be used to filter your data by a threshold.\nexplanation\n: str [optional] - the reasoning for why the evaluation label or score was given. In the case of LLM evals, this is the evaluation model\'s reasoning. While explanations are optional, they can be extremely useful when trying to understand problematic areas of your application.\nLet\'s take a look at an example list of \nQ&A relevance\n evaluations:\nlabel\nexplanation\nscore\ncorrect\nThe reference text explains that YC was not or...\n1\ncorrect\nTo determine if the answer is correct, we need...\n1\nincorrect\nTo determine if the answer is correct, we must...\n0\ncorrect\nTo determine if the answer is correct, we need...\n1\nThese three columns combined can drive any type of evaluation you can imagine. \nlabel\n provides a way to classify responses, \nscore\n provides a way to assign a numeric assessment, and \nexplanation\n gives you a way to get qualitative feedback.\nEvaluating Traces\nAdding evaluations on traces can highlight problematic areas that require further analysis\nWith Phoenix, evaluations can be ""attached"" to the \nspans\n and \ndocuments\n collected. In order to facilitate this, Phoenix supports the following steps.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What pre-built evaluators does Phoenix support?\nAnswer: '}, {'role': 'assistant', 'content': 'Phoenix supports pre-built evaluators for Retrieval Eval, Hallucination Eval, Toxicity Eval, Q&A Eval, Summarization Eval, and Code Generation Eval.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals\ntitle: Use Phoenix Evaluators\n\nUse Phoenix Evaluators\nThe following are simple functions on top of the LLM Evals building blocks that are pre-tested with benchmark data.\nAll evals templates are tested against golden data that are available as part of the LLM eval library\'s \nbenchmarked data\n and target precision at 70-90% and F1 at 70-85%.\nRetrieval Eval\nRAG individual retrieval\nTested on:\nMS Marco, WikiQA\nHallucination Eval\nHallucinations on answers to public and private data\nTested on:\nHallucination QA Dataset, Hallucination RAG Dataset\nToxicity Eval\nIs the AI response racist, biased or toxic\nT\nested on:\nWikiToxic\nQ&A Eval\nPrivate data Q&A Eval\nTested on:\nWikiQA\nSummarization Eval\nSummarization performance\nTested on:\nGigaWorld, CNNDM, Xsum\nCode Generation Eval\nCode writing correctness and readability\nTested on:\nWikiSQL, HumanEval, CodeXGlu\nSupported Models.\nThe models are instantiated and usable in the LLM Eval function. The models are also directly callable with strings.\nCopy\nmodel \n=\n \nOpenAIModel\n(model_name\n=\n""gpt-4""\n,temperature\n=\n0.6\n)\n\n\nmodel\n(\n""What is the largest costal city in France?""\n)\nWe currently support a growing set of models for LLM Evals, please check out the \nAPI section for usage\n.\nModel\nSupport\nGPT-4\n✔\nGPT-3.5 Turbo\n✔\nGPT-3.5 Instruct\n✔\nAzure Hosted Open AI\n✔\nPalm 2 Vertex\n✔\nAWS Bedrock\n✔\nLitellm\n✔\nHuggingface Llama7B\n(use litellm)\nAnthropic\n✔\nCohere\n(use litellm)\nHow we benchmark pre-tested evals\nThe above diagram shows examples of different environments the Eval harness is desinged to run. The benchmarking environment is designed to enable the testing of the Eval model & Eval template performance against a designed set of data.\nThe above approach allows us to compare models easily in an understandable format:\nHallucination Eval\nGPT-4\nGPT-3.5\nPrecision\n0.94\n0.94\nRecall\n0.75\n0.71\nF1\n0.83\n0.81\nPrevious\nHow to: Evals\nNext\nHallucinations\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/evaluation\ntitle: Evaluators\n\nEvaluators\nEvaluation and benchmarking are crucial concepts in LLM development. To improve the performance of an LLM app (RAG, agents), you must have a way to measure it.\nPhoenix offers key modules to measure the quality of generated results as well as modules to measure retrieval quality.\nResponse Evaluation\n: Does the response match the retrieved context? Does it also match the query?\nRetrieval Evaluation\n: Are the retrieved sources relevant to the query?\nResponse Evaluation\nEvaluation of generated results can be challenging. Unlike traditional ML, the predicted results are not numeric or categorical, making it hard to define quantitative metrics for this problem.\nPhoenix offers \nLLM Evaluations\n, a module designed to measure the quality of results. This module uses a ""gold"" LLM (e.g. GPT-4) to decide whether the generated answer is correct in a variety of ways.\n\nNote that many of these evaluation criteria DO NOT require ground-truth labels. Evaluation can be done simply with a combination of the \ninput\n (query), \noutput\n (response), and \ncontext\n.\nLLM Evals supports the following response evaluation criteria:\nQA Correctness\n - Whether a question was correctly answered by the system based on the retrieved data. In contrast to retrieval Evals that are checks on chunks of data returned, this check is a system level check of a correct Q&A.\nHallucinations\n \n-\n Designed to detect LLM hallucinations relative to retrieved context\nToxicity\n - Identify if the AI response is racist, biased, or toxic\nResponse evaluations are a critical first step to figuring out whether your LLM App is running correctly. Response evaluations can pinpoint specific executions (a.k.a. traces) that are performing badly and can be aggregated up so that you can track how your application is running as a whole.\nEvaluations can be aggregated across executions to be used as KPIs\nRetrieval Evaluation\nPhoenix also provides evaluation of retrieval independently.\nThe concept of retrieval evaluation is not new; given a set of relevance scores for a set of retrieved documents, we can evaluate retrievers using retrieval metrics like \nprecision\n, \nNDCG\n, \nhit rate\n and more.\nLLM Evals supports the following retrieval evaluation criteria:\nRelevance\n - Evaluates whether a retrieved document chunk contains an answer to the query.\nRetrieval Evaluations can be run directly on application traces\nRetrieval is possibly the most important step in any LLM application as poor and/or incorrect retrieval can be the cause of bad response generation. If your application uses RAG to power an LLM, retrieval evals can help you identify the cause of hallucinations and incorrect answers.\nEvaluations\nInferences that contain generative records can be fed into evals to produce evaluations for analysis\nWith Phoenix\'s LLM Evals, evaluation results (or just \nEvaluations\n for short) is data consisting of 3 main columns:\nlabel\n: str [optional] - a classification label for the evaluation (e.g. ""hallucinated"" vs ""factual""). Can be used to calculate percentages (e.g. percent hallucinated) and can be used to filter down your data (e.g. \nEvals[""Hallucinations""].label == ""hallucinated""\n)\nscore\n: number [optional] - a numeric score for the evaluation (e.g. 1 for good, 0 for bad). Scores are great way to sort your data to surface poorly performing examples and can be used to filter your data by a threshold.\nexplanation\n: str [optional] - the reasoning for why the evaluation label or score was given. In the case of LLM evals, this is the evaluation model\'s reasoning. While explanations are optional, they can be extremely useful when trying to understand problematic areas of your application.\nLet\'s take a look at an example list of \nQ&A relevance\n evaluations:\nlabel\nexplanation\nscore\ncorrect\nThe reference text explains that YC was not or...\n1\ncorrect\nTo determine if the answer is correct, we need...\n1\nincorrect\nTo determine if the answer is correct, we must...\n0\ncorrect\nTo determine if the answer is correct, we need...\n1\nThese three columns combined can drive any type of evaluation you can imagine. \nlabel\n provides a way to classify responses, \nscore\n provides a way to assign a numeric assessment, and \nexplanation\n gives you a way to get qualitative feedback.\nEvaluating Traces\nAdding evaluations on traces can highlight problematic areas that require further analysis\nWith Phoenix, evaluations can be ""attached"" to the \nspans\n and \ndocuments\n collected. In order to facilitate this, Phoenix supports the following steps.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What pre-built evaluators does Phoenix support?\nAnswer: '}, {'role': 'assistant', 'content': 'Phoenix supports pre-built evaluators for Retrieval Eval, Hallucination Eval, Toxicity Eval, Q&A Eval, Summarization Eval, and Code Generation Eval.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6Mjg=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/llm-as-a-judge\ntitle: LLM as a Judge\n\nLLM as a Judge\nEvaluating tasks performed by LLMs can be difficult due to their complexity and the diverse criteria involved. Traditional methods like rule-based assessment or similarity metrics (e.g., ROUGE, BLEU) often fall short when applied to the nuanced and varied outputs of LLMs.\nFor instance, an AI assistant’s answer to a question can be:\nnot grounded in context\nrepetitive, repetitive, repetitive\ngrammatically incorrect\nexcessively lengthy and characterized by an overabundance of words\nincoherent\nThe list of criteria goes on. And even if we had a limited list, each of these would be hard to measure\nTo overcome this challenge, the concept of ""LLM as a Judge"" employs an LLM to evaluate another\'s output, combining human-like assessment with machine efficiency.\nHow It Works\nA brief description of how LLMs work as evaluators\nHere’s the step-by-step process for using an LLM as a judge:\nIdentify Evaluation Criteria -\n First, determine what you want to evaluate, be it hallucination, toxicity, accuracy, or another characteristic. See our \npre-tested evaluators\n for examples of what can be assessed.\nCraft Your Evaluation Prompt -\n Write a prompt template that will guide the evaluation. This template should clearly define what variables are needed from both the initial prompt and the LLM\'s response to effectively assess the output.\nSelect an Evaluation LLM -\n Choose the most suitable LLM from our available options for conducting your specific evaluations.\nGenerate Evaluations and View Results -\n Execute the evaluations across your data. This process allows for comprehensive testing without the need for manual annotation, enabling you to iterate quickly and refine your LLM\'s prompts.\nUsing an LLM as a judge significantly enhances the scalability and efficiency of the evaluation process. By employing this method, you can run thousands of evaluations across curated data without the need for human annotation.\nThis capability will not only speed up the iteration process for refining your LLM\'s prompts but will also ensure that you can deploy your models to production with confidence.\nPrevious\nConcepts: Evals\nNext\nEval Data Types\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals\ntitle: Concepts: Evals\n\nConcepts: Evals\nHow to leverage Evals to validate and monitor your application\nLLM as a Judge\nEval Data Types\nEvals With Explanations\nEvaluators\nRetrieval Evaluations\nResponse Evaluations\nEvaluation Results\nEvaluating Traces\nCustom Task Evaluations\nPrevious\nQuickstart: Evals\nNext\nLLM as a Judge\nLast updated \n29 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is LLM as a judge?\nAnswer: '}, {'role': 'assistant', 'content': 'LLM as a judge refers to the concept of using a Large Language Model (LLM) to evaluate the output or responses of another system or model. This approach combines the human-like assessment capabilities with the efficiency of machine learning, allowing for comprehensive testing and evaluation without the need for manual annotation.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/llm-as-a-judge\ntitle: LLM as a Judge\n\nLLM as a Judge\nEvaluating tasks performed by LLMs can be difficult due to their complexity and the diverse criteria involved. Traditional methods like rule-based assessment or similarity metrics (e.g., ROUGE, BLEU) often fall short when applied to the nuanced and varied outputs of LLMs.\nFor instance, an AI assistant’s answer to a question can be:\nnot grounded in context\nrepetitive, repetitive, repetitive\ngrammatically incorrect\nexcessively lengthy and characterized by an overabundance of words\nincoherent\nThe list of criteria goes on. And even if we had a limited list, each of these would be hard to measure\nTo overcome this challenge, the concept of ""LLM as a Judge"" employs an LLM to evaluate another\'s output, combining human-like assessment with machine efficiency.\nHow It Works\nA brief description of how LLMs work as evaluators\nHere’s the step-by-step process for using an LLM as a judge:\nIdentify Evaluation Criteria -\n First, determine what you want to evaluate, be it hallucination, toxicity, accuracy, or another characteristic. See our \npre-tested evaluators\n for examples of what can be assessed.\nCraft Your Evaluation Prompt -\n Write a prompt template that will guide the evaluation. This template should clearly define what variables are needed from both the initial prompt and the LLM\'s response to effectively assess the output.\nSelect an Evaluation LLM -\n Choose the most suitable LLM from our available options for conducting your specific evaluations.\nGenerate Evaluations and View Results -\n Execute the evaluations across your data. This process allows for comprehensive testing without the need for manual annotation, enabling you to iterate quickly and refine your LLM\'s prompts.\nUsing an LLM as a judge significantly enhances the scalability and efficiency of the evaluation process. By employing this method, you can run thousands of evaluations across curated data without the need for human annotation.\nThis capability will not only speed up the iteration process for refining your LLM\'s prompts but will also ensure that you can deploy your models to production with confidence.\nPrevious\nConcepts: Evals\nNext\nEval Data Types\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals\ntitle: Concepts: Evals\n\nConcepts: Evals\nHow to leverage Evals to validate and monitor your application\nLLM as a Judge\nEval Data Types\nEvals With Explanations\nEvaluators\nRetrieval Evaluations\nResponse Evaluations\nEvaluation Results\nEvaluating Traces\nCustom Task Evaluations\nPrevious\nQuickstart: Evals\nNext\nLLM as a Judge\nLast updated \n29 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is LLM as a judge?\nAnswer: '}, {'role': 'assistant', 'content': 'LLM as a judge refers to the concept of using a Large Language Model (LLM) to evaluate the output or responses of another system or model. This approach combines the human-like assessment capabilities with the efficiency of machine learning, allowing for comprehensive testing and evaluation without the need for manual annotation.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6Mjk=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/evals-with-explanations\ntitle: Evals With Explanations\n\nEvals With Explanations\nIt can be hard to understand in many cases why an LLM responds in a specific way. The explanation feature of Phoneix allows you to get a Eval output and an explanation from the LLM at the same time. We have found this incredibly useful for debugging LLM Evals.\nCopy\nfrom\n phoenix\n.\nevals \nimport\n (\n\n\n    RAG_RELEVANCY_PROMPT_RAILS_MAP\n,\n\n\n    RAG_RELEVANCY_PROMPT_TEMPLATE\n,\n\n\n    OpenAIModel\n,\n\n\n    download_benchmark_dataset\n,\n\n\n    llm_classify\n,\n\n\n)\n\n\n\n\nmodel \n=\n \nOpenAIModel\n(\n\n\n    model_name\n=\n""gpt-4""\n,\n\n\n    temperature\n=\n0.0\n,\n\n\n)\n\n\n\n\n#The rails is used to hold the output to specific values based on the template\n\n\n#It will remove text such as "",,,"" or ""...""\n\n\n#Will ensure the binary value expected from the template is returned\n\n\nrails \n=\n \nlist\n(RAG_RELEVANCY_PROMPT_RAILS_MAP.\nvalues\n())\n\n\nrelevance_classifications \n=\n \nllm_classify\n(\n\n\n    dataframe\n=\ndf,\n\n\n    template\n=\nRAG_RELEVANCY_PROMPT_TEMPLATE,\n\n\n    model\n=\nmodel,\n\n\n    rails\n=\nrails,\n\n\n    \nprovide_explanation\n=\nTrue\n\n\n)\n\n\n#relevance_classifications is a Dataframe with columns \'label\' and \'explanation\'\nThe flag above can be set with any of the templates or your own custom templates. The example below is from a relevance Evaluation.\nPrevious\nEval Data Types\nNext\nEvaluators\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/evaluation\ntitle: Evaluators\n\nexplanation\n: str [optional] - the reasoning for why the evaluation label or score was given. In the case of LLM evals, this is the evaluation model\'s reasoning. While explanations are optional, they can be extremely useful when trying to understand problematic areas of your application.\nLet\'s take a look at an example list of \nQ&A relevance\n evaluations:\nlabel\nexplanation\nscore\ncorrect\nThe reference text explains that YC was not or...\n1\ncorrect\nTo determine if the answer is correct, we need...\n1\nincorrect\nTo determine if the answer is correct, we must...\n0\ncorrect\nTo determine if the answer is correct, we need...\n1\nThese three columns combined can drive any type of evaluation you can imagine. \nlabel\n provides a way to classify responses, \nscore\n provides a way to assign a numeric assessment, and \nexplanation\n gives you a way to get qualitative feedback.\nEvaluating Traces\nAdding evaluations on traces can highlight problematic areas that require further analysis\nWith Phoenix, evaluations can be ""attached"" to the \nspans\n and \ndocuments\n collected. In order to facilitate this, Phoenix supports the following steps.\nQuerying and downloading data\n - query the spans collected by phoenix and materialize them into DataFrames to be used for evaluation (e.g. question and answer data, documents data).\nRunning Evaluations\n - the data queried in step 1 can be fed into LLM Evals to produce evaluation results.\nLogging Evaluations\n - the evaluations performed in the above step can be logged back to Phoenix to be attached to spans and documents for evaluating responses and retrieval. See \nhere\n on how to log evaluations to Phoenix.\nSorting and Filtering by Evaluation\n - once the evaluations have been logged back to Phoenix, the spans become instantly sortable and filterable by the evaluation values that you attached to the spans. (An example of an evaluation filter would be \nEval[""hallucination""].label == ""hallucinated""\n)\nEnd-to-end evaluation flow\nBy following the above steps, you will have a full end-to-end flow for troubleshooting, evaluating, and root-causing an LLM application. By using LLM Evals in conjunction with Traces, you will be able to surface up problematic queries, get an explanation as to why the the generation is problematic (e.x. \nhallucinated\n because ...), and be able to identify which step of your generative app requires improvement (e.x. did the LLM hallucinate or was the LLM fed bad context?).\\\nIn the above screenshot you can see how poor retrieval directly correlates with hallucinations\nFor a full tutorial on LLM Ops, check out our tutorial below.\nPrevious\nEvals With Explanations\nNext\nCustom Task Evaluation\nLast updated \n3 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I enable explanations for evals?\nAnswer: '}, {'role': 'assistant', 'content': 'Set the flag `provide_explanation` to True when calling the `llm_classify` function to enable explanations for evals.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/evals-with-explanations\ntitle: Evals With Explanations\n\nEvals With Explanations\nIt can be hard to understand in many cases why an LLM responds in a specific way. The explanation feature of Phoneix allows you to get a Eval output and an explanation from the LLM at the same time. We have found this incredibly useful for debugging LLM Evals.\nCopy\nfrom\n phoenix\n.\nevals \nimport\n (\n\n\n    RAG_RELEVANCY_PROMPT_RAILS_MAP\n,\n\n\n    RAG_RELEVANCY_PROMPT_TEMPLATE\n,\n\n\n    OpenAIModel\n,\n\n\n    download_benchmark_dataset\n,\n\n\n    llm_classify\n,\n\n\n)\n\n\n\n\nmodel \n=\n \nOpenAIModel\n(\n\n\n    model_name\n=\n""gpt-4""\n,\n\n\n    temperature\n=\n0.0\n,\n\n\n)\n\n\n\n\n#The rails is used to hold the output to specific values based on the template\n\n\n#It will remove text such as "",,,"" or ""...""\n\n\n#Will ensure the binary value expected from the template is returned\n\n\nrails \n=\n \nlist\n(RAG_RELEVANCY_PROMPT_RAILS_MAP.\nvalues\n())\n\n\nrelevance_classifications \n=\n \nllm_classify\n(\n\n\n    dataframe\n=\ndf,\n\n\n    template\n=\nRAG_RELEVANCY_PROMPT_TEMPLATE,\n\n\n    model\n=\nmodel,\n\n\n    rails\n=\nrails,\n\n\n    \nprovide_explanation\n=\nTrue\n\n\n)\n\n\n#relevance_classifications is a Dataframe with columns \'label\' and \'explanation\'\nThe flag above can be set with any of the templates or your own custom templates. The example below is from a relevance Evaluation.\nPrevious\nEval Data Types\nNext\nEvaluators\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/evaluation\ntitle: Evaluators\n\nexplanation\n: str [optional] - the reasoning for why the evaluation label or score was given. In the case of LLM evals, this is the evaluation model\'s reasoning. While explanations are optional, they can be extremely useful when trying to understand problematic areas of your application.\nLet\'s take a look at an example list of \nQ&A relevance\n evaluations:\nlabel\nexplanation\nscore\ncorrect\nThe reference text explains that YC was not or...\n1\ncorrect\nTo determine if the answer is correct, we need...\n1\nincorrect\nTo determine if the answer is correct, we must...\n0\ncorrect\nTo determine if the answer is correct, we need...\n1\nThese three columns combined can drive any type of evaluation you can imagine. \nlabel\n provides a way to classify responses, \nscore\n provides a way to assign a numeric assessment, and \nexplanation\n gives you a way to get qualitative feedback.\nEvaluating Traces\nAdding evaluations on traces can highlight problematic areas that require further analysis\nWith Phoenix, evaluations can be ""attached"" to the \nspans\n and \ndocuments\n collected. In order to facilitate this, Phoenix supports the following steps.\nQuerying and downloading data\n - query the spans collected by phoenix and materialize them into DataFrames to be used for evaluation (e.g. question and answer data, documents data).\nRunning Evaluations\n - the data queried in step 1 can be fed into LLM Evals to produce evaluation results.\nLogging Evaluations\n - the evaluations performed in the above step can be logged back to Phoenix to be attached to spans and documents for evaluating responses and retrieval. See \nhere\n on how to log evaluations to Phoenix.\nSorting and Filtering by Evaluation\n - once the evaluations have been logged back to Phoenix, the spans become instantly sortable and filterable by the evaluation values that you attached to the spans. (An example of an evaluation filter would be \nEval[""hallucination""].label == ""hallucinated""\n)\nEnd-to-end evaluation flow\nBy following the above steps, you will have a full end-to-end flow for troubleshooting, evaluating, and root-causing an LLM application. By using LLM Evals in conjunction with Traces, you will be able to surface up problematic queries, get an explanation as to why the the generation is problematic (e.x. \nhallucinated\n because ...), and be able to identify which step of your generative app requires improvement (e.x. did the LLM hallucinate or was the LLM fed bad context?).\\\nIn the above screenshot you can see how poor retrieval directly correlates with hallucinations\nFor a full tutorial on LLM Ops, check out our tutorial below.\nPrevious\nEvals With Explanations\nNext\nCustom Task Evaluation\nLast updated \n3 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I enable explanations for evals?\nAnswer: '}, {'role': 'assistant', 'content': 'Set the flag `provide_explanation` to True when calling the `llm_classify` function to enable explanations for evals.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MzA=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/quickstart\ntitle: Quickstart\n\nQuickstart\nThe easiest way to run Phoenix is locally in your own computer. To launch Phoenix, use the following steps.\nInstall\nUsing pip\nUsing conda\nDocker\napp.phoenix.arize.com\nCopy\npip\n \ninstall\n \narize-phoenix\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix\nPhoenix server images are available via \nDocker Hub\n and can be used via \ndocker compose \nor if you simply want a long-running phoenix instance to share with your team.\nCopy\ndocker\n \npull\n \narizephoenix/phoenix:latest\nIf you don\'t want to host an instance of Phoenix yourself or use a notebook instance, you can use a persistent instance provided on our site. Sign up for an Arize Phoenix account at \nhttps://app.phoenix.arize.com/login\nFor more details, see \nHosted Phoenix\nLaunch Phoenix\nLaunching phoenix can be done in many ways depending on your use-case.\nCommand Line\nDocker\nNotebook\napp.phoenix.arize.com\nLaunch your local Phoenix instance using:\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \nserve\nFor details on customizing a local terminal deployment, see \nTerminal Setup\n.\nLaunch the phoenix docker image using:\nCopy\ndocker\n \nrun\n \n-p\n \n6006:6006\n \n-p\n \n4317:4317\n \narizephoenix/phoenix:latest\nThis will expose the Phoenix UI and REST API on \nlocalhost:6006\n and exposes the gRPC endpoint for spans on \nlocalhost:4317\nFor more details on customizing a docker deployment, see \nDocker\nWithin your notebook, launch Phoenix using:\nCopy\nimport\n phoenix \nas\n px\n\n\npx\n.\nlaunch_app\n()\nBy default, notebook instances do not have persistent storage, so your traces will disappear after the notebook is closed. See \nPersistence\n or use one of the other deployment options to retain traces.\nHosted Phoenix instances are always online. Nothing more to do here!\nConnect your App\nTo collect traces from your application, you must configure an OpenTelemetry TracerProvider to send traces to Phoenix. The \nregister\n utility from the \nphoenix.otel\n module streamlines this process.\nPython\nTypeScript\nNotebook\napp.phoenix.arize.com\nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your instance using:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n  \nYou do not have to use phoenix.otel to connect to your phoenix instance, you can use OpenTelemetry itself to initialize  your OTEL connection. See\nUsing OTEL Python Directly\nSee \nSetup Tracing: Python\n for more details on configuration and setup\nFor setting up tracing and OpenTelemetry with TypeScript, see \nSetup Tracing: TS\nConnect your notebook to Phoenix:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n \nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your cloud instance using:\nCopy\nimport\n os\n\n\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# Add Phoenix API Key for tracing\n\n\nos\n.\nenviron\n[\n""OTEL_EXPORTER_OTLP_HEADERS""\n]\n \n=\n f\n""api_key=\n{PHOENIX_API_KEY}\n""\n\n\n\n\n# configure the Phoenix tracer\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""https://app.phoenix.arize.com/v1/traces""\n,\n\n\n)\n \nYour Phoenix API key can be found on the Keys section of your dashboard.\nNext Steps\nTrace\n a running application\nRun \nevaluations\n on traces\nTest changes to you prompts, models, and application via \nexperiments\nPrevious\nArize Phoenix\nNext\nHosted Phoenix\nLast updated \n9 hours ago\n\nsource: https://docs.arize.com/phoenix/reference/contribute-to-phoenix\ntitle: Contribute to Phoenix\n\nContribute to Phoenix\nIf you want to contribute to the cutting edge of LLM and ML Observability, you\'ve come to the right place!\nTo get started, please check out the following:\nOur development guide\nCode of conduct\nContribution License Agreement\nPicking a GitHub Issue\nWe encourage you to start with an issue labeled with the tag \ngood first issue\n on theGitHub issue board, to get familiar with our codebase as a first-time contributor.\nSubmit Your Code\nTo submit your code, \nfork the Phoenix repository\n, create a \nnew branch\n on your fork, and open \na Pull Request (PR)\n once your work is ready for review.\nIn the PR template, please describe the change, including the motivation/context, test coverage, and any other relevant information. Please note if the PR is a breaking change or if it is related to an open GitHub issue.\nA Core reviewer will review your PR in around one business day and provide feedback on any changes it requires to be approved. Once approved and all the tests pass, the reviewer will click the Squash and merge button in Github 🥳.\nYour PR is now merged into Phoenix! We’ll shout out your contribution in the release notes.\nPrevious\nOpenInference\nLast updated \n6 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I get started?\nAnswer: '}, {'role': 'assistant', 'content': 'To get started, you can follow the steps outlined in the Quickstart guide provided in the documentation. This includes installing Phoenix locally on your computer using pip, conda, or Docker, or using a persistent instance provided on the Arize Phoenix site. Once installed, you can launch Phoenix using different methods such as Command Line, Docker, or within a notebook. Additionally, you can connect your application to Phoenix by configuring an OpenTelemetry TracerProvider.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/quickstart\ntitle: Quickstart\n\nQuickstart\nThe easiest way to run Phoenix is locally in your own computer. To launch Phoenix, use the following steps.\nInstall\nUsing pip\nUsing conda\nDocker\napp.phoenix.arize.com\nCopy\npip\n \ninstall\n \narize-phoenix\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix\nPhoenix server images are available via \nDocker Hub\n and can be used via \ndocker compose \nor if you simply want a long-running phoenix instance to share with your team.\nCopy\ndocker\n \npull\n \narizephoenix/phoenix:latest\nIf you don\'t want to host an instance of Phoenix yourself or use a notebook instance, you can use a persistent instance provided on our site. Sign up for an Arize Phoenix account at \nhttps://app.phoenix.arize.com/login\nFor more details, see \nHosted Phoenix\nLaunch Phoenix\nLaunching phoenix can be done in many ways depending on your use-case.\nCommand Line\nDocker\nNotebook\napp.phoenix.arize.com\nLaunch your local Phoenix instance using:\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \nserve\nFor details on customizing a local terminal deployment, see \nTerminal Setup\n.\nLaunch the phoenix docker image using:\nCopy\ndocker\n \nrun\n \n-p\n \n6006:6006\n \n-p\n \n4317:4317\n \narizephoenix/phoenix:latest\nThis will expose the Phoenix UI and REST API on \nlocalhost:6006\n and exposes the gRPC endpoint for spans on \nlocalhost:4317\nFor more details on customizing a docker deployment, see \nDocker\nWithin your notebook, launch Phoenix using:\nCopy\nimport\n phoenix \nas\n px\n\n\npx\n.\nlaunch_app\n()\nBy default, notebook instances do not have persistent storage, so your traces will disappear after the notebook is closed. See \nPersistence\n or use one of the other deployment options to retain traces.\nHosted Phoenix instances are always online. Nothing more to do here!\nConnect your App\nTo collect traces from your application, you must configure an OpenTelemetry TracerProvider to send traces to Phoenix. The \nregister\n utility from the \nphoenix.otel\n module streamlines this process.\nPython\nTypeScript\nNotebook\napp.phoenix.arize.com\nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your instance using:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n  \nYou do not have to use phoenix.otel to connect to your phoenix instance, you can use OpenTelemetry itself to initialize  your OTEL connection. See\nUsing OTEL Python Directly\nSee \nSetup Tracing: Python\n for more details on configuration and setup\nFor setting up tracing and OpenTelemetry with TypeScript, see \nSetup Tracing: TS\nConnect your notebook to Phoenix:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n \nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your cloud instance using:\nCopy\nimport\n os\n\n\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# Add Phoenix API Key for tracing\n\n\nos\n.\nenviron\n[\n""OTEL_EXPORTER_OTLP_HEADERS""\n]\n \n=\n f\n""api_key=\n{PHOENIX_API_KEY}\n""\n\n\n\n\n# configure the Phoenix tracer\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""https://app.phoenix.arize.com/v1/traces""\n,\n\n\n)\n \nYour Phoenix API key can be found on the Keys section of your dashboard.\nNext Steps\nTrace\n a running application\nRun \nevaluations\n on traces\nTest changes to you prompts, models, and application via \nexperiments\nPrevious\nArize Phoenix\nNext\nHosted Phoenix\nLast updated \n9 hours ago\n\nsource: https://docs.arize.com/phoenix/reference/contribute-to-phoenix\ntitle: Contribute to Phoenix\n\nContribute to Phoenix\nIf you want to contribute to the cutting edge of LLM and ML Observability, you\'ve come to the right place!\nTo get started, please check out the following:\nOur development guide\nCode of conduct\nContribution License Agreement\nPicking a GitHub Issue\nWe encourage you to start with an issue labeled with the tag \ngood first issue\n on theGitHub issue board, to get familiar with our codebase as a first-time contributor.\nSubmit Your Code\nTo submit your code, \nfork the Phoenix repository\n, create a \nnew branch\n on your fork, and open \na Pull Request (PR)\n once your work is ready for review.\nIn the PR template, please describe the change, including the motivation/context, test coverage, and any other relevant information. Please note if the PR is a breaking change or if it is related to an open GitHub issue.\nA Core reviewer will review your PR in around one business day and provide feedback on any changes it requires to be approved. Once approved and all the tests pass, the reviewer will click the Squash and merge button in Github 🥳.\nYour PR is now merged into Phoenix! We’ll shout out your contribution in the release notes.\nPrevious\nOpenInference\nLast updated \n6 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I get started?\nAnswer: '}, {'role': 'assistant', 'content': 'To get started, you can follow the steps outlined in the Quickstart guide provided in the documentation. This includes installing Phoenix locally on your computer using pip, conda, or Docker, or using a persistent instance provided on the Arize Phoenix site. Once installed, you can launch Phoenix using different methods such as Command Line, Docker, or within a notebook. Additionally, you can connect your application to Phoenix by configuring an OpenTelemetry TracerProvider.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MzE=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/evaluation-types\ntitle: Eval Data Types\n\nEval Data Types\nThere are a multiple types of evaluations supported by the Phoenix Library. Each category of evaluation is categorized by its output type.\n \nCategorical (binary) -\n The evaluation results in a binary output, such as true/false or yes/no, which can be easily represented as 1/0. This simplicity makes it straightforward for decision-making processes but lacks the ability to capture nuanced judgements.\nCategorical (Multi-class) -\n The evaluation results in one of several predefined categories or classes, which could be text labels or distinct numbers representing different states or types.\nScore -\n The evaluation results is a numeric value within a set range (e.g. 1-10), offering a scale of measurement.\nAlthough score evals are an option in Phoenix, we recommend using categorical evaluations in production environments. LLMs often struggle with the subtleties of continuous scales, leading to inconsistent results even with slight prompt modifications or across different models. Repeated tests have shown that scores can fluctuate significantly, which is problematic when evaluating at scale.\nCategorical evals, especially multi-class, strike a balance between simplicity and the ability to convey distinct evaluative outcomes, making them more suitable for applications where precise and consistent decision-making is important.\nTo explore the full analysis behind our recommendation and understand the limitations of score-based evaluations, check out \nour research\n on LLM eval data types.\nPrevious\nLLM as a Judge\nNext\nEvals With Explanations\nLast updated \n4 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals\ntitle: Concepts: Evals\n\nConcepts: Evals\nHow to leverage Evals to validate and monitor your application\nLLM as a Judge\nEval Data Types\nEvals With Explanations\nEvaluators\nRetrieval Evaluations\nResponse Evaluations\nEvaluation Results\nEvaluating Traces\nCustom Task Evaluations\nPrevious\nQuickstart: Evals\nNext\nLLM as a Judge\nLast updated \n29 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Should I use quantitative or qualitative data types for evals?\nAnswer: '}, {'role': 'assistant', 'content': 'You should use qualitative data types for evals.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/evaluation-types\ntitle: Eval Data Types\n\nEval Data Types\nThere are a multiple types of evaluations supported by the Phoenix Library. Each category of evaluation is categorized by its output type.\n \nCategorical (binary) -\n The evaluation results in a binary output, such as true/false or yes/no, which can be easily represented as 1/0. This simplicity makes it straightforward for decision-making processes but lacks the ability to capture nuanced judgements.\nCategorical (Multi-class) -\n The evaluation results in one of several predefined categories or classes, which could be text labels or distinct numbers representing different states or types.\nScore -\n The evaluation results is a numeric value within a set range (e.g. 1-10), offering a scale of measurement.\nAlthough score evals are an option in Phoenix, we recommend using categorical evaluations in production environments. LLMs often struggle with the subtleties of continuous scales, leading to inconsistent results even with slight prompt modifications or across different models. Repeated tests have shown that scores can fluctuate significantly, which is problematic when evaluating at scale.\nCategorical evals, especially multi-class, strike a balance between simplicity and the ability to convey distinct evaluative outcomes, making them more suitable for applications where precise and consistent decision-making is important.\nTo explore the full analysis behind our recommendation and understand the limitations of score-based evaluations, check out \nour research\n on LLM eval data types.\nPrevious\nLLM as a Judge\nNext\nEvals With Explanations\nLast updated \n4 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals\ntitle: Concepts: Evals\n\nConcepts: Evals\nHow to leverage Evals to validate and monitor your application\nLLM as a Judge\nEval Data Types\nEvals With Explanations\nEvaluators\nRetrieval Evaluations\nResponse Evaluations\nEvaluation Results\nEvaluating Traces\nCustom Task Evaluations\nPrevious\nQuickstart: Evals\nNext\nLLM as a Judge\nLast updated \n29 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Should I use quantitative or qualitative data types for evals?\nAnswer: '}, {'role': 'assistant', 'content': 'You should use qualitative data types for evals.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MzI=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/llm-evaluations\ntitle: Log Evaluation Results\n\nLog Evaluation Results\nThis guide shows how LLM evaluation results in dataframes can be sent to Phoenix.\nAn evaluation must have a \nname\n (e.g. ""Q&A Correctness"") and its DataFrame must contain identifiers for the subject of evaluation, e.g. a span or a document (more on that below), and values under either the \nscore\n, \nlabel\n, or \nexplanation\n columns. See \nEvaluations\n for more information.\nSpan Evaluations\nA dataframe of span evaluations would look similar like the table below. It must contain \nspan_id\n as an index or as a column. Once ingested, Phoenix uses the \nspan_id\n to associate the evaluation with its target span.\nspan_id\nlabel\nvalue\nexplanation\n5B8EF798A381\ncorrect\n1\n""this is correct ...""\nE19B7EC3GG02\nincorrect\n0\n""this is incorrect ...""\nThe evaluations dataframe can be sent to Phoenix as follows. Note that the name of the evaluation must be supplied through the \neval_name=\n parameter. In this case we name it ""Q&A Correctness"".\nCopy\nfrom\n phoenix\n.\ntrace \nimport\n SpanEvaluations\n\n\n\n\npx\n.\nClient\n().\nlog_evaluations\n(\n\n\n    \nSpanEvaluations\n(\n\n\n        dataframe\n=\nqa_correctness_eval_df,\n\n\n        eval_name\n=\n""Q&A Correctness""\n,\n\n\n    ),\n\n\n)\nDocument Evaluations\nA dataframe of document evaluations would look something like the table below. It must contain \nspan_id\n and \ndocument_position\n as either indices or columns. \ndocument_position\n is the document\'s (zero-based) index in the span\'s list of retrieved documents. Once ingested, Phoenix uses the \nspan_id\n and \ndocument_position\n to associate the evaluation with its target span and document.\nspan_id\ndocument_position\nlabel\nscore\nexplanation\n5B8EF798A381\n0\nrelevant\n1\n""this is ...""\n5B8EF798A381\n1\nirrelevant\n0\n""this is ...""\nE19B7EC3GG02\n0\nrelevant\n1\n""this is ...""\nThe evaluations dataframe can be sent to Phoenix as follows. Note that the name of the evaluation must be supplied through the \neval_name=\n parameter. In this case we name it ""Relevance"".\nCopy\nfrom\n phoenix\n.\ntrace \nimport\n DocumentEvaluations\n\n\n\n\npx\n.\nClient\n().\nlog_evaluations\n(\n\n\n    \nDocumentEvaluations\n(\n\n\n        dataframe\n=\ndocument_relevance_eval_df,\n\n\n        eval_name\n=\n""Relevance""\n,\n\n\n    ),\n\n\n)\nLogging Multiple Evaluation DataFrames\nMultiple sets of Evaluations can be logged by the same \npx.Client().log_evaluations()\n function call.\nCopy\npx.Client().log_evaluations(\n\n\n    SpanEvaluations(\n\n\n        dataframe=qa_correctness_eval_df,\n\n\n        eval_name=""Q&A Correctness"",\n\n\n    ),\n\n\n    DocumentEvaluations(\n\n\n        dataframe=document_relevance_eval_df,\n\n\n        eval_name=""Relevance"",\n\n\n    ),\n\n\n    SpanEvaluations(\n\n\n        dataframe=hallucination_eval_df,\n\n\n        eval_name=""Hallucination"",\n\n\n    ),\n\n\n    # ... as many as you like\n\n\n)\nSpecifying A Project for the Evaluations\nBy default the client will push traces to the project specified in the \nPHOENIX_PROJECT_NAME\n environment variable or to the \ndefault\n project. If you want to specify the destination project explicitly, you can pass the project name as a parameter.\nCopy\nfrom\n phoenix\n.\ntrace \nimport\n SpanEvaluations\n\n\n\n\npx\n.\nClient\n().\nlog_evaluations\n(\n\n\n    \nSpanEvaluations\n(\n\n\n        dataframe\n=\nqa_correctness_eval_df,\n\n\n        eval_name\n=\n""Q&A Correctness""\n,\n\n\n    ),\n\n\n    project_name\n=\n""<my-project>""\n\n\n)\nPrevious\nQuerying Spans\nNext\nImporting Existing Traces\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals/online-evals\ntitle: Online Evals\n\nOnline Evals\nYou can use cron to run evals client-side as your traces and spans are generated, augmenting your dataset with evaluations in an online manner. View the \nexample in Github\n.\nThis example:\nContinuously queries a LangChain application to send new traces and spans to your Phoenix session\nQueries new spans once per minute and runs evals, including:\nHallucination\nQ&A Correctness\nRelevance\nLogs evaluations back to Phoenix so they appear in the UI\nThe evaluation script is run as a cron job, enabling you to adjust the frequency of the evaluation job:\nCopy\n* * * * * /path/to/python /path/to/run_evals.py\nThe above script can be run periodically to augment Evals in Phoenix.\nPrevious\nBring Your Own Evaluator\nNext\nEvaluation Models\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I send in an evaluation?\nAnswer: '}, {'role': 'assistant', 'content': 'You can send in an evaluation by using the `log_evaluations` function provided by the Phoenix client. The evaluation must have a name and its DataFrame must contain identifiers for the subject of evaluation, such as a span or a document, along with values under either the score, label, or explanation columns. The name of the evaluation must be supplied through the `eval_name=` parameter when sending the evaluation to Phoenix.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/llm-evaluations\ntitle: Log Evaluation Results\n\nLog Evaluation Results\nThis guide shows how LLM evaluation results in dataframes can be sent to Phoenix.\nAn evaluation must have a \nname\n (e.g. ""Q&A Correctness"") and its DataFrame must contain identifiers for the subject of evaluation, e.g. a span or a document (more on that below), and values under either the \nscore\n, \nlabel\n, or \nexplanation\n columns. See \nEvaluations\n for more information.\nSpan Evaluations\nA dataframe of span evaluations would look similar like the table below. It must contain \nspan_id\n as an index or as a column. Once ingested, Phoenix uses the \nspan_id\n to associate the evaluation with its target span.\nspan_id\nlabel\nvalue\nexplanation\n5B8EF798A381\ncorrect\n1\n""this is correct ...""\nE19B7EC3GG02\nincorrect\n0\n""this is incorrect ...""\nThe evaluations dataframe can be sent to Phoenix as follows. Note that the name of the evaluation must be supplied through the \neval_name=\n parameter. In this case we name it ""Q&A Correctness"".\nCopy\nfrom\n phoenix\n.\ntrace \nimport\n SpanEvaluations\n\n\n\n\npx\n.\nClient\n().\nlog_evaluations\n(\n\n\n    \nSpanEvaluations\n(\n\n\n        dataframe\n=\nqa_correctness_eval_df,\n\n\n        eval_name\n=\n""Q&A Correctness""\n,\n\n\n    ),\n\n\n)\nDocument Evaluations\nA dataframe of document evaluations would look something like the table below. It must contain \nspan_id\n and \ndocument_position\n as either indices or columns. \ndocument_position\n is the document\'s (zero-based) index in the span\'s list of retrieved documents. Once ingested, Phoenix uses the \nspan_id\n and \ndocument_position\n to associate the evaluation with its target span and document.\nspan_id\ndocument_position\nlabel\nscore\nexplanation\n5B8EF798A381\n0\nrelevant\n1\n""this is ...""\n5B8EF798A381\n1\nirrelevant\n0\n""this is ...""\nE19B7EC3GG02\n0\nrelevant\n1\n""this is ...""\nThe evaluations dataframe can be sent to Phoenix as follows. Note that the name of the evaluation must be supplied through the \neval_name=\n parameter. In this case we name it ""Relevance"".\nCopy\nfrom\n phoenix\n.\ntrace \nimport\n DocumentEvaluations\n\n\n\n\npx\n.\nClient\n().\nlog_evaluations\n(\n\n\n    \nDocumentEvaluations\n(\n\n\n        dataframe\n=\ndocument_relevance_eval_df,\n\n\n        eval_name\n=\n""Relevance""\n,\n\n\n    ),\n\n\n)\nLogging Multiple Evaluation DataFrames\nMultiple sets of Evaluations can be logged by the same \npx.Client().log_evaluations()\n function call.\nCopy\npx.Client().log_evaluations(\n\n\n    SpanEvaluations(\n\n\n        dataframe=qa_correctness_eval_df,\n\n\n        eval_name=""Q&A Correctness"",\n\n\n    ),\n\n\n    DocumentEvaluations(\n\n\n        dataframe=document_relevance_eval_df,\n\n\n        eval_name=""Relevance"",\n\n\n    ),\n\n\n    SpanEvaluations(\n\n\n        dataframe=hallucination_eval_df,\n\n\n        eval_name=""Hallucination"",\n\n\n    ),\n\n\n    # ... as many as you like\n\n\n)\nSpecifying A Project for the Evaluations\nBy default the client will push traces to the project specified in the \nPHOENIX_PROJECT_NAME\n environment variable or to the \ndefault\n project. If you want to specify the destination project explicitly, you can pass the project name as a parameter.\nCopy\nfrom\n phoenix\n.\ntrace \nimport\n SpanEvaluations\n\n\n\n\npx\n.\nClient\n().\nlog_evaluations\n(\n\n\n    \nSpanEvaluations\n(\n\n\n        dataframe\n=\nqa_correctness_eval_df,\n\n\n        eval_name\n=\n""Q&A Correctness""\n,\n\n\n    ),\n\n\n    project_name\n=\n""<my-project>""\n\n\n)\nPrevious\nQuerying Spans\nNext\nImporting Existing Traces\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals/online-evals\ntitle: Online Evals\n\nOnline Evals\nYou can use cron to run evals client-side as your traces and spans are generated, augmenting your dataset with evaluations in an online manner. View the \nexample in Github\n.\nThis example:\nContinuously queries a LangChain application to send new traces and spans to your Phoenix session\nQueries new spans once per minute and runs evals, including:\nHallucination\nQ&A Correctness\nRelevance\nLogs evaluations back to Phoenix so they appear in the UI\nThe evaluation script is run as a cron job, enabling you to adjust the frequency of the evaluation job:\nCopy\n* * * * * /path/to/python /path/to/run_evals.py\nThe above script can be run periodically to augment Evals in Phoenix.\nPrevious\nBring Your Own Evaluator\nNext\nEvaluation Models\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I send in an evaluation?\nAnswer: '}, {'role': 'assistant', 'content': 'You can send in an evaluation by using the `log_evaluations` function provided by the Phoenix client. The evaluation must have a name and its DataFrame must contain identifiers for the subject of evaluation, such as a span or a document, along with values under either the score, label, or explanation columns. The name of the evaluation must be supplied through the `eval_name=` parameter when sending the evaluation to Phoenix.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MzM=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/concepts-datasets\ntitle: Concepts: Datasets\n\nConcepts: Datasets\nThere are many ways to build datasets for experimentation and evaluation.\nDatasets\nDatasets are integral to evaluation and experimentation. They are collections of examples that provide the \ninputs\n and, optionally, expected \nreference\n outputs for assessing your application. Each example within a dataset represents a single data point, consisting of an \ninputs\n dictionary, an optional \noutput\n dictionary, and an optional \nmetadata\n dictionary. The \noptional\n output dictionary often contains the the expected LLM application output for the given input.\nDatasets allow you to collect data from production, staging, evaluations, and even manually. The examples collected are then used to run experiments and evaluations to track improvements.\nUse datasets to:\nStore evaluation test cases for your eval script instead of managing large JSONL or CSV files\nCapture generations to assess quality manually or using LLM-graded evals\nStore user reviewed generations to find new test cases\nWith Phoenix, datasets are:\nIntegrated\n. Datasets are integrated with the platform, so you can add production spans to datasets, use datasets to run experiments, and use metadata to track different segments and use-cases.\nVersioned\n. Every insert, update, and delete is versioned, so you can pin experiments and evaluations to a specific version of a dataset and track changes over time.\nCreating Datasets\nThere are various ways to get started with datasets:\nManually Curated Examples\nThis is how we recommend you start. From building your application, you probably have an idea of what types of inputs you expect your application to be able to handle, and what ""good"" responses look like. You probably want to cover a few different common edge cases or situations you can imagine. Even 20 high quality, manually curated examples can go a long way.\nHistorical Logs\nOnce you ship an application, you start gleaning valuable information: how users are actually using it. This information can be valuable to capture and store in datasets. This allows you to test against specific use cases as you iterate on your application.\nIf your application is going well, you will likely get a lot of usage. How can you determine which datapoints are valuable to add? There are a few heuristics you can follow. If possible, try to collect end user feedback. You can then see which datapoints got negative feedback. That is super valuable! These are spots where your application did not perform well. You should add these to your dataset to test against in the future. You can also use other heuristics to identify interesting datapoints - for example, runs that took a long time to complete could be interesting to analyze and add to a dataset.\nSynthetic Data\nOnce you have a few examples, you can try to artificially generate examples to get a lot of datapoints quickly. It\'s generally advised to have a few good handcrafted examples before this step, as the synthetic data will often resemble the source examples in some way. \nDataset Contents\nWhile Phoenix doesn\'t have dataset types, conceptually you can contain:\nKey-Value Pairs:\nInputs and outputs are arbitrary key-value pairs.\nThis dataset type is ideal for evaluating prompts, functions, and agents that require multiple inputs or generate multiple outputs.\nPrompt Template\nIf you have a RAG prompt template such as:\nCopy\nGiven the context information and not prior knowledge, answer the query.\n\n\n---------------------\n\n\n{context}\n\n\n---------------------\n\n\n\n\nQuery: {query}\n\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/overview-datasets\ntitle: Overview: Datasets\n\nOverview: Datasets\nThe velocity of AI application development is bottlenecked by quality evaluations because AI engineers are often faced with hard tradeoffs: which prompt or LLM best balances performance, latency, and cost. High quality evaluations are critical as they can help developers answer these types of questions with greater confidence.\nDatasets\nDatasets are integral to evaluation. They are collections of examples that provide the \ninputs\n and, optionally, expected \nreference\n outputs for assessing your application.  Datasets allow you to collect data from production, staging, evaluations, and even manually. The examples collected are used to run experiments and evaluations to track improvements to your prompt, LLM, or other parts of your LLM application.\nExperiments\nIn AI development, it\'s hard to understand how a change will affect performance. This breaks the dev flow, making iteration more guesswork than engineering.\nExperiments and evaluations solve this, helping distill the indeterminism of LLMs into tangible feedback that helps you ship more reliable product.\nSpecifically, good evals help you:\nUnderstand whether an update is an improvement or a regression\nDrill down into good / bad examples\nCompare specific examples vs. prior runs\nAvoid guesswork\n\n\nPrevious\nEvaluation Models\nNext\nQuickstart: Datasets\nLast updated \n22 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Tell me about datasets\nAnswer: '}, {'role': 'assistant', 'content': 'Datasets are collections of examples that provide inputs and, optionally, expected reference outputs for assessing applications. Each example within a dataset represents a single data point, consisting of an inputs dictionary, an optional output dictionary, and an optional metadata dictionary. Datasets allow for the collection of data from various sources like production, staging, evaluations, and manual inputs. These examples are then used to run experiments and evaluations to track improvements in applications. Datasets can be created manually, from historical logs, or through synthetic data generation. They are integrated and versioned in platforms like Phoenix, allowing for easy management and tracking of changes over time.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/concepts-datasets\ntitle: Concepts: Datasets\n\nConcepts: Datasets\nThere are many ways to build datasets for experimentation and evaluation.\nDatasets\nDatasets are integral to evaluation and experimentation. They are collections of examples that provide the \ninputs\n and, optionally, expected \nreference\n outputs for assessing your application. Each example within a dataset represents a single data point, consisting of an \ninputs\n dictionary, an optional \noutput\n dictionary, and an optional \nmetadata\n dictionary. The \noptional\n output dictionary often contains the the expected LLM application output for the given input.\nDatasets allow you to collect data from production, staging, evaluations, and even manually. The examples collected are then used to run experiments and evaluations to track improvements.\nUse datasets to:\nStore evaluation test cases for your eval script instead of managing large JSONL or CSV files\nCapture generations to assess quality manually or using LLM-graded evals\nStore user reviewed generations to find new test cases\nWith Phoenix, datasets are:\nIntegrated\n. Datasets are integrated with the platform, so you can add production spans to datasets, use datasets to run experiments, and use metadata to track different segments and use-cases.\nVersioned\n. Every insert, update, and delete is versioned, so you can pin experiments and evaluations to a specific version of a dataset and track changes over time.\nCreating Datasets\nThere are various ways to get started with datasets:\nManually Curated Examples\nThis is how we recommend you start. From building your application, you probably have an idea of what types of inputs you expect your application to be able to handle, and what ""good"" responses look like. You probably want to cover a few different common edge cases or situations you can imagine. Even 20 high quality, manually curated examples can go a long way.\nHistorical Logs\nOnce you ship an application, you start gleaning valuable information: how users are actually using it. This information can be valuable to capture and store in datasets. This allows you to test against specific use cases as you iterate on your application.\nIf your application is going well, you will likely get a lot of usage. How can you determine which datapoints are valuable to add? There are a few heuristics you can follow. If possible, try to collect end user feedback. You can then see which datapoints got negative feedback. That is super valuable! These are spots where your application did not perform well. You should add these to your dataset to test against in the future. You can also use other heuristics to identify interesting datapoints - for example, runs that took a long time to complete could be interesting to analyze and add to a dataset.\nSynthetic Data\nOnce you have a few examples, you can try to artificially generate examples to get a lot of datapoints quickly. It\'s generally advised to have a few good handcrafted examples before this step, as the synthetic data will often resemble the source examples in some way. \nDataset Contents\nWhile Phoenix doesn\'t have dataset types, conceptually you can contain:\nKey-Value Pairs:\nInputs and outputs are arbitrary key-value pairs.\nThis dataset type is ideal for evaluating prompts, functions, and agents that require multiple inputs or generate multiple outputs.\nPrompt Template\nIf you have a RAG prompt template such as:\nCopy\nGiven the context information and not prior knowledge, answer the query.\n\n\n---------------------\n\n\n{context}\n\n\n---------------------\n\n\n\n\nQuery: {query}\n\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/overview-datasets\ntitle: Overview: Datasets\n\nOverview: Datasets\nThe velocity of AI application development is bottlenecked by quality evaluations because AI engineers are often faced with hard tradeoffs: which prompt or LLM best balances performance, latency, and cost. High quality evaluations are critical as they can help developers answer these types of questions with greater confidence.\nDatasets\nDatasets are integral to evaluation. They are collections of examples that provide the \ninputs\n and, optionally, expected \nreference\n outputs for assessing your application.  Datasets allow you to collect data from production, staging, evaluations, and even manually. The examples collected are used to run experiments and evaluations to track improvements to your prompt, LLM, or other parts of your LLM application.\nExperiments\nIn AI development, it\'s hard to understand how a change will affect performance. This breaks the dev flow, making iteration more guesswork than engineering.\nExperiments and evaluations solve this, helping distill the indeterminism of LLMs into tangible feedback that helps you ship more reliable product.\nSpecifically, good evals help you:\nUnderstand whether an update is an improvement or a regression\nDrill down into good / bad examples\nCompare specific examples vs. prior runs\nAvoid guesswork\n\n\nPrevious\nEvaluation Models\nNext\nQuickstart: Datasets\nLast updated \n22 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Tell me about datasets\nAnswer: '}, {'role': 'assistant', 'content': 'Datasets are collections of examples that provide inputs and, optionally, expected reference outputs for assessing applications. Each example within a dataset represents a single data point, consisting of an inputs dictionary, an optional output dictionary, and an optional metadata dictionary. Datasets allow for the collection of data from various sources like production, staging, evaluations, and manual inputs. These examples are then used to run experiments and evaluations to track improvements in applications. Datasets can be created manually, from historical logs, or through synthetic data generation. They are integrated and versioned in platforms like Phoenix, allowing for easy management and tracking of changes over time.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MzQ=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/concepts-datasets\ntitle: Concepts: Datasets\n\nConcepts: Datasets\nThere are many ways to build datasets for experimentation and evaluation.\nDatasets\nDatasets are integral to evaluation and experimentation. They are collections of examples that provide the \ninputs\n and, optionally, expected \nreference\n outputs for assessing your application. Each example within a dataset represents a single data point, consisting of an \ninputs\n dictionary, an optional \noutput\n dictionary, and an optional \nmetadata\n dictionary. The \noptional\n output dictionary often contains the the expected LLM application output for the given input.\nDatasets allow you to collect data from production, staging, evaluations, and even manually. The examples collected are then used to run experiments and evaluations to track improvements.\nUse datasets to:\nStore evaluation test cases for your eval script instead of managing large JSONL or CSV files\nCapture generations to assess quality manually or using LLM-graded evals\nStore user reviewed generations to find new test cases\nWith Phoenix, datasets are:\nIntegrated\n. Datasets are integrated with the platform, so you can add production spans to datasets, use datasets to run experiments, and use metadata to track different segments and use-cases.\nVersioned\n. Every insert, update, and delete is versioned, so you can pin experiments and evaluations to a specific version of a dataset and track changes over time.\nCreating Datasets\nThere are various ways to get started with datasets:\nManually Curated Examples\nThis is how we recommend you start. From building your application, you probably have an idea of what types of inputs you expect your application to be able to handle, and what ""good"" responses look like. You probably want to cover a few different common edge cases or situations you can imagine. Even 20 high quality, manually curated examples can go a long way.\nHistorical Logs\nOnce you ship an application, you start gleaning valuable information: how users are actually using it. This information can be valuable to capture and store in datasets. This allows you to test against specific use cases as you iterate on your application.\nIf your application is going well, you will likely get a lot of usage. How can you determine which datapoints are valuable to add? There are a few heuristics you can follow. If possible, try to collect end user feedback. You can then see which datapoints got negative feedback. That is super valuable! These are spots where your application did not perform well. You should add these to your dataset to test against in the future. You can also use other heuristics to identify interesting datapoints - for example, runs that took a long time to complete could be interesting to analyze and add to a dataset.\nSynthetic Data\nOnce you have a few examples, you can try to artificially generate examples to get a lot of datapoints quickly. It\'s generally advised to have a few good handcrafted examples before this step, as the synthetic data will often resemble the source examples in some way. \nDataset Contents\nWhile Phoenix doesn\'t have dataset types, conceptually you can contain:\nKey-Value Pairs:\nInputs and outputs are arbitrary key-value pairs.\nThis dataset type is ideal for evaluating prompts, functions, and agents that require multiple inputs or generate multiple outputs.\nPrompt Template\nIf you have a RAG prompt template such as:\nCopy\nGiven the context information and not prior knowledge, answer the query.\n\n\n---------------------\n\n\n{context}\n\n\n---------------------\n\n\n\n\nQuery: {query}\n\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/overview-datasets\ntitle: Overview: Datasets\n\nOverview: Datasets\nThe velocity of AI application development is bottlenecked by quality evaluations because AI engineers are often faced with hard tradeoffs: which prompt or LLM best balances performance, latency, and cost. High quality evaluations are critical as they can help developers answer these types of questions with greater confidence.\nDatasets\nDatasets are integral to evaluation. They are collections of examples that provide the \ninputs\n and, optionally, expected \nreference\n outputs for assessing your application.  Datasets allow you to collect data from production, staging, evaluations, and even manually. The examples collected are used to run experiments and evaluations to track improvements to your prompt, LLM, or other parts of your LLM application.\nExperiments\nIn AI development, it\'s hard to understand how a change will affect performance. This breaks the dev flow, making iteration more guesswork than engineering.\nExperiments and evaluations solve this, helping distill the indeterminism of LLMs into tangible feedback that helps you ship more reliable product.\nSpecifically, good evals help you:\nUnderstand whether an update is an improvement or a regression\nDrill down into good / bad examples\nCompare specific examples vs. prior runs\nAvoid guesswork\n\n\nPrevious\nEvaluation Models\nNext\nQuickstart: Datasets\nLast updated \n22 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is a dataset?\nAnswer: '}, {'role': 'assistant', 'content': 'A dataset is a collection of examples that provide inputs and, optionally, expected reference outputs for assessing an application. Each example within a dataset represents a single data point, consisting of an inputs dictionary, an optional output dictionary, and an optional metadata dictionary. Datasets are integral to evaluation and experimentation, allowing data collection from various sources like production, staging, evaluations, and manual inputs. They are used to run experiments and evaluations to track improvements in applications.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/concepts-datasets\ntitle: Concepts: Datasets\n\nConcepts: Datasets\nThere are many ways to build datasets for experimentation and evaluation.\nDatasets\nDatasets are integral to evaluation and experimentation. They are collections of examples that provide the \ninputs\n and, optionally, expected \nreference\n outputs for assessing your application. Each example within a dataset represents a single data point, consisting of an \ninputs\n dictionary, an optional \noutput\n dictionary, and an optional \nmetadata\n dictionary. The \noptional\n output dictionary often contains the the expected LLM application output for the given input.\nDatasets allow you to collect data from production, staging, evaluations, and even manually. The examples collected are then used to run experiments and evaluations to track improvements.\nUse datasets to:\nStore evaluation test cases for your eval script instead of managing large JSONL or CSV files\nCapture generations to assess quality manually or using LLM-graded evals\nStore user reviewed generations to find new test cases\nWith Phoenix, datasets are:\nIntegrated\n. Datasets are integrated with the platform, so you can add production spans to datasets, use datasets to run experiments, and use metadata to track different segments and use-cases.\nVersioned\n. Every insert, update, and delete is versioned, so you can pin experiments and evaluations to a specific version of a dataset and track changes over time.\nCreating Datasets\nThere are various ways to get started with datasets:\nManually Curated Examples\nThis is how we recommend you start. From building your application, you probably have an idea of what types of inputs you expect your application to be able to handle, and what ""good"" responses look like. You probably want to cover a few different common edge cases or situations you can imagine. Even 20 high quality, manually curated examples can go a long way.\nHistorical Logs\nOnce you ship an application, you start gleaning valuable information: how users are actually using it. This information can be valuable to capture and store in datasets. This allows you to test against specific use cases as you iterate on your application.\nIf your application is going well, you will likely get a lot of usage. How can you determine which datapoints are valuable to add? There are a few heuristics you can follow. If possible, try to collect end user feedback. You can then see which datapoints got negative feedback. That is super valuable! These are spots where your application did not perform well. You should add these to your dataset to test against in the future. You can also use other heuristics to identify interesting datapoints - for example, runs that took a long time to complete could be interesting to analyze and add to a dataset.\nSynthetic Data\nOnce you have a few examples, you can try to artificially generate examples to get a lot of datapoints quickly. It\'s generally advised to have a few good handcrafted examples before this step, as the synthetic data will often resemble the source examples in some way. \nDataset Contents\nWhile Phoenix doesn\'t have dataset types, conceptually you can contain:\nKey-Value Pairs:\nInputs and outputs are arbitrary key-value pairs.\nThis dataset type is ideal for evaluating prompts, functions, and agents that require multiple inputs or generate multiple outputs.\nPrompt Template\nIf you have a RAG prompt template such as:\nCopy\nGiven the context information and not prior knowledge, answer the query.\n\n\n---------------------\n\n\n{context}\n\n\n---------------------\n\n\n\n\nQuery: {query}\n\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/overview-datasets\ntitle: Overview: Datasets\n\nOverview: Datasets\nThe velocity of AI application development is bottlenecked by quality evaluations because AI engineers are often faced with hard tradeoffs: which prompt or LLM best balances performance, latency, and cost. High quality evaluations are critical as they can help developers answer these types of questions with greater confidence.\nDatasets\nDatasets are integral to evaluation. They are collections of examples that provide the \ninputs\n and, optionally, expected \nreference\n outputs for assessing your application.  Datasets allow you to collect data from production, staging, evaluations, and even manually. The examples collected are used to run experiments and evaluations to track improvements to your prompt, LLM, or other parts of your LLM application.\nExperiments\nIn AI development, it\'s hard to understand how a change will affect performance. This breaks the dev flow, making iteration more guesswork than engineering.\nExperiments and evaluations solve this, helping distill the indeterminism of LLMs into tangible feedback that helps you ship more reliable product.\nSpecifically, good evals help you:\nUnderstand whether an update is an improvement or a regression\nDrill down into good / bad examples\nCompare specific examples vs. prior runs\nAvoid guesswork\n\n\nPrevious\nEvaluation Models\nNext\nQuickstart: Datasets\nLast updated \n22 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is a dataset?\nAnswer: '}, {'role': 'assistant', 'content': 'A dataset is a collection of examples that provide inputs and, optionally, expected reference outputs for assessing an application. Each example within a dataset represents a single data point, consisting of an inputs dictionary, an optional output dictionary, and an optional metadata dictionary. Datasets are integral to evaluation and experimentation, allowing data collection from various sources like production, staging, evaluations, and manual inputs. They are used to run experiments and evaluations to track improvements in applications.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MzU=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/how-to-experiments/run-experiments\ntitle: Run Experiments\n\nRun Experiments\nThe following are the key steps of running an experiment illustrated by simple example.\nThe key steps of running an experiment are:\nDefine/upload a \nDataset\n (e.g. a dataframe)\nEach record of the dataset is called an \nExample\nDefine a task\nA task is a function that takes each \nExample\n and returns an output\nDefine Evaluators\nAn \nEvaluator\n is a function evaluates the output for each \nExample\nRun the experiment\nWe\'ll start by launching the Phoenix app.\nCopy\nimport\n phoenix \nas\n px\n\n\n\n\npx\n.\nlaunch_app\n()\nLoad a Dataset\nA dataset can be as simple as a list of strings inside a dataframe. More sophisticated datasets can be also extracted from traces based on actual production data. Here we just have a small list of questions that we want to ask an LLM about the NBA games:\nCreate pandas dataframe\nCopy\nimport\n pandas \nas\n pd\n\n\n\n\ndf \n=\n pd\n.\nDataFrame\n(\n\n\n    {\n\n\n        \n""question""\n: [\n\n\n            \n""Which team won the most games?""\n,\n\n\n            \n""Which team won the most games in 2015?""\n,\n\n\n            \n""Who led the league in 3 point shots?""\n,\n\n\n        ]\n\n\n    }\n\n\n)\nThe dataframe can be sent to \nPhoenix\n via the \nClient\n. \ninput_keys\n and \noutput_keys\n are column names of the dataframe, representing the input/output to the task in question. Here we have just questions, so we left the outputs blank:\nUpload dataset to Phoenix\nCopy\nimport\n phoenix \nas\n px\n\n\n\n\ndataset \n=\n px\n.\nClient\n().\nupload_dataset\n(\n\n\n    dataframe\n=\ndf,\n\n\n    input_keys\n=\n[\n""question""\n],\n\n\n    output_keys\n=\n[],\n\n\n    dataset_name\n=\n""nba-questions""\n,\n\n\n)\nEach row of the dataset is called an \nExample\n.\nCreate a Task\nA task is any function/process that returns a JSON serializable output. Task can also be an \nasync\n function, but we used sync function here for simplicity. If the task is a function of one argument, then that argument will be bound to the \ninput\n field of the dataset example.\nCopy\ndef\n \ntask\n(\nx\n):\n\n\n    \nreturn\n ...\nFor our example here, we\'ll ask an LLM to build SQL queries based on our question, which we\'ll run on a database and obtain a set of results:\nSet Up Database\nCopy\nimport\n duckdb\n\n\nfrom\n datasets \nimport\n load_dataset\n\n\n\n\ndata \n=\n \nload_dataset\n(\n""suzyanil/nba-data""\n)\n[\n""train""\n]\n\n\nconn \n=\n duckdb\n.\nconnect\n(database\n=\n"":memory:""\n, read_only\n=\nFalse\n)\n\n\nconn\n.\nregister\n(\n""nba""\n, data.\nto_pandas\n())\nSet Up Prompt and LLM\nCopy\nfrom\n textwrap \nimport\n dedent\n\n\n\n\nimport\n openai\n\n\n\n\nclient \n=\n openai\n.\nClient\n()\n\n\ncolumns \n=\n conn\n.\nquery\n(\n""DESCRIBE nba""\n).\nto_df\n().\nto_dict\n(orient\n=\n""records""\n)\n\n\n\n\nLLM_MODEL \n=\n \n""gpt-4o""\n\n\n\n\ncolumns_str \n=\n \n"",""\n.\njoin\n(column[\n""column_name""\n] \n+\n \n"": ""\n \n+\n column[\n""column_type""\n] \nfor\n column \nin\n columns)\n\n\nsystem_prompt \n=\n \ndedent\n(f\n""""""\n\n\nYou are a SQL expert, and you are given a single table named nba with the following columns:\n\n\n{columns_str}\n\\n\n\n\nWrite a SQL query corresponding to the user\'s\n\n\nrequest. Return just the query text, with no formatting (backticks, markdown, etc.).""""""\n)\n\n\n\n\n\n\ndef\n \ngenerate_query\n(\nquestion\n):\n\n\n    response \n=\n client\n.\nchat\n.\ncompletions\n.\ncreate\n(\n\n\n        model\n=\nLLM_MODEL,\n\n\n        messages\n=\n[\n\n\n            {\n""role""\n: \n""system""\n, \n""content""\n: system_prompt},\n\n\n            {\n""role""\n: \n""user""\n, \n""content""\n: question},\n\n\n        ],\n\n\n    )\n\n\n    \nreturn\n response\n.\nchoices\n[\n0\n].\nmessage\n.\ncontent\n\n\n\n\n\n\ndef\n \nexecute_query\n(\nquery\n):\n\n\n    \nreturn\n conn\n.\nquery\n(query).\nfetchdf\n().\nto_dict\n(orient\n=\n""records""\n)\n\n\n    \n\n\n\n\ndef\n \ntext2sql\n(\nquestion\n):\n\n\n    results \n=\n error \n=\n \nNone\n\n\n    \ntry\n:\n\n\n        results \n=\n \nexecute_query\n(\ngenerate_query\n(question))\n\n\n    \nexcept\n duckdb\n.\nError \nas\n e\n:\n\n\n        error \n=\n \nstr\n(e)\n\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/overview-datasets\ntitle: Overview: Datasets\n\nOverview: Datasets\nThe velocity of AI application development is bottlenecked by quality evaluations because AI engineers are often faced with hard tradeoffs: which prompt or LLM best balances performance, latency, and cost. High quality evaluations are critical as they can help developers answer these types of questions with greater confidence.\nDatasets\nDatasets are integral to evaluation. They are collections of examples that provide the \ninputs\n and, optionally, expected \nreference\n outputs for assessing your application.  Datasets allow you to collect data from production, staging, evaluations, and even manually. The examples collected are used to run experiments and evaluations to track improvements to your prompt, LLM, or other parts of your LLM application.\nExperiments\nIn AI development, it\'s hard to understand how a change will affect performance. This breaks the dev flow, making iteration more guesswork than engineering.\nExperiments and evaluations solve this, helping distill the indeterminism of LLMs into tangible feedback that helps you ship more reliable product.\nSpecifically, good evals help you:\nUnderstand whether an update is an improvement or a regression\nDrill down into good / bad examples\nCompare specific examples vs. prior runs\nAvoid guesswork\n\n\nPrevious\nEvaluation Models\nNext\nQuickstart: Datasets\nLast updated \n22 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is an experiment?\nAnswer: '}, {'role': 'assistant', 'content': 'An experiment involves defining/uploading a dataset, creating a task, defining evaluators, and running the experiment. It helps in understanding how a change will affect performance in AI development, providing tangible feedback to help improve the reliability of the product.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/how-to-experiments/run-experiments\ntitle: Run Experiments\n\nRun Experiments\nThe following are the key steps of running an experiment illustrated by simple example.\nThe key steps of running an experiment are:\nDefine/upload a \nDataset\n (e.g. a dataframe)\nEach record of the dataset is called an \nExample\nDefine a task\nA task is a function that takes each \nExample\n and returns an output\nDefine Evaluators\nAn \nEvaluator\n is a function evaluates the output for each \nExample\nRun the experiment\nWe\'ll start by launching the Phoenix app.\nCopy\nimport\n phoenix \nas\n px\n\n\n\n\npx\n.\nlaunch_app\n()\nLoad a Dataset\nA dataset can be as simple as a list of strings inside a dataframe. More sophisticated datasets can be also extracted from traces based on actual production data. Here we just have a small list of questions that we want to ask an LLM about the NBA games:\nCreate pandas dataframe\nCopy\nimport\n pandas \nas\n pd\n\n\n\n\ndf \n=\n pd\n.\nDataFrame\n(\n\n\n    {\n\n\n        \n""question""\n: [\n\n\n            \n""Which team won the most games?""\n,\n\n\n            \n""Which team won the most games in 2015?""\n,\n\n\n            \n""Who led the league in 3 point shots?""\n,\n\n\n        ]\n\n\n    }\n\n\n)\nThe dataframe can be sent to \nPhoenix\n via the \nClient\n. \ninput_keys\n and \noutput_keys\n are column names of the dataframe, representing the input/output to the task in question. Here we have just questions, so we left the outputs blank:\nUpload dataset to Phoenix\nCopy\nimport\n phoenix \nas\n px\n\n\n\n\ndataset \n=\n px\n.\nClient\n().\nupload_dataset\n(\n\n\n    dataframe\n=\ndf,\n\n\n    input_keys\n=\n[\n""question""\n],\n\n\n    output_keys\n=\n[],\n\n\n    dataset_name\n=\n""nba-questions""\n,\n\n\n)\nEach row of the dataset is called an \nExample\n.\nCreate a Task\nA task is any function/process that returns a JSON serializable output. Task can also be an \nasync\n function, but we used sync function here for simplicity. If the task is a function of one argument, then that argument will be bound to the \ninput\n field of the dataset example.\nCopy\ndef\n \ntask\n(\nx\n):\n\n\n    \nreturn\n ...\nFor our example here, we\'ll ask an LLM to build SQL queries based on our question, which we\'ll run on a database and obtain a set of results:\nSet Up Database\nCopy\nimport\n duckdb\n\n\nfrom\n datasets \nimport\n load_dataset\n\n\n\n\ndata \n=\n \nload_dataset\n(\n""suzyanil/nba-data""\n)\n[\n""train""\n]\n\n\nconn \n=\n duckdb\n.\nconnect\n(database\n=\n"":memory:""\n, read_only\n=\nFalse\n)\n\n\nconn\n.\nregister\n(\n""nba""\n, data.\nto_pandas\n())\nSet Up Prompt and LLM\nCopy\nfrom\n textwrap \nimport\n dedent\n\n\n\n\nimport\n openai\n\n\n\n\nclient \n=\n openai\n.\nClient\n()\n\n\ncolumns \n=\n conn\n.\nquery\n(\n""DESCRIBE nba""\n).\nto_df\n().\nto_dict\n(orient\n=\n""records""\n)\n\n\n\n\nLLM_MODEL \n=\n \n""gpt-4o""\n\n\n\n\ncolumns_str \n=\n \n"",""\n.\njoin\n(column[\n""column_name""\n] \n+\n \n"": ""\n \n+\n column[\n""column_type""\n] \nfor\n column \nin\n columns)\n\n\nsystem_prompt \n=\n \ndedent\n(f\n""""""\n\n\nYou are a SQL expert, and you are given a single table named nba with the following columns:\n\n\n{columns_str}\n\\n\n\n\nWrite a SQL query corresponding to the user\'s\n\n\nrequest. Return just the query text, with no formatting (backticks, markdown, etc.).""""""\n)\n\n\n\n\n\n\ndef\n \ngenerate_query\n(\nquestion\n):\n\n\n    response \n=\n client\n.\nchat\n.\ncompletions\n.\ncreate\n(\n\n\n        model\n=\nLLM_MODEL,\n\n\n        messages\n=\n[\n\n\n            {\n""role""\n: \n""system""\n, \n""content""\n: system_prompt},\n\n\n            {\n""role""\n: \n""user""\n, \n""content""\n: question},\n\n\n        ],\n\n\n    )\n\n\n    \nreturn\n response\n.\nchoices\n[\n0\n].\nmessage\n.\ncontent\n\n\n\n\n\n\ndef\n \nexecute_query\n(\nquery\n):\n\n\n    \nreturn\n conn\n.\nquery\n(query).\nfetchdf\n().\nto_dict\n(orient\n=\n""records""\n)\n\n\n    \n\n\n\n\ndef\n \ntext2sql\n(\nquestion\n):\n\n\n    results \n=\n error \n=\n \nNone\n\n\n    \ntry\n:\n\n\n        results \n=\n \nexecute_query\n(\ngenerate_query\n(question))\n\n\n    \nexcept\n duckdb\n.\nError \nas\n e\n:\n\n\n        error \n=\n \nstr\n(e)\n\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/overview-datasets\ntitle: Overview: Datasets\n\nOverview: Datasets\nThe velocity of AI application development is bottlenecked by quality evaluations because AI engineers are often faced with hard tradeoffs: which prompt or LLM best balances performance, latency, and cost. High quality evaluations are critical as they can help developers answer these types of questions with greater confidence.\nDatasets\nDatasets are integral to evaluation. They are collections of examples that provide the \ninputs\n and, optionally, expected \nreference\n outputs for assessing your application.  Datasets allow you to collect data from production, staging, evaluations, and even manually. The examples collected are used to run experiments and evaluations to track improvements to your prompt, LLM, or other parts of your LLM application.\nExperiments\nIn AI development, it\'s hard to understand how a change will affect performance. This breaks the dev flow, making iteration more guesswork than engineering.\nExperiments and evaluations solve this, helping distill the indeterminism of LLMs into tangible feedback that helps you ship more reliable product.\nSpecifically, good evals help you:\nUnderstand whether an update is an improvement or a regression\nDrill down into good / bad examples\nCompare specific examples vs. prior runs\nAvoid guesswork\n\n\nPrevious\nEvaluation Models\nNext\nQuickstart: Datasets\nLast updated \n22 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is an experiment?\nAnswer: '}, {'role': 'assistant', 'content': 'An experiment involves defining/uploading a dataset, creating a task, defining evaluators, and running the experiment. It helps in understanding how a change will affect performance in AI development, providing tangible feedback to help improve the reliability of the product.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MzY=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/use-cases-datasets\ntitle: Use Cases: Experiments\n\nUse Cases: Experiments\nDatasets and experiments can be used to improve and iterate on various LLM application parts:\nOptimizing Txt2Sql\nOptimizing Document Summarization\nOptimizing Email Extraction\nPrevious\nUsing Evaluators\nNext\nText2SQL\nLast updated \n1 month ago\n\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/overview-datasets\ntitle: Overview: Datasets\n\nOverview: Datasets\nThe velocity of AI application development is bottlenecked by quality evaluations because AI engineers are often faced with hard tradeoffs: which prompt or LLM best balances performance, latency, and cost. High quality evaluations are critical as they can help developers answer these types of questions with greater confidence.\nDatasets\nDatasets are integral to evaluation. They are collections of examples that provide the \ninputs\n and, optionally, expected \nreference\n outputs for assessing your application.  Datasets allow you to collect data from production, staging, evaluations, and even manually. The examples collected are used to run experiments and evaluations to track improvements to your prompt, LLM, or other parts of your LLM application.\nExperiments\nIn AI development, it's hard to understand how a change will affect performance. This breaks the dev flow, making iteration more guesswork than engineering.\nExperiments and evaluations solve this, helping distill the indeterminism of LLMs into tangible feedback that helps you ship more reliable product.\nSpecifically, good evals help you:\nUnderstand whether an update is an improvement or a regression\nDrill down into good / bad examples\nCompare specific examples vs. prior runs\nAvoid guesswork\n\n\nPrevious\nEvaluation Models\nNext\nQuickstart: Datasets\nLast updated \n22 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: When do I use experiments?\nAnswer: ""}, {'role': 'assistant', 'content': 'You use experiments in AI development to understand how a change will affect performance, helping to distill the indeterminism of LLMs into tangible feedback that assists in shipping more reliable products.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/use-cases-datasets\ntitle: Use Cases: Experiments\n\nUse Cases: Experiments\nDatasets and experiments can be used to improve and iterate on various LLM application parts:\nOptimizing Txt2Sql\nOptimizing Document Summarization\nOptimizing Email Extraction\nPrevious\nUsing Evaluators\nNext\nText2SQL\nLast updated \n1 month ago\n\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/overview-datasets\ntitle: Overview: Datasets\n\nOverview: Datasets\nThe velocity of AI application development is bottlenecked by quality evaluations because AI engineers are often faced with hard tradeoffs: which prompt or LLM best balances performance, latency, and cost. High quality evaluations are critical as they can help developers answer these types of questions with greater confidence.\nDatasets\nDatasets are integral to evaluation. They are collections of examples that provide the \ninputs\n and, optionally, expected \nreference\n outputs for assessing your application.  Datasets allow you to collect data from production, staging, evaluations, and even manually. The examples collected are used to run experiments and evaluations to track improvements to your prompt, LLM, or other parts of your LLM application.\nExperiments\nIn AI development, it's hard to understand how a change will affect performance. This breaks the dev flow, making iteration more guesswork than engineering.\nExperiments and evaluations solve this, helping distill the indeterminism of LLMs into tangible feedback that helps you ship more reliable product.\nSpecifically, good evals help you:\nUnderstand whether an update is an improvement or a regression\nDrill down into good / bad examples\nCompare specific examples vs. prior runs\nAvoid guesswork\n\n\nPrevious\nEvaluation Models\nNext\nQuickstart: Datasets\nLast updated \n22 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: When do I use experiments?\nAnswer: ""}, {'role': 'assistant', 'content': 'You use experiments in AI development to understand how a change will affect performance, helping to distill the indeterminism of LLMs into tangible feedback that assists in shipping more reliable products.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6Mzc=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/concepts-datasets\ntitle: Concepts: Datasets\n\nConcepts: Datasets\nThere are many ways to build datasets for experimentation and evaluation.\nDatasets\nDatasets are integral to evaluation and experimentation. They are collections of examples that provide the \ninputs\n and, optionally, expected \nreference\n outputs for assessing your application. Each example within a dataset represents a single data point, consisting of an \ninputs\n dictionary, an optional \noutput\n dictionary, and an optional \nmetadata\n dictionary. The \noptional\n output dictionary often contains the the expected LLM application output for the given input.\nDatasets allow you to collect data from production, staging, evaluations, and even manually. The examples collected are then used to run experiments and evaluations to track improvements.\nUse datasets to:\nStore evaluation test cases for your eval script instead of managing large JSONL or CSV files\nCapture generations to assess quality manually or using LLM-graded evals\nStore user reviewed generations to find new test cases\nWith Phoenix, datasets are:\nIntegrated\n. Datasets are integrated with the platform, so you can add production spans to datasets, use datasets to run experiments, and use metadata to track different segments and use-cases.\nVersioned\n. Every insert, update, and delete is versioned, so you can pin experiments and evaluations to a specific version of a dataset and track changes over time.\nCreating Datasets\nThere are various ways to get started with datasets:\nManually Curated Examples\nThis is how we recommend you start. From building your application, you probably have an idea of what types of inputs you expect your application to be able to handle, and what ""good"" responses look like. You probably want to cover a few different common edge cases or situations you can imagine. Even 20 high quality, manually curated examples can go a long way.\nHistorical Logs\nOnce you ship an application, you start gleaning valuable information: how users are actually using it. This information can be valuable to capture and store in datasets. This allows you to test against specific use cases as you iterate on your application.\nIf your application is going well, you will likely get a lot of usage. How can you determine which datapoints are valuable to add? There are a few heuristics you can follow. If possible, try to collect end user feedback. You can then see which datapoints got negative feedback. That is super valuable! These are spots where your application did not perform well. You should add these to your dataset to test against in the future. You can also use other heuristics to identify interesting datapoints - for example, runs that took a long time to complete could be interesting to analyze and add to a dataset.\nSynthetic Data\nOnce you have a few examples, you can try to artificially generate examples to get a lot of datapoints quickly. It\'s generally advised to have a few good handcrafted examples before this step, as the synthetic data will often resemble the source examples in some way. \nDataset Contents\nWhile Phoenix doesn\'t have dataset types, conceptually you can contain:\nKey-Value Pairs:\nInputs and outputs are arbitrary key-value pairs.\nThis dataset type is ideal for evaluating prompts, functions, and agents that require multiple inputs or generate multiple outputs.\nPrompt Template\nIf you have a RAG prompt template such as:\nCopy\nGiven the context information and not prior knowledge, answer the query.\n\n\n---------------------\n\n\n{context}\n\n\n---------------------\n\n\n\n\nQuery: {query}\n\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/how-to-datasets/creating-datasets\ntitle: Creating Datasets\n\n)\nOnce your synthetic data has been created, this data can be uploaded to Phoenix for later re-use.\nCopy\nimport\n json\n\n\n\n\nfrom\n phoenix\n.\nevals \nimport\n OpenAIModel\n,\n llm_generate\n\n\n\n\n\n\ndef\n \noutput_parser\n(\nresponse\n:\n \nstr\n,\n \nindex\n:\n \nint\n):\n\n\n    \ntry\n:\n\n\n        \nreturn\n json\n.\nloads\n(response)\n\n\n    \nexcept\n json\n.\nJSONDecodeError \nas\n e\n:\n\n\n        \nreturn\n \n{\n""__error__""\n:\n \nstr\n(e)}\n\n\n\n\n\n\nquestions_df \n=\n \nllm_generate\n(\n\n\n    dataframe\n=\ndocument_chunks_df,\n\n\n    template\n=\ngenerate_questions_template,\n\n\n    model\n=\nOpenAIModel\n(model\n=\n""gpt-3.5-turbo""\n),\n\n\n    output_parser\n=\noutput_parser,\n\n\n    concurrency\n=\n20\n,\n\n\n)\n\n\nquestions_df\n[\n""output""\n]\n \n=\n [\nNone\n,\n \nNone\n,\n \nNone\n]\nOnce we\'ve constructed a collection of synthetic questions, we can upload them to a Phoenix dataset.\nCopy\nimport\n phoenix \nas\n px\n\n\n\n\n# Note that the below code assumes that phoenix is running and accessible\n\n\nclient \n=\n px\n.\nClient\n()\n\n\nclient\n.\nupload_dataset\n(\n\n\n    dataframe\n=\nquestions_df, dataset_name\n=\n""paul-graham-questions""\n,\n\n\n    input_keys\n=\n[\n""question""\n],\n\n\n    output_keys\n=\n[\n""output""\n],\n\n\n)\nFrom Spans\nIf you have an application that is traced using instrumentation, you can quickly add any span or group of spans using the Phoenix UI.\nTo add a single span to a dataset, simply select the span in the trace details view. You should see an add to dataset button on the top right. From there you can select the dataset you would like to add it to and make any changes you might need to make before saving the example.\nAdd a specific span as a golden dataset or an example for further testing\n\nYou can also use the filters on the spans table and select multiple spans to add to a specific dataset.\nAdd LLM spans for fine tuning to a dataset\nPrevious\nHow-to: Datasets\nNext\nExporting Datasets\nLast updated \n1 month ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I add data to a dataset?\nAnswer: '}, {'role': 'assistant', 'content': 'You can add data to a dataset in several ways. One method is by manually curating examples, where you can start by building high-quality examples that cover different scenarios. Another approach is by using historical logs to capture valuable information on how users interact with your application. Additionally, you can generate synthetic data to quickly create a large number of data points. Lastly, you can add data to a dataset by uploading synthetic questions or spans to Phoenix using the provided code snippets and functions.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/concepts-datasets\ntitle: Concepts: Datasets\n\nConcepts: Datasets\nThere are many ways to build datasets for experimentation and evaluation.\nDatasets\nDatasets are integral to evaluation and experimentation. They are collections of examples that provide the \ninputs\n and, optionally, expected \nreference\n outputs for assessing your application. Each example within a dataset represents a single data point, consisting of an \ninputs\n dictionary, an optional \noutput\n dictionary, and an optional \nmetadata\n dictionary. The \noptional\n output dictionary often contains the the expected LLM application output for the given input.\nDatasets allow you to collect data from production, staging, evaluations, and even manually. The examples collected are then used to run experiments and evaluations to track improvements.\nUse datasets to:\nStore evaluation test cases for your eval script instead of managing large JSONL or CSV files\nCapture generations to assess quality manually or using LLM-graded evals\nStore user reviewed generations to find new test cases\nWith Phoenix, datasets are:\nIntegrated\n. Datasets are integrated with the platform, so you can add production spans to datasets, use datasets to run experiments, and use metadata to track different segments and use-cases.\nVersioned\n. Every insert, update, and delete is versioned, so you can pin experiments and evaluations to a specific version of a dataset and track changes over time.\nCreating Datasets\nThere are various ways to get started with datasets:\nManually Curated Examples\nThis is how we recommend you start. From building your application, you probably have an idea of what types of inputs you expect your application to be able to handle, and what ""good"" responses look like. You probably want to cover a few different common edge cases or situations you can imagine. Even 20 high quality, manually curated examples can go a long way.\nHistorical Logs\nOnce you ship an application, you start gleaning valuable information: how users are actually using it. This information can be valuable to capture and store in datasets. This allows you to test against specific use cases as you iterate on your application.\nIf your application is going well, you will likely get a lot of usage. How can you determine which datapoints are valuable to add? There are a few heuristics you can follow. If possible, try to collect end user feedback. You can then see which datapoints got negative feedback. That is super valuable! These are spots where your application did not perform well. You should add these to your dataset to test against in the future. You can also use other heuristics to identify interesting datapoints - for example, runs that took a long time to complete could be interesting to analyze and add to a dataset.\nSynthetic Data\nOnce you have a few examples, you can try to artificially generate examples to get a lot of datapoints quickly. It\'s generally advised to have a few good handcrafted examples before this step, as the synthetic data will often resemble the source examples in some way. \nDataset Contents\nWhile Phoenix doesn\'t have dataset types, conceptually you can contain:\nKey-Value Pairs:\nInputs and outputs are arbitrary key-value pairs.\nThis dataset type is ideal for evaluating prompts, functions, and agents that require multiple inputs or generate multiple outputs.\nPrompt Template\nIf you have a RAG prompt template such as:\nCopy\nGiven the context information and not prior knowledge, answer the query.\n\n\n---------------------\n\n\n{context}\n\n\n---------------------\n\n\n\n\nQuery: {query}\n\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/how-to-datasets/creating-datasets\ntitle: Creating Datasets\n\n)\nOnce your synthetic data has been created, this data can be uploaded to Phoenix for later re-use.\nCopy\nimport\n json\n\n\n\n\nfrom\n phoenix\n.\nevals \nimport\n OpenAIModel\n,\n llm_generate\n\n\n\n\n\n\ndef\n \noutput_parser\n(\nresponse\n:\n \nstr\n,\n \nindex\n:\n \nint\n):\n\n\n    \ntry\n:\n\n\n        \nreturn\n json\n.\nloads\n(response)\n\n\n    \nexcept\n json\n.\nJSONDecodeError \nas\n e\n:\n\n\n        \nreturn\n \n{\n""__error__""\n:\n \nstr\n(e)}\n\n\n\n\n\n\nquestions_df \n=\n \nllm_generate\n(\n\n\n    dataframe\n=\ndocument_chunks_df,\n\n\n    template\n=\ngenerate_questions_template,\n\n\n    model\n=\nOpenAIModel\n(model\n=\n""gpt-3.5-turbo""\n),\n\n\n    output_parser\n=\noutput_parser,\n\n\n    concurrency\n=\n20\n,\n\n\n)\n\n\nquestions_df\n[\n""output""\n]\n \n=\n [\nNone\n,\n \nNone\n,\n \nNone\n]\nOnce we\'ve constructed a collection of synthetic questions, we can upload them to a Phoenix dataset.\nCopy\nimport\n phoenix \nas\n px\n\n\n\n\n# Note that the below code assumes that phoenix is running and accessible\n\n\nclient \n=\n px\n.\nClient\n()\n\n\nclient\n.\nupload_dataset\n(\n\n\n    dataframe\n=\nquestions_df, dataset_name\n=\n""paul-graham-questions""\n,\n\n\n    input_keys\n=\n[\n""question""\n],\n\n\n    output_keys\n=\n[\n""output""\n],\n\n\n)\nFrom Spans\nIf you have an application that is traced using instrumentation, you can quickly add any span or group of spans using the Phoenix UI.\nTo add a single span to a dataset, simply select the span in the trace details view. You should see an add to dataset button on the top right. From there you can select the dataset you would like to add it to and make any changes you might need to make before saving the example.\nAdd a specific span as a golden dataset or an example for further testing\n\nYou can also use the filters on the spans table and select multiple spans to add to a specific dataset.\nAdd LLM spans for fine tuning to a dataset\nPrevious\nHow-to: Datasets\nNext\nExporting Datasets\nLast updated \n1 month ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I add data to a dataset?\nAnswer: '}, {'role': 'assistant', 'content': 'You can add data to a dataset in several ways. One method is by manually curating examples, where you can start by building high-quality examples that cover different scenarios. Another approach is by using historical logs to capture valuable information on how users interact with your application. Additionally, you can generate synthetic data to quickly create a large number of data points. Lastly, you can add data to a dataset by uploading synthetic questions or spans to Phoenix using the provided code snippets and functions.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6Mzg=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/use-cases-tracing/rag-evaluation\ntitle: Evaluate RAG\n\nEvaluate RAG\nBuilding a RAG pipeline and evaluating it with Phoenix Evals.\nIn this tutorial we will look into building a RAG pipeline and evaluating it with Phoenix Evals.\nIt has the the following sections:\nUnderstanding Retrieval Augmented Generation (RAG).\nBuilding RAG (with the help of a framework such as LlamaIndex).\nEvaluating RAG with Phoenix Evals.\nRetrieval Augmented Generation (RAG)\nLLMs are trained on vast amounts of data, but these will not include your specific data (things like company knowledge bases and documentation). Retrieval-Augmented Generation (RAG) addresses this by dynamically incorporating your data as context during the generation process. This is done not by altering the training data of the LLMs but by allowing the model to access and utilize your data in real-time to provide more tailored and contextually relevant responses.\nIn RAG, your data is loaded and prepared for queries. This process is called indexing. User queries act on this index, which filters your data down to the most relevant context. This context and your query then are sent to the LLM along with a prompt, and the LLM provides a response.\nRAG is a critical component for building applications such a chatbots or agents and you will want to know RAG techniques on how to get data into your application.\nStages within RAG\nThere are five key stages within RAG, which will in turn be a part of any larger RAG application.\nLoading\n: This refers to getting your data from where it lives - whether it\'s text files, PDFs, another website, a database or an API - into your pipeline.\nIndexing\n: This means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\nStoring\n: Once your data is indexed, you will want to store your index, along with any other metadata, to avoid the need to re-index it.\nQuerying\n: For any given indexing strategy there are many ways you can utilize LLMs and data structures to query, including sub-queries, multi-step queries, and hybrid strategies.\nEvaluation\n: A critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures on how accurate, faithful, and fast your responses to queries are.\nBuild a RAG system\nNow that we have understood the stages of RAG, let\'s build a pipeline. We will use \nLlamaIndex\n for RAG and \nPhoenix Evals\n for evaluation.\nCopy\n!pip install \n-\nqq \n""arize-phoenix[experimental,llama-index]>=2.0""\nCopy\n# The nest_asyncio module enables the nesting of asynchronous functions within an already running async loop.\n\n\n# This is necessary because Jupyter notebooks inherently operate in an asynchronous loop.\n\n\n# By applying nest_asyncio, we can run additional async functions within this existing loop without conflicts.\n\n\nimport\n nest_asyncio\n\n\n\n\nnest_asyncio\n.\napply\n()\n\n\n\n\nimport\n os\n\n\nfrom\n getpass \nimport\n getpass\n\n\n\n\nimport\n pandas \nas\n pd\n\n\nimport\n phoenix \nas\n px\n\n\nfrom\n llama_index \nimport\n SimpleDirectoryReader\n,\n VectorStoreIndex\n,\n set_global_handler\n\n\nfrom\n llama_index\n.\nllms \nimport\n OpenAI\n\n\nfrom\n llama_index\n.\nnode_parser \nimport\n SimpleNodeParser\nDuring this tutorial, we will capture all the data we need to evaluate our RAG pipeline using Phoenix Tracing. To enable this, simply start the phoenix application and instrument LlamaIndex.\nCopy\npx\n.\nlaunch_app\n()\nCopy\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace \nimport\n TracerProvider\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\n\n\nendpoint \n=\n \n""http://127.0.0.1:6006/v1/traces""\n\n\ntracer_provider \n=\n \nTracerProvider\n()\n\n\ntracer_provider\n.\nadd_span_processor\n(\nSimpleSpanProcessor\n(\nOTLPSpanExporter\n(endpoint)))\n\n\n\n\nLlamaIndexInstrumentor\n().\ninstrument\n(tracer_provider\n=\ntracer_provider)\nFor this tutorial we will be using OpenAI for creating synthetic data as well as for evaluation.\nCopy\nif\n \nnot\n (openai_api_key \n:=\n os\n.\ngetenv\n(\n""OPENAI_API_KEY""\n)\n)\n:\n\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals/retrieval-rag-relevance\ntitle: Retrieval (RAG) Relevance\n\nRetrieval (RAG) Relevance\nWhen To Use RAG Eval Template\nThis Eval evaluates whether a retrieved chunk contains an answer to the query. It\'s extremely useful for evaluating retrieval systems.\nRAG Eval Template\nGoogle Colaboratory\nTry it out!\nCopy\nYou are comparing a reference text to a question \nand\n trying to determine \nif\n the reference text\n\n\ncontains information relevant to answering the question\n.\n Here \nis\n the data\n:\n\n\n    [BEGIN DATA]\n\n\n    \n************\n\n\n    [Question]\n:\n \n{\nquery\n}\n\n\n    \n************\n\n\n    [Reference text]\n:\n \n{\nreference\n}\n\n\n    [END DATA]\n\n\n\n\nCompare the Question above to the Reference text\n.\n You must determine whether the Reference text\n\n\ncontains information that can answer the Question\n.\n Please focus on whether the very specific\n\n\nquestion can be answered by the information \nin\n the Reference text\n.\n\n\nYour response must be single word\n,\n either \n""relevant""\n \nor\n \n""unrelated""\n,\n\n\nand\n should \nnot\n contain \nany\n text \nor\n characters aside \nfrom\n that word\n.\n\n\n""unrelated""\n means that the reference text does \nnot\n contain an answer to the Question\n.\n\n\n""relevant""\n means the reference text contains an answer to the Question\n.\nWe are continually iterating our templates, view the most up-to-date template on GitHub. Last updated on 10/12/2023\nBenchmark Results\nGPT-4 Result\nGPT-3.5 Results\nClaude V2 Results\nGPT 4 Turbo\nHow To Run the Eval\nCopy\nfrom\n phoenix\n.\nevals \nimport\n (\n\n\n    RAG_RELEVANCY_PROMPT_RAILS_MAP\n,\n\n\n    RAG_RELEVANCY_PROMPT_TEMPLATE\n,\n\n\n    OpenAIModel\n,\n\n\n    download_benchmark_dataset\n,\n\n\n    llm_classify\n,\n\n\n)\n\n\n\n\nmodel \n=\n \nOpenAIModel\n(\n\n\n    model_name\n=\n""gpt-4""\n,\n\n\n    temperature\n=\n0.0\n,\n\n\n)\n\n\n\n\n#The rails is used to hold the output to specific values based on the template\n\n\n#It will remove text such as "",,,"" or ""...""\n\n\n#Will ensure the binary value expected from the template is returned\n\n\nrails \n=\n \nlist\n(RAG_RELEVANCY_PROMPT_RAILS_MAP.\nvalues\n())\n\n\nrelevance_classifications \n=\n \nllm_classify\n(\n\n\n    dataframe\n=\ndf,\n\n\n    template\n=\nRAG_RELEVANCY_PROMPT_TEMPLATE,\n\n\n    model\n=\nmodel,\n\n\n    rails\n=\nrails,\n\n\n    provide_explanation\n=\nTrue\n, \n#optional to generate explanations for the value produced by the eval LLM\n\n\n)\nThe above runs the RAG relevancy LLM template against the dataframe df.\nRAG Eval\nGPT-4o\nGPT-4\nGPT-4 Turbo\nGemini Pro\nGPT-3.5\nPalm (Text Bison)\nClaude V2\nPrecision\n0.60\n0.70\n0.68\n0.61\n0.42\n0.53\n0.79\nRecall\n0.77\n0.88\n0.91\n1\n1.0\n1\n0.22\nF1\n0.67\n0.78\n0.78\n0.76\n0.59\n0.69\n0.34\nThroughput\nGPT-4\nGPT-4 Turbo\nGPT-3.5\n100 Samples\n113 Sec\n61 sec\n73 Sec\nPrevious\nQ&A on Retrieved Data\nNext\nSummarization\nLast updated \n2 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is RAG?\nAnswer: '}, {'role': 'assistant', 'content': 'RAG stands for Retrieval Augmented Generation. It is a technique that dynamically incorporates specific data as context during the generation process without altering the training data of Language Model Models (LLMs). This allows the model to access and utilize real-time data to provide more tailored and contextually relevant responses. RAG involves loading data, indexing it for querying, storing the index, querying the data structure, and evaluating the effectiveness of the pipeline in providing accurate and relevant responses.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/use-cases-tracing/rag-evaluation\ntitle: Evaluate RAG\n\nEvaluate RAG\nBuilding a RAG pipeline and evaluating it with Phoenix Evals.\nIn this tutorial we will look into building a RAG pipeline and evaluating it with Phoenix Evals.\nIt has the the following sections:\nUnderstanding Retrieval Augmented Generation (RAG).\nBuilding RAG (with the help of a framework such as LlamaIndex).\nEvaluating RAG with Phoenix Evals.\nRetrieval Augmented Generation (RAG)\nLLMs are trained on vast amounts of data, but these will not include your specific data (things like company knowledge bases and documentation). Retrieval-Augmented Generation (RAG) addresses this by dynamically incorporating your data as context during the generation process. This is done not by altering the training data of the LLMs but by allowing the model to access and utilize your data in real-time to provide more tailored and contextually relevant responses.\nIn RAG, your data is loaded and prepared for queries. This process is called indexing. User queries act on this index, which filters your data down to the most relevant context. This context and your query then are sent to the LLM along with a prompt, and the LLM provides a response.\nRAG is a critical component for building applications such a chatbots or agents and you will want to know RAG techniques on how to get data into your application.\nStages within RAG\nThere are five key stages within RAG, which will in turn be a part of any larger RAG application.\nLoading\n: This refers to getting your data from where it lives - whether it\'s text files, PDFs, another website, a database or an API - into your pipeline.\nIndexing\n: This means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\nStoring\n: Once your data is indexed, you will want to store your index, along with any other metadata, to avoid the need to re-index it.\nQuerying\n: For any given indexing strategy there are many ways you can utilize LLMs and data structures to query, including sub-queries, multi-step queries, and hybrid strategies.\nEvaluation\n: A critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures on how accurate, faithful, and fast your responses to queries are.\nBuild a RAG system\nNow that we have understood the stages of RAG, let\'s build a pipeline. We will use \nLlamaIndex\n for RAG and \nPhoenix Evals\n for evaluation.\nCopy\n!pip install \n-\nqq \n""arize-phoenix[experimental,llama-index]>=2.0""\nCopy\n# The nest_asyncio module enables the nesting of asynchronous functions within an already running async loop.\n\n\n# This is necessary because Jupyter notebooks inherently operate in an asynchronous loop.\n\n\n# By applying nest_asyncio, we can run additional async functions within this existing loop without conflicts.\n\n\nimport\n nest_asyncio\n\n\n\n\nnest_asyncio\n.\napply\n()\n\n\n\n\nimport\n os\n\n\nfrom\n getpass \nimport\n getpass\n\n\n\n\nimport\n pandas \nas\n pd\n\n\nimport\n phoenix \nas\n px\n\n\nfrom\n llama_index \nimport\n SimpleDirectoryReader\n,\n VectorStoreIndex\n,\n set_global_handler\n\n\nfrom\n llama_index\n.\nllms \nimport\n OpenAI\n\n\nfrom\n llama_index\n.\nnode_parser \nimport\n SimpleNodeParser\nDuring this tutorial, we will capture all the data we need to evaluate our RAG pipeline using Phoenix Tracing. To enable this, simply start the phoenix application and instrument LlamaIndex.\nCopy\npx\n.\nlaunch_app\n()\nCopy\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace \nimport\n TracerProvider\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\n\n\nendpoint \n=\n \n""http://127.0.0.1:6006/v1/traces""\n\n\ntracer_provider \n=\n \nTracerProvider\n()\n\n\ntracer_provider\n.\nadd_span_processor\n(\nSimpleSpanProcessor\n(\nOTLPSpanExporter\n(endpoint)))\n\n\n\n\nLlamaIndexInstrumentor\n().\ninstrument\n(tracer_provider\n=\ntracer_provider)\nFor this tutorial we will be using OpenAI for creating synthetic data as well as for evaluation.\nCopy\nif\n \nnot\n (openai_api_key \n:=\n os\n.\ngetenv\n(\n""OPENAI_API_KEY""\n)\n)\n:\n\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals/retrieval-rag-relevance\ntitle: Retrieval (RAG) Relevance\n\nRetrieval (RAG) Relevance\nWhen To Use RAG Eval Template\nThis Eval evaluates whether a retrieved chunk contains an answer to the query. It\'s extremely useful for evaluating retrieval systems.\nRAG Eval Template\nGoogle Colaboratory\nTry it out!\nCopy\nYou are comparing a reference text to a question \nand\n trying to determine \nif\n the reference text\n\n\ncontains information relevant to answering the question\n.\n Here \nis\n the data\n:\n\n\n    [BEGIN DATA]\n\n\n    \n************\n\n\n    [Question]\n:\n \n{\nquery\n}\n\n\n    \n************\n\n\n    [Reference text]\n:\n \n{\nreference\n}\n\n\n    [END DATA]\n\n\n\n\nCompare the Question above to the Reference text\n.\n You must determine whether the Reference text\n\n\ncontains information that can answer the Question\n.\n Please focus on whether the very specific\n\n\nquestion can be answered by the information \nin\n the Reference text\n.\n\n\nYour response must be single word\n,\n either \n""relevant""\n \nor\n \n""unrelated""\n,\n\n\nand\n should \nnot\n contain \nany\n text \nor\n characters aside \nfrom\n that word\n.\n\n\n""unrelated""\n means that the reference text does \nnot\n contain an answer to the Question\n.\n\n\n""relevant""\n means the reference text contains an answer to the Question\n.\nWe are continually iterating our templates, view the most up-to-date template on GitHub. Last updated on 10/12/2023\nBenchmark Results\nGPT-4 Result\nGPT-3.5 Results\nClaude V2 Results\nGPT 4 Turbo\nHow To Run the Eval\nCopy\nfrom\n phoenix\n.\nevals \nimport\n (\n\n\n    RAG_RELEVANCY_PROMPT_RAILS_MAP\n,\n\n\n    RAG_RELEVANCY_PROMPT_TEMPLATE\n,\n\n\n    OpenAIModel\n,\n\n\n    download_benchmark_dataset\n,\n\n\n    llm_classify\n,\n\n\n)\n\n\n\n\nmodel \n=\n \nOpenAIModel\n(\n\n\n    model_name\n=\n""gpt-4""\n,\n\n\n    temperature\n=\n0.0\n,\n\n\n)\n\n\n\n\n#The rails is used to hold the output to specific values based on the template\n\n\n#It will remove text such as "",,,"" or ""...""\n\n\n#Will ensure the binary value expected from the template is returned\n\n\nrails \n=\n \nlist\n(RAG_RELEVANCY_PROMPT_RAILS_MAP.\nvalues\n())\n\n\nrelevance_classifications \n=\n \nllm_classify\n(\n\n\n    dataframe\n=\ndf,\n\n\n    template\n=\nRAG_RELEVANCY_PROMPT_TEMPLATE,\n\n\n    model\n=\nmodel,\n\n\n    rails\n=\nrails,\n\n\n    provide_explanation\n=\nTrue\n, \n#optional to generate explanations for the value produced by the eval LLM\n\n\n)\nThe above runs the RAG relevancy LLM template against the dataframe df.\nRAG Eval\nGPT-4o\nGPT-4\nGPT-4 Turbo\nGemini Pro\nGPT-3.5\nPalm (Text Bison)\nClaude V2\nPrecision\n0.60\n0.70\n0.68\n0.61\n0.42\n0.53\n0.79\nRecall\n0.77\n0.88\n0.91\n1\n1.0\n1\n0.22\nF1\n0.67\n0.78\n0.78\n0.76\n0.59\n0.69\n0.34\nThroughput\nGPT-4\nGPT-4 Turbo\nGPT-3.5\n100 Samples\n113 Sec\n61 sec\n73 Sec\nPrevious\nQ&A on Retrieved Data\nNext\nSummarization\nLast updated \n2 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is RAG?\nAnswer: '}, {'role': 'assistant', 'content': 'RAG stands for Retrieval Augmented Generation. It is a technique that dynamically incorporates specific data as context during the generation process without altering the training data of Language Model Models (LLMs). This allows the model to access and utilize real-time data to provide more tailored and contextually relevant responses. RAG involves loading data, indexing it for querying, storing the index, querying the data structure, and evaluating the effectiveness of the pipeline in providing accurate and relevant responses.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6Mzk=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/retrieval/quickstart-retrieval\ntitle: Quickstart: Retrieval\n\nQuickstart: Retrieval\nDebug your Search and Retrieval LLM workflows\nThis quickstart shows how to start logging your retrievals from your vector datastore to Phoenix and run evaluations.\nNotebooks\nFollow our tutorial in a notebook with our Langchain and LlamaIndex integrations\nFramework\nPhoenix Inferences\nPhoenix Traces & Spans\nLangChain\nRetrieval Analyzer w/ Embeddings\n \nTraces and Spans\n \nLlamaIndex\nRetrieval Analyzer w/ Embeddings\nLogging Retrievals to Phoenix (as Inferences)\nStep 1: Logging Knowledge Base\nThe first thing we need is to collect some sample from your vector store, to be able to compare against later. This is to able to see if some sections are not being retrieved, or some sections are getting a lot of traffic where you might want to beef up your context or documents in that area.\nFor more details, visit this \npage\n.\nid\ntext\nembedding\n1\nVoyager 2 is a spacecraft used by NASA to expl...\n[-0.02785328, -0.04709944, 0.042922903, 0.0559...\nCopy\ncorpus_schema \n=\n px\n.\nSchema\n(\n\n\n    id_column_name\n=\n""id""\n,\n\n\n    document_column_names\n=\nEmbeddingColumnNames\n(\n\n\n        vector_column_name\n=\n""embedding""\n,\n\n\n        raw_data_column_name\n=\n""text""\n,\n\n\n    ),\n\n\n)\nStep 2: Logging Retrieval and Response\nWe also will be logging the prompt/response pairs from the deployed application.\nFor more details, visit this \npage\n.\nquery\nembedding\nretrieved_document_ids\nrelevance_scores\nresponse\nwho was the first person that walked on the moon\n[-0.0126, 0.0039, 0.0217, ...\n[7395, 567965, 323794, ...\n[11.30, 7.67, 5.85, ...\nNeil Armstrong\nCopy\nprimary_schema \n=\n \nSchema\n(\n\n\n    prediction_id_column_name\n=\n""id""\n,\n\n\n    prompt_column_names\n=\nRetrievalEmbeddingColumnNames\n(\n\n\n        vector_column_name\n=\n""embedding""\n,\n\n\n        raw_data_column_name\n=\n""query""\n,\n\n\n        context_retrieval_ids_column_name\n=\n""retrieved_document_ids""\n,\n\n\n        context_retrieval_scores_column_name\n=\n""relevance_scores""\n,\n\n\n    )\n\n\n    response_column_names\n=\n""response""\n,\n\n\n)\nRunning Evaluations on your Retrievals\nIn order to run retrieval Evals the following code can be used for quick analysis of common frameworks of LangChain and LlamaIndex.\nIndependent of the framework you are instrumenting, Phoenix traces allow you to get retrieval data in a common dataframe format that follows the \nOpenInference\n specification.\nCopy\n# Get traces from Phoenix into dataframe \n\n\n\n\nspans_df \n=\n px\n.\nactive_session\n().\nget_spans_dataframe\n()\n\n\nspans_df\n[\n[\n""name""\n,\n \n""span_kind""\n,\n \n""attributes.input.value""\n,\n \n""attributes.retrieval.documents""\n]\n].\nhead\n()\n\n\n\n\nfrom\n phoenix\n.\nsession\n.\nevaluation \nimport\n get_qa_with_reference\n,\n get_retrieved_documents\n\n\n\n\nretrieved_documents_df \n=\n \nget_retrieved_documents\n(px.\nactive_session\n())\n\n\nqueries_df \n=\n \nget_qa_with_reference\n(px.\nactive_session\n())\n\n\nOnce the data is in a dataframe, evaluations can be run on the data. Evaluations can be run on on different spans of data. In the below example we run on the top level spans that represent a single trace.\nQ&A and Hallucination Evals\nThis example shows how to run Q&A and Hallucnation Evals with OpenAI (many other \nmodels\n are available including Anthropic, Mixtral/Mistral, Gemini, OpenAI Azure, Bedrock, etc...)\nCopy\nfrom\n phoenix\n.\ntrace \nimport\n SpanEvaluations\n,\n DocumentEvaluations\n\n\nfrom\n phoenix\n.\nevals \nimport\n (\n\n\n  HALLUCINATION_PROMPT_RAILS_MAP\n,\n\n\n  HALLUCINATION_PROMPT_TEMPLATE\n,\n\n\n  QA_PROMPT_RAILS_MAP\n,\n\n\n  QA_PROMPT_TEMPLATE\n,\n\n\n  OpenAIModel\n,\n\n\n  llm_classify\n,\n\n\n)\n\n\n\n\n# Creating Hallucination Eval which checks if the application hallucinated\n\n\nhallucination_eval \n=\n \nllm_classify\n(\n\n\n  dataframe\n=\nqueries_df,\n\n\n  model\n=\nOpenAIModel\n(\n""gpt-4-turbo-preview""\n, temperature\n=\n0.0\n),\n\n\n  template\n=\nHALLUCINATION_PROMPT_TEMPLATE,\n\nsource: https://docs.arize.com/phoenix/retrieval/concepts-retrieval/troubleshooting-llm-retrieval-with-vector-stores\ntitle: Retrieval with Embeddings\n\nRetrieval with Embeddings\nOverview\nQ&A with Retrieval at a Glance\nLLM Input:\n User Query + retrieved document\nLLM Output:\n Response based on query + document\nEvaluation Metrics:\nDid the LLM answer the question correctly (correctness)\nFor each retrieved document, is the document relevant to answer the user query?\nPossibly the most common use-case for creating a LLM application is to connect an LLM to proprietary data such as enterprise documents or video transcriptions. Applications such as these often times are built on top of LLM frameworks such as \nLangchain\n or \nllama_index\n, which have first-class support for vector store retrievers. Vector Stores enable teams to connect their own data to LLMs. A common application is chatbots looking across a company\'s knowledge base/context to answer specific questions.\nHow to Evaluate Retrieval Systems\nThere are varying degrees of how we can evaluate retrieval systems.\nStep 1:\n First we care if the chatbot is correctly answering the user\'s questions. Are there certain types of questions the chatbot gets wrong more often?\nStep 2: \nOnce we know there\'s an issue, then we need metrics to trace where specifically did it go wrong. Is the issue with retrieval? Are the documents that the system retrieves irrelevant?\nStep 3:\n If retrieval is not the issue, we should check if we even have the right documents to answer the question.\nQuestion\nMetric\nPros\nCons\nIs this a bad response to the answer?\nUser feedback or \nLLM Eval for Q&A\nMost relevant way to measure application\nHard to trace down specifically what to fix\nIs the retrieved context relevant?\nLLM Eval for Relevance\nDirectly measures effectiveness of retrieval\nRequires additional LLMs calls\nIs the knowledge base missing areas of user queries?\nQuery density (drift) - Phoenix generated\nHighlights groups of queries with large distance from context\nIdentifies broad topics missing from knowledge base, but not small gaps\nUsing Phoenix Traces & Spans\nVisualize the chain of the traces and spans for a Q&A chatbot use case. You can click into specific spans.\nWhen clicking into the retrieval span, you can see the relevance score for each document. This can surface irrelevant context.\nUsing Phoenix Inferences to Analyze RAG (Retrieval Augmented Generation)\nStep 1. Identifying Clusters of Bad Responses\nPhoenix surfaces up clusters of similar queries that have poor feedback.\nStep 2: Irrelevant Documents Being Retrieved\nPhoenix can help uncover when irrelevant context is being retrieved using the \nLLM Evals for Relevance\n. You can look at a cluster\'s aggregate relevance metric with precision @k, NDCG, MRR, etc to identify where to improve. You can also look at a single prompt/response pair and see the relevance of documents.\nStep 3: Don\'t Have Any Documents Close Enough\nPhoenix can help you identify if there is context that is missing from your knowledge base. By visualizing query density, you can understand what topics you need to add additional documentation for in order to improve your chatbots responses.\nBy setting the ""primary"" dataset as the user queries, and the ""corpus"" dataset as the context I have in my vector store, I can see if there are clusters of user query embeddings that have no nearby context embeddings, as seen in the example below.\nTroubleshooting Tip:\nFound a problematic cluster you want to dig into, but don\'t want to manually sift through all of the prompts and responses? \nAsk chatGPT to help you understand the make up of the cluster.\n \nTry out the colab here\n.\nLooking for code to get started? Go to our \nQuickstart guide for Search and Retrieval\n.\nPrevious\nConcepts: Retrieval\nNext\nBenchmarking Retrieval\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I debug retrieval?\nAnswer: '}, {'role': 'assistant', 'content': 'To debug retrieval, you can start by logging your retrievals from your vector store to Phoenix and running evaluations. This involves collecting samples from your vector store to compare against later, logging prompt/response pairs from the deployed application, and then running evaluations on the retrieved data. By following these steps, you can identify if certain sections are not being retrieved, if some sections are receiving a lot of traffic, and if the responses are relevant to the queries.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/retrieval/quickstart-retrieval\ntitle: Quickstart: Retrieval\n\nQuickstart: Retrieval\nDebug your Search and Retrieval LLM workflows\nThis quickstart shows how to start logging your retrievals from your vector datastore to Phoenix and run evaluations.\nNotebooks\nFollow our tutorial in a notebook with our Langchain and LlamaIndex integrations\nFramework\nPhoenix Inferences\nPhoenix Traces & Spans\nLangChain\nRetrieval Analyzer w/ Embeddings\n \nTraces and Spans\n \nLlamaIndex\nRetrieval Analyzer w/ Embeddings\nLogging Retrievals to Phoenix (as Inferences)\nStep 1: Logging Knowledge Base\nThe first thing we need is to collect some sample from your vector store, to be able to compare against later. This is to able to see if some sections are not being retrieved, or some sections are getting a lot of traffic where you might want to beef up your context or documents in that area.\nFor more details, visit this \npage\n.\nid\ntext\nembedding\n1\nVoyager 2 is a spacecraft used by NASA to expl...\n[-0.02785328, -0.04709944, 0.042922903, 0.0559...\nCopy\ncorpus_schema \n=\n px\n.\nSchema\n(\n\n\n    id_column_name\n=\n""id""\n,\n\n\n    document_column_names\n=\nEmbeddingColumnNames\n(\n\n\n        vector_column_name\n=\n""embedding""\n,\n\n\n        raw_data_column_name\n=\n""text""\n,\n\n\n    ),\n\n\n)\nStep 2: Logging Retrieval and Response\nWe also will be logging the prompt/response pairs from the deployed application.\nFor more details, visit this \npage\n.\nquery\nembedding\nretrieved_document_ids\nrelevance_scores\nresponse\nwho was the first person that walked on the moon\n[-0.0126, 0.0039, 0.0217, ...\n[7395, 567965, 323794, ...\n[11.30, 7.67, 5.85, ...\nNeil Armstrong\nCopy\nprimary_schema \n=\n \nSchema\n(\n\n\n    prediction_id_column_name\n=\n""id""\n,\n\n\n    prompt_column_names\n=\nRetrievalEmbeddingColumnNames\n(\n\n\n        vector_column_name\n=\n""embedding""\n,\n\n\n        raw_data_column_name\n=\n""query""\n,\n\n\n        context_retrieval_ids_column_name\n=\n""retrieved_document_ids""\n,\n\n\n        context_retrieval_scores_column_name\n=\n""relevance_scores""\n,\n\n\n    )\n\n\n    response_column_names\n=\n""response""\n,\n\n\n)\nRunning Evaluations on your Retrievals\nIn order to run retrieval Evals the following code can be used for quick analysis of common frameworks of LangChain and LlamaIndex.\nIndependent of the framework you are instrumenting, Phoenix traces allow you to get retrieval data in a common dataframe format that follows the \nOpenInference\n specification.\nCopy\n# Get traces from Phoenix into dataframe \n\n\n\n\nspans_df \n=\n px\n.\nactive_session\n().\nget_spans_dataframe\n()\n\n\nspans_df\n[\n[\n""name""\n,\n \n""span_kind""\n,\n \n""attributes.input.value""\n,\n \n""attributes.retrieval.documents""\n]\n].\nhead\n()\n\n\n\n\nfrom\n phoenix\n.\nsession\n.\nevaluation \nimport\n get_qa_with_reference\n,\n get_retrieved_documents\n\n\n\n\nretrieved_documents_df \n=\n \nget_retrieved_documents\n(px.\nactive_session\n())\n\n\nqueries_df \n=\n \nget_qa_with_reference\n(px.\nactive_session\n())\n\n\nOnce the data is in a dataframe, evaluations can be run on the data. Evaluations can be run on on different spans of data. In the below example we run on the top level spans that represent a single trace.\nQ&A and Hallucination Evals\nThis example shows how to run Q&A and Hallucnation Evals with OpenAI (many other \nmodels\n are available including Anthropic, Mixtral/Mistral, Gemini, OpenAI Azure, Bedrock, etc...)\nCopy\nfrom\n phoenix\n.\ntrace \nimport\n SpanEvaluations\n,\n DocumentEvaluations\n\n\nfrom\n phoenix\n.\nevals \nimport\n (\n\n\n  HALLUCINATION_PROMPT_RAILS_MAP\n,\n\n\n  HALLUCINATION_PROMPT_TEMPLATE\n,\n\n\n  QA_PROMPT_RAILS_MAP\n,\n\n\n  QA_PROMPT_TEMPLATE\n,\n\n\n  OpenAIModel\n,\n\n\n  llm_classify\n,\n\n\n)\n\n\n\n\n# Creating Hallucination Eval which checks if the application hallucinated\n\n\nhallucination_eval \n=\n \nllm_classify\n(\n\n\n  dataframe\n=\nqueries_df,\n\n\n  model\n=\nOpenAIModel\n(\n""gpt-4-turbo-preview""\n, temperature\n=\n0.0\n),\n\n\n  template\n=\nHALLUCINATION_PROMPT_TEMPLATE,\n\nsource: https://docs.arize.com/phoenix/retrieval/concepts-retrieval/troubleshooting-llm-retrieval-with-vector-stores\ntitle: Retrieval with Embeddings\n\nRetrieval with Embeddings\nOverview\nQ&A with Retrieval at a Glance\nLLM Input:\n User Query + retrieved document\nLLM Output:\n Response based on query + document\nEvaluation Metrics:\nDid the LLM answer the question correctly (correctness)\nFor each retrieved document, is the document relevant to answer the user query?\nPossibly the most common use-case for creating a LLM application is to connect an LLM to proprietary data such as enterprise documents or video transcriptions. Applications such as these often times are built on top of LLM frameworks such as \nLangchain\n or \nllama_index\n, which have first-class support for vector store retrievers. Vector Stores enable teams to connect their own data to LLMs. A common application is chatbots looking across a company\'s knowledge base/context to answer specific questions.\nHow to Evaluate Retrieval Systems\nThere are varying degrees of how we can evaluate retrieval systems.\nStep 1:\n First we care if the chatbot is correctly answering the user\'s questions. Are there certain types of questions the chatbot gets wrong more often?\nStep 2: \nOnce we know there\'s an issue, then we need metrics to trace where specifically did it go wrong. Is the issue with retrieval? Are the documents that the system retrieves irrelevant?\nStep 3:\n If retrieval is not the issue, we should check if we even have the right documents to answer the question.\nQuestion\nMetric\nPros\nCons\nIs this a bad response to the answer?\nUser feedback or \nLLM Eval for Q&A\nMost relevant way to measure application\nHard to trace down specifically what to fix\nIs the retrieved context relevant?\nLLM Eval for Relevance\nDirectly measures effectiveness of retrieval\nRequires additional LLMs calls\nIs the knowledge base missing areas of user queries?\nQuery density (drift) - Phoenix generated\nHighlights groups of queries with large distance from context\nIdentifies broad topics missing from knowledge base, but not small gaps\nUsing Phoenix Traces & Spans\nVisualize the chain of the traces and spans for a Q&A chatbot use case. You can click into specific spans.\nWhen clicking into the retrieval span, you can see the relevance score for each document. This can surface irrelevant context.\nUsing Phoenix Inferences to Analyze RAG (Retrieval Augmented Generation)\nStep 1. Identifying Clusters of Bad Responses\nPhoenix surfaces up clusters of similar queries that have poor feedback.\nStep 2: Irrelevant Documents Being Retrieved\nPhoenix can help uncover when irrelevant context is being retrieved using the \nLLM Evals for Relevance\n. You can look at a cluster\'s aggregate relevance metric with precision @k, NDCG, MRR, etc to identify where to improve. You can also look at a single prompt/response pair and see the relevance of documents.\nStep 3: Don\'t Have Any Documents Close Enough\nPhoenix can help you identify if there is context that is missing from your knowledge base. By visualizing query density, you can understand what topics you need to add additional documentation for in order to improve your chatbots responses.\nBy setting the ""primary"" dataset as the user queries, and the ""corpus"" dataset as the context I have in my vector store, I can see if there are clusters of user query embeddings that have no nearby context embeddings, as seen in the example below.\nTroubleshooting Tip:\nFound a problematic cluster you want to dig into, but don\'t want to manually sift through all of the prompts and responses? \nAsk chatGPT to help you understand the make up of the cluster.\n \nTry out the colab here\n.\nLooking for code to get started? Go to our \nQuickstart guide for Search and Retrieval\n.\nPrevious\nConcepts: Retrieval\nNext\nBenchmarking Retrieval\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I debug retrieval?\nAnswer: '}, {'role': 'assistant', 'content': 'To debug retrieval, you can start by logging your retrievals from your vector store to Phoenix and running evaluations. This involves collecting samples from your vector store to compare against later, logging prompt/response pairs from the deployed application, and then running evaluations on the retrieved data. By following these steps, you can identify if certain sections are not being retrieved, if some sections are receiving a lot of traffic, and if the responses are relevant to the queries.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NDA=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/use-cases-tracing/rag-evaluation\ntitle: Evaluate RAG\n\nEvaluate RAG\nBuilding a RAG pipeline and evaluating it with Phoenix Evals.\nIn this tutorial we will look into building a RAG pipeline and evaluating it with Phoenix Evals.\nIt has the the following sections:\nUnderstanding Retrieval Augmented Generation (RAG).\nBuilding RAG (with the help of a framework such as LlamaIndex).\nEvaluating RAG with Phoenix Evals.\nRetrieval Augmented Generation (RAG)\nLLMs are trained on vast amounts of data, but these will not include your specific data (things like company knowledge bases and documentation). Retrieval-Augmented Generation (RAG) addresses this by dynamically incorporating your data as context during the generation process. This is done not by altering the training data of the LLMs but by allowing the model to access and utilize your data in real-time to provide more tailored and contextually relevant responses.\nIn RAG, your data is loaded and prepared for queries. This process is called indexing. User queries act on this index, which filters your data down to the most relevant context. This context and your query then are sent to the LLM along with a prompt, and the LLM provides a response.\nRAG is a critical component for building applications such a chatbots or agents and you will want to know RAG techniques on how to get data into your application.\nStages within RAG\nThere are five key stages within RAG, which will in turn be a part of any larger RAG application.\nLoading\n: This refers to getting your data from where it lives - whether it\'s text files, PDFs, another website, a database or an API - into your pipeline.\nIndexing\n: This means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\nStoring\n: Once your data is indexed, you will want to store your index, along with any other metadata, to avoid the need to re-index it.\nQuerying\n: For any given indexing strategy there are many ways you can utilize LLMs and data structures to query, including sub-queries, multi-step queries, and hybrid strategies.\nEvaluation\n: A critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures on how accurate, faithful, and fast your responses to queries are.\nBuild a RAG system\nNow that we have understood the stages of RAG, let\'s build a pipeline. We will use \nLlamaIndex\n for RAG and \nPhoenix Evals\n for evaluation.\nCopy\n!pip install \n-\nqq \n""arize-phoenix[experimental,llama-index]>=2.0""\nCopy\n# The nest_asyncio module enables the nesting of asynchronous functions within an already running async loop.\n\n\n# This is necessary because Jupyter notebooks inherently operate in an asynchronous loop.\n\n\n# By applying nest_asyncio, we can run additional async functions within this existing loop without conflicts.\n\n\nimport\n nest_asyncio\n\n\n\n\nnest_asyncio\n.\napply\n()\n\n\n\n\nimport\n os\n\n\nfrom\n getpass \nimport\n getpass\n\n\n\n\nimport\n pandas \nas\n pd\n\n\nimport\n phoenix \nas\n px\n\n\nfrom\n llama_index \nimport\n SimpleDirectoryReader\n,\n VectorStoreIndex\n,\n set_global_handler\n\n\nfrom\n llama_index\n.\nllms \nimport\n OpenAI\n\n\nfrom\n llama_index\n.\nnode_parser \nimport\n SimpleNodeParser\nDuring this tutorial, we will capture all the data we need to evaluate our RAG pipeline using Phoenix Tracing. To enable this, simply start the phoenix application and instrument LlamaIndex.\nCopy\npx\n.\nlaunch_app\n()\nCopy\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace \nimport\n TracerProvider\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\n\n\nendpoint \n=\n \n""http://127.0.0.1:6006/v1/traces""\n\n\ntracer_provider \n=\n \nTracerProvider\n()\n\n\ntracer_provider\n.\nadd_span_processor\n(\nSimpleSpanProcessor\n(\nOTLPSpanExporter\n(endpoint)))\n\n\n\n\nLlamaIndexInstrumentor\n().\ninstrument\n(tracer_provider\n=\ntracer_provider)\nFor this tutorial we will be using OpenAI for creating synthetic data as well as for evaluation.\nCopy\nif\n \nnot\n (openai_api_key \n:=\n os\n.\ngetenv\n(\n""OPENAI_API_KEY""\n)\n)\n:\n\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals/retrieval-rag-relevance\ntitle: Retrieval (RAG) Relevance\n\nRetrieval (RAG) Relevance\nWhen To Use RAG Eval Template\nThis Eval evaluates whether a retrieved chunk contains an answer to the query. It\'s extremely useful for evaluating retrieval systems.\nRAG Eval Template\nGoogle Colaboratory\nTry it out!\nCopy\nYou are comparing a reference text to a question \nand\n trying to determine \nif\n the reference text\n\n\ncontains information relevant to answering the question\n.\n Here \nis\n the data\n:\n\n\n    [BEGIN DATA]\n\n\n    \n************\n\n\n    [Question]\n:\n \n{\nquery\n}\n\n\n    \n************\n\n\n    [Reference text]\n:\n \n{\nreference\n}\n\n\n    [END DATA]\n\n\n\n\nCompare the Question above to the Reference text\n.\n You must determine whether the Reference text\n\n\ncontains information that can answer the Question\n.\n Please focus on whether the very specific\n\n\nquestion can be answered by the information \nin\n the Reference text\n.\n\n\nYour response must be single word\n,\n either \n""relevant""\n \nor\n \n""unrelated""\n,\n\n\nand\n should \nnot\n contain \nany\n text \nor\n characters aside \nfrom\n that word\n.\n\n\n""unrelated""\n means that the reference text does \nnot\n contain an answer to the Question\n.\n\n\n""relevant""\n means the reference text contains an answer to the Question\n.\nWe are continually iterating our templates, view the most up-to-date template on GitHub. Last updated on 10/12/2023\nBenchmark Results\nGPT-4 Result\nGPT-3.5 Results\nClaude V2 Results\nGPT 4 Turbo\nHow To Run the Eval\nCopy\nfrom\n phoenix\n.\nevals \nimport\n (\n\n\n    RAG_RELEVANCY_PROMPT_RAILS_MAP\n,\n\n\n    RAG_RELEVANCY_PROMPT_TEMPLATE\n,\n\n\n    OpenAIModel\n,\n\n\n    download_benchmark_dataset\n,\n\n\n    llm_classify\n,\n\n\n)\n\n\n\n\nmodel \n=\n \nOpenAIModel\n(\n\n\n    model_name\n=\n""gpt-4""\n,\n\n\n    temperature\n=\n0.0\n,\n\n\n)\n\n\n\n\n#The rails is used to hold the output to specific values based on the template\n\n\n#It will remove text such as "",,,"" or ""...""\n\n\n#Will ensure the binary value expected from the template is returned\n\n\nrails \n=\n \nlist\n(RAG_RELEVANCY_PROMPT_RAILS_MAP.\nvalues\n())\n\n\nrelevance_classifications \n=\n \nllm_classify\n(\n\n\n    dataframe\n=\ndf,\n\n\n    template\n=\nRAG_RELEVANCY_PROMPT_TEMPLATE,\n\n\n    model\n=\nmodel,\n\n\n    rails\n=\nrails,\n\n\n    provide_explanation\n=\nTrue\n, \n#optional to generate explanations for the value produced by the eval LLM\n\n\n)\nThe above runs the RAG relevancy LLM template against the dataframe df.\nRAG Eval\nGPT-4o\nGPT-4\nGPT-4 Turbo\nGemini Pro\nGPT-3.5\nPalm (Text Bison)\nClaude V2\nPrecision\n0.60\n0.70\n0.68\n0.61\n0.42\n0.53\n0.79\nRecall\n0.77\n0.88\n0.91\n1\n1.0\n1\n0.22\nF1\n0.67\n0.78\n0.78\n0.76\n0.59\n0.69\n0.34\nThroughput\nGPT-4\nGPT-4 Turbo\nGPT-3.5\n100 Samples\n113 Sec\n61 sec\n73 Sec\nPrevious\nQ&A on Retrieved Data\nNext\nSummarization\nLast updated \n2 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I improve my RAG system?\nAnswer: '}, {'role': 'assistant', 'content': 'To improve your RAG system, you can focus on enhancing the different stages within the RAG pipeline. This includes optimizing the loading process to efficiently bring in your data from various sources, ensuring effective indexing to create a data structure that facilitates accurate querying, implementing robust storing mechanisms to avoid unnecessary re-indexing, refining querying strategies to leverage LLMs and data structures effectively, and conducting thorough evaluations to measure the accuracy, faithfulness, and speed of your responses to queries. By continuously iterating on these stages and incorporating best practices, you can enhance the performance and relevance of your RAG system.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/use-cases-tracing/rag-evaluation\ntitle: Evaluate RAG\n\nEvaluate RAG\nBuilding a RAG pipeline and evaluating it with Phoenix Evals.\nIn this tutorial we will look into building a RAG pipeline and evaluating it with Phoenix Evals.\nIt has the the following sections:\nUnderstanding Retrieval Augmented Generation (RAG).\nBuilding RAG (with the help of a framework such as LlamaIndex).\nEvaluating RAG with Phoenix Evals.\nRetrieval Augmented Generation (RAG)\nLLMs are trained on vast amounts of data, but these will not include your specific data (things like company knowledge bases and documentation). Retrieval-Augmented Generation (RAG) addresses this by dynamically incorporating your data as context during the generation process. This is done not by altering the training data of the LLMs but by allowing the model to access and utilize your data in real-time to provide more tailored and contextually relevant responses.\nIn RAG, your data is loaded and prepared for queries. This process is called indexing. User queries act on this index, which filters your data down to the most relevant context. This context and your query then are sent to the LLM along with a prompt, and the LLM provides a response.\nRAG is a critical component for building applications such a chatbots or agents and you will want to know RAG techniques on how to get data into your application.\nStages within RAG\nThere are five key stages within RAG, which will in turn be a part of any larger RAG application.\nLoading\n: This refers to getting your data from where it lives - whether it\'s text files, PDFs, another website, a database or an API - into your pipeline.\nIndexing\n: This means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\nStoring\n: Once your data is indexed, you will want to store your index, along with any other metadata, to avoid the need to re-index it.\nQuerying\n: For any given indexing strategy there are many ways you can utilize LLMs and data structures to query, including sub-queries, multi-step queries, and hybrid strategies.\nEvaluation\n: A critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures on how accurate, faithful, and fast your responses to queries are.\nBuild a RAG system\nNow that we have understood the stages of RAG, let\'s build a pipeline. We will use \nLlamaIndex\n for RAG and \nPhoenix Evals\n for evaluation.\nCopy\n!pip install \n-\nqq \n""arize-phoenix[experimental,llama-index]>=2.0""\nCopy\n# The nest_asyncio module enables the nesting of asynchronous functions within an already running async loop.\n\n\n# This is necessary because Jupyter notebooks inherently operate in an asynchronous loop.\n\n\n# By applying nest_asyncio, we can run additional async functions within this existing loop without conflicts.\n\n\nimport\n nest_asyncio\n\n\n\n\nnest_asyncio\n.\napply\n()\n\n\n\n\nimport\n os\n\n\nfrom\n getpass \nimport\n getpass\n\n\n\n\nimport\n pandas \nas\n pd\n\n\nimport\n phoenix \nas\n px\n\n\nfrom\n llama_index \nimport\n SimpleDirectoryReader\n,\n VectorStoreIndex\n,\n set_global_handler\n\n\nfrom\n llama_index\n.\nllms \nimport\n OpenAI\n\n\nfrom\n llama_index\n.\nnode_parser \nimport\n SimpleNodeParser\nDuring this tutorial, we will capture all the data we need to evaluate our RAG pipeline using Phoenix Tracing. To enable this, simply start the phoenix application and instrument LlamaIndex.\nCopy\npx\n.\nlaunch_app\n()\nCopy\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace \nimport\n TracerProvider\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\n\n\nendpoint \n=\n \n""http://127.0.0.1:6006/v1/traces""\n\n\ntracer_provider \n=\n \nTracerProvider\n()\n\n\ntracer_provider\n.\nadd_span_processor\n(\nSimpleSpanProcessor\n(\nOTLPSpanExporter\n(endpoint)))\n\n\n\n\nLlamaIndexInstrumentor\n().\ninstrument\n(tracer_provider\n=\ntracer_provider)\nFor this tutorial we will be using OpenAI for creating synthetic data as well as for evaluation.\nCopy\nif\n \nnot\n (openai_api_key \n:=\n os\n.\ngetenv\n(\n""OPENAI_API_KEY""\n)\n)\n:\n\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals/retrieval-rag-relevance\ntitle: Retrieval (RAG) Relevance\n\nRetrieval (RAG) Relevance\nWhen To Use RAG Eval Template\nThis Eval evaluates whether a retrieved chunk contains an answer to the query. It\'s extremely useful for evaluating retrieval systems.\nRAG Eval Template\nGoogle Colaboratory\nTry it out!\nCopy\nYou are comparing a reference text to a question \nand\n trying to determine \nif\n the reference text\n\n\ncontains information relevant to answering the question\n.\n Here \nis\n the data\n:\n\n\n    [BEGIN DATA]\n\n\n    \n************\n\n\n    [Question]\n:\n \n{\nquery\n}\n\n\n    \n************\n\n\n    [Reference text]\n:\n \n{\nreference\n}\n\n\n    [END DATA]\n\n\n\n\nCompare the Question above to the Reference text\n.\n You must determine whether the Reference text\n\n\ncontains information that can answer the Question\n.\n Please focus on whether the very specific\n\n\nquestion can be answered by the information \nin\n the Reference text\n.\n\n\nYour response must be single word\n,\n either \n""relevant""\n \nor\n \n""unrelated""\n,\n\n\nand\n should \nnot\n contain \nany\n text \nor\n characters aside \nfrom\n that word\n.\n\n\n""unrelated""\n means that the reference text does \nnot\n contain an answer to the Question\n.\n\n\n""relevant""\n means the reference text contains an answer to the Question\n.\nWe are continually iterating our templates, view the most up-to-date template on GitHub. Last updated on 10/12/2023\nBenchmark Results\nGPT-4 Result\nGPT-3.5 Results\nClaude V2 Results\nGPT 4 Turbo\nHow To Run the Eval\nCopy\nfrom\n phoenix\n.\nevals \nimport\n (\n\n\n    RAG_RELEVANCY_PROMPT_RAILS_MAP\n,\n\n\n    RAG_RELEVANCY_PROMPT_TEMPLATE\n,\n\n\n    OpenAIModel\n,\n\n\n    download_benchmark_dataset\n,\n\n\n    llm_classify\n,\n\n\n)\n\n\n\n\nmodel \n=\n \nOpenAIModel\n(\n\n\n    model_name\n=\n""gpt-4""\n,\n\n\n    temperature\n=\n0.0\n,\n\n\n)\n\n\n\n\n#The rails is used to hold the output to specific values based on the template\n\n\n#It will remove text such as "",,,"" or ""...""\n\n\n#Will ensure the binary value expected from the template is returned\n\n\nrails \n=\n \nlist\n(RAG_RELEVANCY_PROMPT_RAILS_MAP.\nvalues\n())\n\n\nrelevance_classifications \n=\n \nllm_classify\n(\n\n\n    dataframe\n=\ndf,\n\n\n    template\n=\nRAG_RELEVANCY_PROMPT_TEMPLATE,\n\n\n    model\n=\nmodel,\n\n\n    rails\n=\nrails,\n\n\n    provide_explanation\n=\nTrue\n, \n#optional to generate explanations for the value produced by the eval LLM\n\n\n)\nThe above runs the RAG relevancy LLM template against the dataframe df.\nRAG Eval\nGPT-4o\nGPT-4\nGPT-4 Turbo\nGemini Pro\nGPT-3.5\nPalm (Text Bison)\nClaude V2\nPrecision\n0.60\n0.70\n0.68\n0.61\n0.42\n0.53\n0.79\nRecall\n0.77\n0.88\n0.91\n1\n1.0\n1\n0.22\nF1\n0.67\n0.78\n0.78\n0.76\n0.59\n0.69\n0.34\nThroughput\nGPT-4\nGPT-4 Turbo\nGPT-3.5\n100 Samples\n113 Sec\n61 sec\n73 Sec\nPrevious\nQ&A on Retrieved Data\nNext\nSummarization\nLast updated \n2 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I improve my RAG system?\nAnswer: '}, {'role': 'assistant', 'content': 'To improve your RAG system, you can focus on enhancing the different stages within the RAG pipeline. This includes optimizing the loading process to efficiently bring in your data from various sources, ensuring effective indexing to create a data structure that facilitates accurate querying, implementing robust storing mechanisms to avoid unnecessary re-indexing, refining querying strategies to leverage LLMs and data structures effectively, and conducting thorough evaluations to measure the accuracy, faithfulness, and speed of your responses to queries. By continuously iterating on these stages and incorporating best practices, you can enhance the performance and relevance of your RAG system.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NDE=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/hosted-phoenix\ntitle: Hosted Phoenix\n\nHosted Phoenix\nWe now offer a hosted version of Phoenix to make it easier for developers to use Phoenix to trace their LLM applications and avoid setting up infrastructure. You can use our Colab links to follow along.\nFramework\nLlamaindex\nColab\nLlamaindex with Llamacloud\nColab\nOpenAI\nColab\nThe main differences for Hosted Phoenix:\nHosted Phoenix runs the latest version of our open source package and gates access to your data behind API keys and user authentication.\nYou must create an account\nYou need to add an API key as an environment variable during tracing\nYou need to add an API key as an environment variable when using the Client SDK\nWe also use 3rd party analytics tools to measure usage of our application to improve our services.\nHow to create an account\nClick signup on \nphoenix.arize.com\n. We offer logins via Google, Github, and email. This account will use the same account credentials as your Arize account if you have one.\nTracing: How to send in your first trace\nGet your API keys from your Phoenix application on the left hand side. \nHere\'s the full sample code for LlamaIndex and OpenAI instrumentation. You can see all of our automatic tracing options \nhere\n.\nLlamaIndex\nOpenAI\nInstall the following libraries\nCopy\n!pip install opentelemetry-sdk opentelemetry-exporter-otlp\n\n\n!pip install ""arize-phoenix[evals,llama-index]"" ""openai>=1"" gcsfs nest-asyncio ""openinference-instrumentation-llama-index>=2.0.0""\nUse the following python code to start instrumentation.\nCopy\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\n\n\n# Setup authentication and endpoint\n\n\nos\n.\nenviron\n[\n""OTEL_EXPORTER_OTLP_HEADERS""\n]\n \n=\n f\n""api_key=\n{PHOENIX_API_KEY}\n""\n\n\nendpoint \n=\n \n""https://app.phoenix.arize.com/v1/traces""\n\n\n\n\n# Setup tracing with OpenTelemetry\n\n\nspan_phoenix_processor \n=\n \nSimpleSpanProcessor\n(\nOTLPSpanExporter\n(endpoint\n=\nendpoint))\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n()\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor\n=\nspan_phoenix_processor)\n\n\n\n\n# Start instrumentation\n\n\nLlamaIndexInstrumentor\n().\ninstrument\n(tracer_provider\n=\ntracer_provider, skip_dep_check\n=\nTrue\n)\nCheckout our colab tutorial here:\nInstall the following libraries\nCopy\npip\n \ninstall\n \narize-otel\n \nopeninference-instrumentation-openai\n \nopenai\nThen, use our library \narize-otel\n, which sets up OpenTelemetry tracing with Hosted Phoenix. Run the following code to start instrumentation.\nCopy\nimport\n os\n\n\nfrom\n arize_otel \nimport\n register_otel\n,\n Endpoints\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nopenai \nimport\n OpenAIInstrumentor\n\n\n\n\n# Setup OTEL tracing for hosted Phoenix\n\n\n# Endpoints.HOSTED_PHOENIX = ""https://app.phoenix.arize.com""\n\n\nPHOENIX_API_KEY \n=\n os\n.\nenviron\n[\n""PHOENIX_API_KEY""\n]\n\n\nregister_otel\n(\n\n\n    endpoints\n=\n[Endpoints.HOSTED_PHOENIX],\n\n\n    api_key\n=\nPHOENIX_API_KEY\n\n\n)\n\n\n\n\n# Turn on instrumentation for OpenAI\n\n\nOpenAIInstrumentor\n().\ninstrument\n()\nCheckout our colab tutorial here:\nGoogle Colab\nUsing the Client SDK (downloading data & uploading datasets)\nOnce you collect trace data from the above configuration, you can access the data using the client SDK. You can also upload datasets for experiments using the client SDK.\nYou\'ll need to add the following environment variable to authenticate to hosted Phoenix.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n \n""api_key=...""\nHere\'s more sample code.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n f\n""api_key=...""\n\n\nos\n.\nenviron\n[\n""PHOENIX_COLLECTOR_ENDPOINT""\n]\n \n=\n \n""https://app.phoenix.arize.com""\n\n\n\n\nimport\n phoenix \nas\n px\n\nsource: https://docs.arize.com/phoenix/hosted-phoenix\ntitle: Hosted Phoenix\n\nPHOENIX_API_KEY \n=\n os\n.\nenviron\n[\n""PHOENIX_API_KEY""\n]\n\n\nregister_otel\n(\n\n\n    endpoints\n=\n[Endpoints.HOSTED_PHOENIX],\n\n\n    api_key\n=\nPHOENIX_API_KEY\n\n\n)\n\n\n\n\n# Turn on instrumentation for OpenAI\n\n\nOpenAIInstrumentor\n().\ninstrument\n()\nCheckout our colab tutorial here:\nGoogle Colab\nUsing the Client SDK (downloading data & uploading datasets)\nOnce you collect trace data from the above configuration, you can access the data using the client SDK. You can also upload datasets for experiments using the client SDK.\nYou\'ll need to add the following environment variable to authenticate to hosted Phoenix.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n \n""api_key=...""\nHere\'s more sample code.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n f\n""api_key=...""\n\n\nos\n.\nenviron\n[\n""PHOENIX_COLLECTOR_ENDPOINT""\n]\n \n=\n \n""https://app.phoenix.arize.com""\n\n\n\n\nimport\n phoenix \nas\n px\n\n\n\n\npx_client \n=\n px\n.\nClient\n()\n\n\nphoenix_df \n=\n px_client\n.\nget_spans_dataframe\n()\nFAQ\nWill hosted Phoenix be on the latest version of Phoenix?\nOn account creation, we will always use the latest version of Phoenix. We try to keep all instances of hosted Phoenix up to date and run upgrades for them when new versions are available. There will be a few minutes of downtime during these periods.\nData retention\nWe have a 30 day data retention policy. We are working on plans to offer a longer data retention period.\nSharing\nCurrently accounts are setup to be used specifically for one developer. We will be adding ways to share your traces with other developers on your team shortly!\nPricing\nHosted Phoenix is free for all developers. We will add a paid tier in the future which increases your data retention and also give you access to more storage.\nAre there other demos available?\nYes. This demo and \naccompanying blog\n show how to deploy Phoenix via Docker/Kubernetes:\nPrevious\nQuickstart\nNext\nUser Guide\nLast updated \n7 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is hosted Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Hosted Phoenix is a version of Phoenix that is hosted by the service provider, allowing developers to trace their LLM applications without the need to set up their own infrastructure. It runs the latest version of the open source package and requires users to create an account, add API keys for data access and user authentication, and use 3rd party analytics tools for service improvement.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/hosted-phoenix\ntitle: Hosted Phoenix\n\nHosted Phoenix\nWe now offer a hosted version of Phoenix to make it easier for developers to use Phoenix to trace their LLM applications and avoid setting up infrastructure. You can use our Colab links to follow along.\nFramework\nLlamaindex\nColab\nLlamaindex with Llamacloud\nColab\nOpenAI\nColab\nThe main differences for Hosted Phoenix:\nHosted Phoenix runs the latest version of our open source package and gates access to your data behind API keys and user authentication.\nYou must create an account\nYou need to add an API key as an environment variable during tracing\nYou need to add an API key as an environment variable when using the Client SDK\nWe also use 3rd party analytics tools to measure usage of our application to improve our services.\nHow to create an account\nClick signup on \nphoenix.arize.com\n. We offer logins via Google, Github, and email. This account will use the same account credentials as your Arize account if you have one.\nTracing: How to send in your first trace\nGet your API keys from your Phoenix application on the left hand side. \nHere\'s the full sample code for LlamaIndex and OpenAI instrumentation. You can see all of our automatic tracing options \nhere\n.\nLlamaIndex\nOpenAI\nInstall the following libraries\nCopy\n!pip install opentelemetry-sdk opentelemetry-exporter-otlp\n\n\n!pip install ""arize-phoenix[evals,llama-index]"" ""openai>=1"" gcsfs nest-asyncio ""openinference-instrumentation-llama-index>=2.0.0""\nUse the following python code to start instrumentation.\nCopy\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\n\n\n# Setup authentication and endpoint\n\n\nos\n.\nenviron\n[\n""OTEL_EXPORTER_OTLP_HEADERS""\n]\n \n=\n f\n""api_key=\n{PHOENIX_API_KEY}\n""\n\n\nendpoint \n=\n \n""https://app.phoenix.arize.com/v1/traces""\n\n\n\n\n# Setup tracing with OpenTelemetry\n\n\nspan_phoenix_processor \n=\n \nSimpleSpanProcessor\n(\nOTLPSpanExporter\n(endpoint\n=\nendpoint))\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n()\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor\n=\nspan_phoenix_processor)\n\n\n\n\n# Start instrumentation\n\n\nLlamaIndexInstrumentor\n().\ninstrument\n(tracer_provider\n=\ntracer_provider, skip_dep_check\n=\nTrue\n)\nCheckout our colab tutorial here:\nInstall the following libraries\nCopy\npip\n \ninstall\n \narize-otel\n \nopeninference-instrumentation-openai\n \nopenai\nThen, use our library \narize-otel\n, which sets up OpenTelemetry tracing with Hosted Phoenix. Run the following code to start instrumentation.\nCopy\nimport\n os\n\n\nfrom\n arize_otel \nimport\n register_otel\n,\n Endpoints\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nopenai \nimport\n OpenAIInstrumentor\n\n\n\n\n# Setup OTEL tracing for hosted Phoenix\n\n\n# Endpoints.HOSTED_PHOENIX = ""https://app.phoenix.arize.com""\n\n\nPHOENIX_API_KEY \n=\n os\n.\nenviron\n[\n""PHOENIX_API_KEY""\n]\n\n\nregister_otel\n(\n\n\n    endpoints\n=\n[Endpoints.HOSTED_PHOENIX],\n\n\n    api_key\n=\nPHOENIX_API_KEY\n\n\n)\n\n\n\n\n# Turn on instrumentation for OpenAI\n\n\nOpenAIInstrumentor\n().\ninstrument\n()\nCheckout our colab tutorial here:\nGoogle Colab\nUsing the Client SDK (downloading data & uploading datasets)\nOnce you collect trace data from the above configuration, you can access the data using the client SDK. You can also upload datasets for experiments using the client SDK.\nYou\'ll need to add the following environment variable to authenticate to hosted Phoenix.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n \n""api_key=...""\nHere\'s more sample code.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n f\n""api_key=...""\n\n\nos\n.\nenviron\n[\n""PHOENIX_COLLECTOR_ENDPOINT""\n]\n \n=\n \n""https://app.phoenix.arize.com""\n\n\n\n\nimport\n phoenix \nas\n px\n\nsource: https://docs.arize.com/phoenix/hosted-phoenix\ntitle: Hosted Phoenix\n\nPHOENIX_API_KEY \n=\n os\n.\nenviron\n[\n""PHOENIX_API_KEY""\n]\n\n\nregister_otel\n(\n\n\n    endpoints\n=\n[Endpoints.HOSTED_PHOENIX],\n\n\n    api_key\n=\nPHOENIX_API_KEY\n\n\n)\n\n\n\n\n# Turn on instrumentation for OpenAI\n\n\nOpenAIInstrumentor\n().\ninstrument\n()\nCheckout our colab tutorial here:\nGoogle Colab\nUsing the Client SDK (downloading data & uploading datasets)\nOnce you collect trace data from the above configuration, you can access the data using the client SDK. You can also upload datasets for experiments using the client SDK.\nYou\'ll need to add the following environment variable to authenticate to hosted Phoenix.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n \n""api_key=...""\nHere\'s more sample code.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n f\n""api_key=...""\n\n\nos\n.\nenviron\n[\n""PHOENIX_COLLECTOR_ENDPOINT""\n]\n \n=\n \n""https://app.phoenix.arize.com""\n\n\n\n\nimport\n phoenix \nas\n px\n\n\n\n\npx_client \n=\n px\n.\nClient\n()\n\n\nphoenix_df \n=\n px_client\n.\nget_spans_dataframe\n()\nFAQ\nWill hosted Phoenix be on the latest version of Phoenix?\nOn account creation, we will always use the latest version of Phoenix. We try to keep all instances of hosted Phoenix up to date and run upgrades for them when new versions are available. There will be a few minutes of downtime during these periods.\nData retention\nWe have a 30 day data retention policy. We are working on plans to offer a longer data retention period.\nSharing\nCurrently accounts are setup to be used specifically for one developer. We will be adding ways to share your traces with other developers on your team shortly!\nPricing\nHosted Phoenix is free for all developers. We will add a paid tier in the future which increases your data retention and also give you access to more storage.\nAre there other demos available?\nYes. This demo and \naccompanying blog\n show how to deploy Phoenix via Docker/Kubernetes:\nPrevious\nQuickstart\nNext\nUser Guide\nLast updated \n7 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is hosted Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Hosted Phoenix is a version of Phoenix that is hosted by the service provider, allowing developers to trace their LLM applications without the need to set up their own infrastructure. It runs the latest version of the open source package and requires users to create an account, add API keys for data access and user authentication, and use 3rd party analytics tools for service improvement.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NDI=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/deployment/deploying-phoenix\ntitle: Quickstart: Deployment\n\nphoenix\n:\n\n\n    \nimage\n:\n \narizephoenix/phoenix:latest\n\n\n    \nports\n:\n\n\n      - \n""6006:6006""\n  \n# UI and OTLP HTTP collector\n\n\n      - \n""4317:4317""\n  \n# OTLP gRPC collector\n\n\n  \nbackend\n:\n\n\n    \nbuild\n:\n\n\n      \ncontext\n:\n \n./backend\n\n\n      \ndockerfile\n:\n \nDockerfile\n\n\n      \nargs\n:\n\n\n        \nOPENAI_API_KEY\n:\n \n${OPENAI_API_KEY}\n\n\n    \nports\n:\n\n\n      - \n""8000:8000""\n\n\n    \nenvironment\n:\n\n\n      - \nOPENAI_API_KEY=${OPENAI_API_KEY}\n\n\n      - \nCOLLECTOR_ENDPOINT=http://phoenix:6006/v1/traces\n\n\n      - \nPROD_CORS_ORIGIN=http://localhost:3000\n\n\n      \n# Set INSTRUMENT_LLAMA_INDEX=false to disable instrumentation\n\n\n      - \nINSTRUMENT_LLAMA_INDEX=true\n\n\n    \nhealthcheck\n:\n\n\n      \ntest\n:\n [\n""CMD""\n,\n \n""wget""\n,\n \n""--spider""\n,\n \n""http://0.0.0.0:8000/api/chat/healthcheck""\n]\n\n\n      \ninterval\n:\n \n5s\n\n\n      \ntimeout\n:\n \n1s\n\n\n      \nretries\n:\n \n5\n\n\n  \nfrontend\n:\n\n\n    \nbuild\n:\n \nfrontend\n\n\n    \nports\n:\n\n\n      - \n""3000:3000""\n\n\n    \ndepends_on\n:\n\n\n      \nbackend\n:\n\n\n        \ncondition\n:\n \nservice_healthy\nThis way you will always have a running Phoenix instance when you run\nCopy\ndocker compose up\nFor the full details of on how to configure Phoenix, check out the \nConfiguration section\n\n\nPrevious\nDeployment\nNext\nPersistence\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/deployment\ntitle: Deployment\n\nDeployment\nHow to self-host a phoenix instance\nPhoenix can natively be run as a container that collects traces and evaluations as you run them.\nQuickstart\nHow to deploy an LLM application with Phoenix observability\nPersistence\nHow to setup a persistent disc or database\nKubernetes\nHow to deploy Phoenix on K8s\nDocker\nHow to deploy Phoenix using Docker\nImages\nThis table lists the images we publish that can be used to run Phoenix.\nImage Tag\nDescription\narizephoenix/phoenix:latest\nLatest released version of Phoenix using root permissions.\narizephoenix/phoenix:latest-nonroot\nLatest released version of Phoenix using nonroot permissions. \nEnsure the image has the required filesystem permissions before using.\narizephoenix/phoenix:latest-debug\nLatest released version of Phoenix using a debug base image.\narizephoenix/phoenix:version-X.X.X\nBuild for a specific release version using root permissions.\narizephoenix/phoenix:version-X.X.X-nonroot\nBuild for a specific release version using nonroot permissions.\narizephoenix/phoenix:version-X.X.X-debug\nBuild for a specific release version using a debug image.\nPrevious\nConfiguration\nNext\nQuickstart: Deployment\nLast updated \n3 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the deployment options for Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'The deployment options for Phoenix include self-hosting Phoenix as a container, deploying an LLM application with Phoenix observability, setting up a persistent disc or database, deploying Phoenix on Kubernetes, and deploying Phoenix using Docker.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/deployment/deploying-phoenix\ntitle: Quickstart: Deployment\n\nphoenix\n:\n\n\n    \nimage\n:\n \narizephoenix/phoenix:latest\n\n\n    \nports\n:\n\n\n      - \n""6006:6006""\n  \n# UI and OTLP HTTP collector\n\n\n      - \n""4317:4317""\n  \n# OTLP gRPC collector\n\n\n  \nbackend\n:\n\n\n    \nbuild\n:\n\n\n      \ncontext\n:\n \n./backend\n\n\n      \ndockerfile\n:\n \nDockerfile\n\n\n      \nargs\n:\n\n\n        \nOPENAI_API_KEY\n:\n \n${OPENAI_API_KEY}\n\n\n    \nports\n:\n\n\n      - \n""8000:8000""\n\n\n    \nenvironment\n:\n\n\n      - \nOPENAI_API_KEY=${OPENAI_API_KEY}\n\n\n      - \nCOLLECTOR_ENDPOINT=http://phoenix:6006/v1/traces\n\n\n      - \nPROD_CORS_ORIGIN=http://localhost:3000\n\n\n      \n# Set INSTRUMENT_LLAMA_INDEX=false to disable instrumentation\n\n\n      - \nINSTRUMENT_LLAMA_INDEX=true\n\n\n    \nhealthcheck\n:\n\n\n      \ntest\n:\n [\n""CMD""\n,\n \n""wget""\n,\n \n""--spider""\n,\n \n""http://0.0.0.0:8000/api/chat/healthcheck""\n]\n\n\n      \ninterval\n:\n \n5s\n\n\n      \ntimeout\n:\n \n1s\n\n\n      \nretries\n:\n \n5\n\n\n  \nfrontend\n:\n\n\n    \nbuild\n:\n \nfrontend\n\n\n    \nports\n:\n\n\n      - \n""3000:3000""\n\n\n    \ndepends_on\n:\n\n\n      \nbackend\n:\n\n\n        \ncondition\n:\n \nservice_healthy\nThis way you will always have a running Phoenix instance when you run\nCopy\ndocker compose up\nFor the full details of on how to configure Phoenix, check out the \nConfiguration section\n\n\nPrevious\nDeployment\nNext\nPersistence\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/deployment\ntitle: Deployment\n\nDeployment\nHow to self-host a phoenix instance\nPhoenix can natively be run as a container that collects traces and evaluations as you run them.\nQuickstart\nHow to deploy an LLM application with Phoenix observability\nPersistence\nHow to setup a persistent disc or database\nKubernetes\nHow to deploy Phoenix on K8s\nDocker\nHow to deploy Phoenix using Docker\nImages\nThis table lists the images we publish that can be used to run Phoenix.\nImage Tag\nDescription\narizephoenix/phoenix:latest\nLatest released version of Phoenix using root permissions.\narizephoenix/phoenix:latest-nonroot\nLatest released version of Phoenix using nonroot permissions. \nEnsure the image has the required filesystem permissions before using.\narizephoenix/phoenix:latest-debug\nLatest released version of Phoenix using a debug base image.\narizephoenix/phoenix:version-X.X.X\nBuild for a specific release version using root permissions.\narizephoenix/phoenix:version-X.X.X-nonroot\nBuild for a specific release version using nonroot permissions.\narizephoenix/phoenix:version-X.X.X-debug\nBuild for a specific release version using a debug image.\nPrevious\nConfiguration\nNext\nQuickstart: Deployment\nLast updated \n3 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the deployment options for Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'The deployment options for Phoenix include self-hosting Phoenix as a container, deploying an LLM application with Phoenix observability, setting up a persistent disc or database, deploying Phoenix on Kubernetes, and deploying Phoenix using Docker.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NDM=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/api/session\ntitle: Session\n\nUsage\nLaunch Phoenix as a collector of \nLLM Traces\n generated by your LLM applications. By default the collector listens on port \n6006\n.\nCopy\nsession = px.launch_app()\nLaunch Phoenix with primary and reference inferences \nprim_inf_\n and \nref_inf_\n, both instances of \nInferences\n, with\nCopy\nsession \n=\n px\n.\nlaunch_app\n(prim_inf_, ref_inf_)\nAlternatively, launch Phoenix with a single dataset \ninf\n, an instance of \nInferences\n, with\nCopy\nsession \n=\n px\n.\nlaunch_app\n(inf)\nThen \nsession\n is an instance of \nSession\n that can be used to open the Phoenix UI in an inline frame within the notebook or in a separate browser tab or window.\nphoenix.active_session\nCopy\ndef\n \nactive_session\n() \n->\n Optional\n[\nSession\n]\nReturns the active Phoenix \nSession\n if one exists, otherwise, returns \nNone\n.\n[\nsession\n]\nUsage\nSuppose you previously ran\nCopy\npx\n.\nlaunch_app\n()\nwithout assigning the returned \nSession\n instance to a variable. If you later find that you need access to the running session object, run\nCopy\nsession \n=\n px\n.\nactive_session\n()\nThen \nsession\n is an instance of \nSession\n that can be used to open the Phoenix UI in an inline frame within your notebook or in a separate browser tab or window.\nphoenix.close_app\nCopy\ndef\n \nclose_app\n() \n->\n \nNone\nCloses the running Phoenix session, if it exists.\nThe Phoenix server will continue running in the background until it is explicitly closed, even if the Jupyter server and kernel are stopped.\n[\nsource\n]\nUsage\nSuppose you previously launched a Phoenix session with \nlaunch_app\n. You can close the running session with\nCopy\npx\n.\nclose_app\n()\nphoenix.Session\nA session that maintains the state of the Phoenix app. Obtain the active session as follows.\nCopy\nsession = px.active_session()\nMethods\nview\n(height: int = 1000) -> IPython.display.IFrame\n\nDisplays the Phoenix UI for a running session within an inline frame in the notebook.\n\n\nParameters\nheight\n (int = 1000): The height in pixels of the inline frame element displaying the Phoenix UI within the notebook. Used to adjust the height of the inline frame to the desired height.\nget_spans_dataframe\n -> pandas.DataFrame\n\nReturns spans in a pandas.dataframe. Filters can be applied. See \nLLM Traces\n for more about tracing your LLM application.\n\nsource: https://docs.arize.com/phoenix/api/session\ntitle: Session\n\nos\n.\nenviron\n[\n""PHOENIX_PORT""\n]\n \n=\n \n""54321""\nPrevious\nInferences and Schema\nNext\nClient\nLast updated \n3 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Tell me about sessions\nAnswer: '}, {'role': 'assistant', 'content': ""Sessions in Phoenix are instances that maintain the state of the Phoenix app. You can launch Phoenix with different configurations, such as primary and reference inferences or a single dataset, resulting in a session object that can be used to open the Phoenix UI within a notebook or in a separate browser tab or window. Additionally, there is a method to retrieve the active session if one exists, and another method to close the running Phoenix session. The session object provides methods like 'view' to display the Phoenix UI within a notebook and 'get_spans_dataframe' to retrieve spans in a pandas DataFrame.""}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/api/session\ntitle: Session\n\nUsage\nLaunch Phoenix as a collector of \nLLM Traces\n generated by your LLM applications. By default the collector listens on port \n6006\n.\nCopy\nsession = px.launch_app()\nLaunch Phoenix with primary and reference inferences \nprim_inf_\n and \nref_inf_\n, both instances of \nInferences\n, with\nCopy\nsession \n=\n px\n.\nlaunch_app\n(prim_inf_, ref_inf_)\nAlternatively, launch Phoenix with a single dataset \ninf\n, an instance of \nInferences\n, with\nCopy\nsession \n=\n px\n.\nlaunch_app\n(inf)\nThen \nsession\n is an instance of \nSession\n that can be used to open the Phoenix UI in an inline frame within the notebook or in a separate browser tab or window.\nphoenix.active_session\nCopy\ndef\n \nactive_session\n() \n->\n Optional\n[\nSession\n]\nReturns the active Phoenix \nSession\n if one exists, otherwise, returns \nNone\n.\n[\nsession\n]\nUsage\nSuppose you previously ran\nCopy\npx\n.\nlaunch_app\n()\nwithout assigning the returned \nSession\n instance to a variable. If you later find that you need access to the running session object, run\nCopy\nsession \n=\n px\n.\nactive_session\n()\nThen \nsession\n is an instance of \nSession\n that can be used to open the Phoenix UI in an inline frame within your notebook or in a separate browser tab or window.\nphoenix.close_app\nCopy\ndef\n \nclose_app\n() \n->\n \nNone\nCloses the running Phoenix session, if it exists.\nThe Phoenix server will continue running in the background until it is explicitly closed, even if the Jupyter server and kernel are stopped.\n[\nsource\n]\nUsage\nSuppose you previously launched a Phoenix session with \nlaunch_app\n. You can close the running session with\nCopy\npx\n.\nclose_app\n()\nphoenix.Session\nA session that maintains the state of the Phoenix app. Obtain the active session as follows.\nCopy\nsession = px.active_session()\nMethods\nview\n(height: int = 1000) -> IPython.display.IFrame\n\nDisplays the Phoenix UI for a running session within an inline frame in the notebook.\n\n\nParameters\nheight\n (int = 1000): The height in pixels of the inline frame element displaying the Phoenix UI within the notebook. Used to adjust the height of the inline frame to the desired height.\nget_spans_dataframe\n -> pandas.DataFrame\n\nReturns spans in a pandas.dataframe. Filters can be applied. See \nLLM Traces\n for more about tracing your LLM application.\n\nsource: https://docs.arize.com/phoenix/api/session\ntitle: Session\n\nos\n.\nenviron\n[\n""PHOENIX_PORT""\n]\n \n=\n \n""54321""\nPrevious\nInferences and Schema\nNext\nClient\nLast updated \n3 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Tell me about sessions\nAnswer: '}, {'role': 'assistant', 'content': ""Sessions in Phoenix are instances that maintain the state of the Phoenix app. You can launch Phoenix with different configurations, such as primary and reference inferences or a single dataset, resulting in a session object that can be used to open the Phoenix UI within a notebook or in a separate browser tab or window. Additionally, there is a method to retrieve the active session if one exists, and another method to close the running Phoenix session. The session object provides methods like 'view' to display the Phoenix UI within a notebook and 'get_spans_dataframe' to retrieve spans in a pandas DataFrame.""}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NDQ=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/customize-traces\ntitle: Customize Traces\n\nCustomize Traces\nOftentimes you want to customize various aspects of traces you log to Phoenix\nLog to a specific project\nPhoenix uses \nprojects\n to group traces. If left unspecified, all traces are sent to a default project.\nNotebook\nServer (Python)\nIn the notebook, you can set the \nPHOENIX_PROJECT_NAME\n environment variable \nbefore \nadding instrumentation or running any of your code.\nIn python this would look like:\nCopy\nimport\n os\n\n\n\n\nos\n.\nenviron\n[\n\'PHOENIX_PROJECT_NAME\'\n]\n \n=\n \n""<your-project-name>""\nNote that setting a project via an environment variable only works in a notebook and must be done \nBEFORE\n instrumentation is initialized. If you are using OpenInference Instrumentation, see the Server tab for how to set the project name in the Resource attributes.\nIf you are using Phoenix as a collector and running your application separately, you can set the project name in the \nResource\n attributes for the trace provider. \nCopy\nfrom\n openinference\n.\nsemconv\n.\nresource \nimport\n ResourceAttributes\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\nresources \nimport\n Resource\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\n\n\nresource \n=\n \nResource\n(attributes\n=\n{\n\n\n    ResourceAttributes.PROJECT_NAME: \n\'<your-project-name>\'\n\n\n})\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n(resource\n=\nresource)\n\n\nspan_exporter \n=\n \nOTLPSpanExporter\n(endpoint\n=\n""http://phoenix:6006/v1/traces""\n)\n\n\nspan_processor \n=\n \nSimpleSpanProcessor\n(span_exporter\n=\nspan_exporter)\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor\n=\nspan_processor)\n\n\ntrace_api\n.\nset_tracer_provider\n(tracer_provider\n=\ntracer_provider)\n\n\n# Add any auto-instrumentation you want \n\n\nLlamaIndexInstrumentor\n().\ninstrument\n()\nProjects work by setting something called the \nResource\n attributes (as seen in the Server example above). The phoenix server uses the project name attribute to group traces into the appropriate project.\nSwitching projects in a notebook\nTypically you want traces for an LLM app to all be grouped in one project. However, while working with Phoenix inside a notebook, we provide a utility to temporarily associate spans with different projects.  You can use this to trace things like evaluations.\nNotebook\nCopy\nfrom\n phoenix\n.\ntrace \nimport\n using_project\n\n\n\n\n# Switch project to run evals\n\n\nwith\n \nusing_project\n(\n""my-eval-project""\n):\n\n\n    \n# all spans created within this context will be associated with\n\n\n    \n# the ""my-eval-project"" project.\n\n\n    \n# Run evaluations here...\nAdding custom metadata to spans\nSpans produced by \nauto-instrumentation\n can get you very far. However at some point you may want to track \nmetadata\n - things like account or user info. \n\nLangChain\nDSPy\nWith LangChain, you can provide metadata directly via the chain or to to an invocation of a chain.\nCopy\n# Pass metadata into the chain\n\n\nllm \n=\n \nLLMChain\n(llm\n=\nOpenAI\n(), prompt\n=\nprompt, metadata\n=\n{\n""category""\n: \n""jokes""\n})\n\n\n\n\n# Pass metadata into the invocation\n\n\ncompletion \n=\n llm\n.\npredict\n(adjective\n=\n""funny""\n, metadata\n=\n{\n""variant""\n: \n""funny""\n})\n\n\nprint\n(completion)\nTo add metadata to a span, you will have to use OpenTelemetry\'s trace_api. \nCopy\nimport\n dspy\n\n\nfrom\n openinference\n.\nsemconv\n.\ntrace \nimport\n SpanAttributes\n\n\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\n\n\nclass\n \nQuestionClassifier\n(\ndspy\n.\nModule\n):\n\n\n    \ndef\n \n__init__\n(\nself\n):\n\n\n        \nsuper\n().\n__init__\n()\n\n\n        ...\n\n\n    \ndef\n \nforward\n(\nself\n,\n \nquestion\n:\n \nstr\n) \n->\n tuple\n[\nstr\n,\nstr\n]\n:\n\n\n        current_span \n=\n trace_api\n.\nget_current_span\n()\n\n\n        current_span\n.\nset_attribute\n(SpanAttributes.METADATA, \n""{ \'foo\': \'bar\' }""\n)\n\nsource: https://docs.arize.com/phoenix/setup/configuration\ntitle: Configuration\n\nConfiguration\nHow to configure Phoenix for your needs\nPorts\nPhoenix is an all-in-one solution that has a tracing UI as well as a trace collector over both HTTP and gRPC.\n\nBy default, the container exposes the following ports:\nPort\nProtocol\nEndpoint\nFunction\nEnv Var\n6006\nHTTP\n/\nUser interface (UI) of the web application.\nPHOENIX_PORT\n6006\nHTTP\n/v1/traces\nAccepts traces in \nOpenTelemetry OTLP format\u2009\n (Protobuf).\nPHOENIX_PORT\n4317\ngRPC\nn/a\nAccepts traces in \nOpenTelemetry OTLP format\u2009\n (Protobuf).\nPHOENIX_GRPC_PORT\nIf the above ports need to be modified, consult the \nEnvironment Variables\n section below.\nEnvironment Variables\nPhoenix uses environment variables to control how data is sent, received, and stored. Here is the comprehensive list:\nServer Configuration\nThe following environment variables will control how your phoenix server runs.\nPHOENIX_PORT:\n The port to run the phoenix web server. Defaults to 6006.\nPHOENIX_GRPC_PORT:\n The port to run the gRPC OTLP trace collector. Defaults to 4317.\nPHOENIX_HOST:\n The host to run the phoenix server. Defaults to 0.0.0.0\nPHOENIX_HOST_ROOT_PATH:\n The root path prefix for your application. If provided, allows Phoenix to run behind a reverse proxy at the specified subpath. See an example \nhere\n.\nPHOENIX_WORKING_DIR:\n The directory in which to save, load, and export data. This directory must be accessible by both the Phoenix server and the notebook environment. Defaults to \n~/.phoenix/\nPHOENIX_SQL_DATABASE_URL:\n The SQL database URL to use when logging traces and evals. if you plan on using SQLite, it\'s advised to to use a persistent volume and simply point the \nPHOENIX_WORKING_DIR\n to that volume. If URL is not specified, by default Phoenix starts with a file-based SQLite database in a temporary folder, the location of which will be shown at startup. Phoenix also supports PostgresSQL as shown below:\nPostgreSQL, e.g. \npostgresql://@host/dbname?user=user&password=password\n or \npostgresql://user:password@host/dbname\nSQLite, e.g. \nsqlite:///path/to/database.db\nPHOENIX_ENABLE_PROMETHEUS:\n Whether to enable Prometheus metrics at port 9090. Defaults to false.\nPHOENIX_SERVER_INSTRUMENTATION_OTLP_TRACE_COLLECTOR_HTTP_ENDPOINT:\n Specifies an HTTP endpoint for the OTLP trace collector. Specifying this variable enables the OpenTelemetry tracer and exporter for the Phoenix server.\nPHOENIX_SERVER_INSTRUMENTATION_OTLP_TRACE_COLLECTOR_GRPC_ENDPOINT:\n Specifies an gRPC endpoint for the OTLP trace collector. Specifying this variable enables the OpenTelemetry tracer and exporter for the Phoenix server.\nClient Configuration\nThe following environment variables will control your client or notebook environment.\nPHOENIX_NOTEBOOK_ENV:\n The notebook environment. Typically you do not need to set this but it can be set explicitly (e.x. \nsagemaker\n)\nPHOENIX_COLLECTOR_ENDPOINT:\n The endpoint traces and evals are sent to. This must be set if the Phoenix server is running on a remote instance. For example if phoenix is running at \nhttp://125.2.3.5:4040\n , this environment variable must be set where your LLM application is running and being traced. Note that the endpoint should not contain trailing slashes or slugs.\nPHOENIX_PROJECT_NAME:\n The project under which traces will be sent. See \nprojects\n.\nPHOENIX_CLIENT_HEADERS:\n The headers to set when talking to the phoenix server. This might be things like authentication headers for hosted phoenix.\nPrevious\nEnvironments\nNext\nDeployment\nLast updated \n14 minutes ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I configure projects?\nAnswer: '}, {'role': 'assistant', 'content': 'To configure projects, you can set the project name using the PHOENIX_PROJECT_NAME environment variable before adding instrumentation or running any code in a notebook environment. Additionally, if you are using Phoenix as a collector and running your application separately, you can set the project name in the Resource attributes for the trace provider as shown in the provided code snippet.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/customize-traces\ntitle: Customize Traces\n\nCustomize Traces\nOftentimes you want to customize various aspects of traces you log to Phoenix\nLog to a specific project\nPhoenix uses \nprojects\n to group traces. If left unspecified, all traces are sent to a default project.\nNotebook\nServer (Python)\nIn the notebook, you can set the \nPHOENIX_PROJECT_NAME\n environment variable \nbefore \nadding instrumentation or running any of your code.\nIn python this would look like:\nCopy\nimport\n os\n\n\n\n\nos\n.\nenviron\n[\n\'PHOENIX_PROJECT_NAME\'\n]\n \n=\n \n""<your-project-name>""\nNote that setting a project via an environment variable only works in a notebook and must be done \nBEFORE\n instrumentation is initialized. If you are using OpenInference Instrumentation, see the Server tab for how to set the project name in the Resource attributes.\nIf you are using Phoenix as a collector and running your application separately, you can set the project name in the \nResource\n attributes for the trace provider. \nCopy\nfrom\n openinference\n.\nsemconv\n.\nresource \nimport\n ResourceAttributes\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\nresources \nimport\n Resource\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\n\n\nresource \n=\n \nResource\n(attributes\n=\n{\n\n\n    ResourceAttributes.PROJECT_NAME: \n\'<your-project-name>\'\n\n\n})\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n(resource\n=\nresource)\n\n\nspan_exporter \n=\n \nOTLPSpanExporter\n(endpoint\n=\n""http://phoenix:6006/v1/traces""\n)\n\n\nspan_processor \n=\n \nSimpleSpanProcessor\n(span_exporter\n=\nspan_exporter)\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor\n=\nspan_processor)\n\n\ntrace_api\n.\nset_tracer_provider\n(tracer_provider\n=\ntracer_provider)\n\n\n# Add any auto-instrumentation you want \n\n\nLlamaIndexInstrumentor\n().\ninstrument\n()\nProjects work by setting something called the \nResource\n attributes (as seen in the Server example above). The phoenix server uses the project name attribute to group traces into the appropriate project.\nSwitching projects in a notebook\nTypically you want traces for an LLM app to all be grouped in one project. However, while working with Phoenix inside a notebook, we provide a utility to temporarily associate spans with different projects.  You can use this to trace things like evaluations.\nNotebook\nCopy\nfrom\n phoenix\n.\ntrace \nimport\n using_project\n\n\n\n\n# Switch project to run evals\n\n\nwith\n \nusing_project\n(\n""my-eval-project""\n):\n\n\n    \n# all spans created within this context will be associated with\n\n\n    \n# the ""my-eval-project"" project.\n\n\n    \n# Run evaluations here...\nAdding custom metadata to spans\nSpans produced by \nauto-instrumentation\n can get you very far. However at some point you may want to track \nmetadata\n - things like account or user info. \n\nLangChain\nDSPy\nWith LangChain, you can provide metadata directly via the chain or to to an invocation of a chain.\nCopy\n# Pass metadata into the chain\n\n\nllm \n=\n \nLLMChain\n(llm\n=\nOpenAI\n(), prompt\n=\nprompt, metadata\n=\n{\n""category""\n: \n""jokes""\n})\n\n\n\n\n# Pass metadata into the invocation\n\n\ncompletion \n=\n llm\n.\npredict\n(adjective\n=\n""funny""\n, metadata\n=\n{\n""variant""\n: \n""funny""\n})\n\n\nprint\n(completion)\nTo add metadata to a span, you will have to use OpenTelemetry\'s trace_api. \nCopy\nimport\n dspy\n\n\nfrom\n openinference\n.\nsemconv\n.\ntrace \nimport\n SpanAttributes\n\n\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\n\n\nclass\n \nQuestionClassifier\n(\ndspy\n.\nModule\n):\n\n\n    \ndef\n \n__init__\n(\nself\n):\n\n\n        \nsuper\n().\n__init__\n()\n\n\n        ...\n\n\n    \ndef\n \nforward\n(\nself\n,\n \nquestion\n:\n \nstr\n) \n->\n tuple\n[\nstr\n,\nstr\n]\n:\n\n\n        current_span \n=\n trace_api\n.\nget_current_span\n()\n\n\n        current_span\n.\nset_attribute\n(SpanAttributes.METADATA, \n""{ \'foo\': \'bar\' }""\n)\n\nsource: https://docs.arize.com/phoenix/setup/configuration\ntitle: Configuration\n\nConfiguration\nHow to configure Phoenix for your needs\nPorts\nPhoenix is an all-in-one solution that has a tracing UI as well as a trace collector over both HTTP and gRPC.\n\nBy default, the container exposes the following ports:\nPort\nProtocol\nEndpoint\nFunction\nEnv Var\n6006\nHTTP\n/\nUser interface (UI) of the web application.\nPHOENIX_PORT\n6006\nHTTP\n/v1/traces\nAccepts traces in \nOpenTelemetry OTLP format\u2009\n (Protobuf).\nPHOENIX_PORT\n4317\ngRPC\nn/a\nAccepts traces in \nOpenTelemetry OTLP format\u2009\n (Protobuf).\nPHOENIX_GRPC_PORT\nIf the above ports need to be modified, consult the \nEnvironment Variables\n section below.\nEnvironment Variables\nPhoenix uses environment variables to control how data is sent, received, and stored. Here is the comprehensive list:\nServer Configuration\nThe following environment variables will control how your phoenix server runs.\nPHOENIX_PORT:\n The port to run the phoenix web server. Defaults to 6006.\nPHOENIX_GRPC_PORT:\n The port to run the gRPC OTLP trace collector. Defaults to 4317.\nPHOENIX_HOST:\n The host to run the phoenix server. Defaults to 0.0.0.0\nPHOENIX_HOST_ROOT_PATH:\n The root path prefix for your application. If provided, allows Phoenix to run behind a reverse proxy at the specified subpath. See an example \nhere\n.\nPHOENIX_WORKING_DIR:\n The directory in which to save, load, and export data. This directory must be accessible by both the Phoenix server and the notebook environment. Defaults to \n~/.phoenix/\nPHOENIX_SQL_DATABASE_URL:\n The SQL database URL to use when logging traces and evals. if you plan on using SQLite, it\'s advised to to use a persistent volume and simply point the \nPHOENIX_WORKING_DIR\n to that volume. If URL is not specified, by default Phoenix starts with a file-based SQLite database in a temporary folder, the location of which will be shown at startup. Phoenix also supports PostgresSQL as shown below:\nPostgreSQL, e.g. \npostgresql://@host/dbname?user=user&password=password\n or \npostgresql://user:password@host/dbname\nSQLite, e.g. \nsqlite:///path/to/database.db\nPHOENIX_ENABLE_PROMETHEUS:\n Whether to enable Prometheus metrics at port 9090. Defaults to false.\nPHOENIX_SERVER_INSTRUMENTATION_OTLP_TRACE_COLLECTOR_HTTP_ENDPOINT:\n Specifies an HTTP endpoint for the OTLP trace collector. Specifying this variable enables the OpenTelemetry tracer and exporter for the Phoenix server.\nPHOENIX_SERVER_INSTRUMENTATION_OTLP_TRACE_COLLECTOR_GRPC_ENDPOINT:\n Specifies an gRPC endpoint for the OTLP trace collector. Specifying this variable enables the OpenTelemetry tracer and exporter for the Phoenix server.\nClient Configuration\nThe following environment variables will control your client or notebook environment.\nPHOENIX_NOTEBOOK_ENV:\n The notebook environment. Typically you do not need to set this but it can be set explicitly (e.x. \nsagemaker\n)\nPHOENIX_COLLECTOR_ENDPOINT:\n The endpoint traces and evals are sent to. This must be set if the Phoenix server is running on a remote instance. For example if phoenix is running at \nhttp://125.2.3.5:4040\n , this environment variable must be set where your LLM application is running and being traced. Note that the endpoint should not contain trailing slashes or slugs.\nPHOENIX_PROJECT_NAME:\n The project under which traces will be sent. See \nprojects\n.\nPHOENIX_CLIENT_HEADERS:\n The headers to set when talking to the phoenix server. This might be things like authentication headers for hosted phoenix.\nPrevious\nEnvironments\nNext\nDeployment\nLast updated \n14 minutes ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I configure projects?\nAnswer: '}, {'role': 'assistant', 'content': 'To configure projects, you can set the project name using the PHOENIX_PROJECT_NAME environment variable before adding instrumentation or running any code in a notebook environment. Additionally, if you are using Phoenix as a collector and running your application separately, you can set the project name in the Resource attributes for the trace provider as shown in the provided code snippet.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NDU=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/how-does-tracing-work\ntitle: How Tracing Works\n\nHow Tracing Works\nThe components behind tracing\nThe phoenix server is collector of traces over OTLP\nInstrumentation\nIn order for an application to emit traces for analysis, the application must be \ninstrumented\n.  Your application can be \nmanually\n instrumented or be \nautomatically\n instrumented.\n\nWith phoenix, there a set of plugins (\ninstrumentors\n) that can be added to your application's startup process that perform auto-instrumentation. These plugins collect spans for your application and export them for collection and visualization. For phoenix, all the instrumentors are managed via a single repository called \nOpenInference\n. The comprehensive list of instrumentors can be found in the \nhow-to guide.\nExporter\nAn exporter takes the spans created via \ninstrumentation \nand exports them to a \ncollector\n. In simple terms, it just sends the data to the Phoenix. When using Phoenix, most of this is completely done under the hood when you call instrument on an instrumentor.\nCollector\nThe Phoenix server is a collector and a UI that helps you troubleshoot your application in real time. When you run or run phoenix (e.x. \npx.launch_app()\n, container), Phoenix starts receiving spans form any application(s) that is exporting spans to it.\nOpenTelememetry Protocol\nOpenTelemetetry Protocol (or OTLP for short) is the means by which traces arrive from your application to the Phoenix collector. Phoenix currently supports OTLP over HTTP.\nPrevious\nTraces\nNext\nAnnotating Traces\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/tracing/llm-traces\ntitle: Overview: Tracing\n\nOverview: Tracing\nTracing the execution of LLM applications using Telemetry\nLLM tracing records the paths taken by requests as they propagate through multiple steps or components of an LLM application. For example, when a user interacts with an LLM application, tracing can capture the sequence of operations, such as document retrieval, embedding generation, language model invocation, and response generation to provide a detailed timeline of the request's execution.\nTracing is a helpful tool for understanding how your LLM application works. Phoenix offers comprehensive tracing capabilities that are not tied to any specific LLM vendor or framework. Phoenix accepts traces over the OpenTelemetry protocol (OTLP) and supports first-class instrumentation for a variety of frameworks ( \nLlamaIndex\n, \nLangChain\n,\n DSPy\n),  SDKs (\nOpenAI\n, \nBedrock\n, \nMistral\n, \nVertex\n), and Languages. (Python, Javascript, etc.)\nUsing Phoenix's tracing capabilities can provide important insights into the inner workings of your LLM application. By analyzing the collected trace data, you can identify and address various performance and operational issues and improve the overall reliability and efficiency of your system.\nApplication Latency\n: Identify and address slow invocations of LLMs, Retrievers, and other components within your application, enabling you to optimize performance and responsiveness.\nToken Usage\n: Gain a detailed breakdown of token usage for your LLM calls, allowing you to identify and optimize the most expensive LLM invocations.\nRuntime Exceptions\n: Capture and inspect critical runtime exceptions, such as rate-limiting events, that can help you proactively address and mitigate potential issues.\nRetrieved Documents\n: Inspect the documents retrieved during a Retriever call, including the score and order in which they were returned to provide insight into the retrieval process.\nEmbeddings\n: Examine the embedding text used for retrieval and the underlying embedding model to allow you to validate and refine your embedding strategies.\nLLM Parameters\n: Inspect the parameters used when calling an LLM, such as temperature and system prompts, to ensure optimal configuration and debugging.\nPrompt Templates\n: Understand the prompt templates used during the prompting step and the variables that were applied, allowing you to fine-tune and improve your prompting strategies.\nTool Descriptions\n: View the descriptions and function signatures of the tools your LLM has been given access to in order to better understand and control your LLM’s capabilities.\nLLM Function Calls\n: For LLMs with function call capabilities (e.g., OpenAI), you can inspect the function selection and function messages in the input to the LLM, further improving your ability to debug and optimize your application.\nBy using tracing in Phoenix, you can gain increased visibility into your LLM application, empowering you to identify and address performance bottlenecks, optimize resource utilization, and ensure the overall reliability and effectiveness of your system.\nView the inner workings for your LLM Application\nTo get started, check out the \nQuickstart guide\nAfter that, read through the \nConcepts Section\n to get and understanding of the different components.\nIf you want to learn how to accomplish a particular task, check out the \nHow-To Guides.\n\n\nPrevious\nFAQs: Deployment\nNext\nQuickstart: Tracing\nLast updated \n6 hours ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How does tracing work?\nAnswer: ""}, {'role': 'assistant', 'content': 'Tracing works by instrumenting an application to emit traces for analysis. This can be done manually or automatically through the use of instrumentors. These instrumentors collect spans for the application and export them to a collector for visualization. The spans are then exported by an exporter to a collector, such as the Phoenix server, which receives the spans and helps troubleshoot the application in real time. The traces arrive at the Phoenix collector via the OpenTelemetry Protocol (OTLP) over HTTP.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/how-does-tracing-work\ntitle: How Tracing Works\n\nHow Tracing Works\nThe components behind tracing\nThe phoenix server is collector of traces over OTLP\nInstrumentation\nIn order for an application to emit traces for analysis, the application must be \ninstrumented\n.  Your application can be \nmanually\n instrumented or be \nautomatically\n instrumented.\n\nWith phoenix, there a set of plugins (\ninstrumentors\n) that can be added to your application's startup process that perform auto-instrumentation. These plugins collect spans for your application and export them for collection and visualization. For phoenix, all the instrumentors are managed via a single repository called \nOpenInference\n. The comprehensive list of instrumentors can be found in the \nhow-to guide.\nExporter\nAn exporter takes the spans created via \ninstrumentation \nand exports them to a \ncollector\n. In simple terms, it just sends the data to the Phoenix. When using Phoenix, most of this is completely done under the hood when you call instrument on an instrumentor.\nCollector\nThe Phoenix server is a collector and a UI that helps you troubleshoot your application in real time. When you run or run phoenix (e.x. \npx.launch_app()\n, container), Phoenix starts receiving spans form any application(s) that is exporting spans to it.\nOpenTelememetry Protocol\nOpenTelemetetry Protocol (or OTLP for short) is the means by which traces arrive from your application to the Phoenix collector. Phoenix currently supports OTLP over HTTP.\nPrevious\nTraces\nNext\nAnnotating Traces\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/tracing/llm-traces\ntitle: Overview: Tracing\n\nOverview: Tracing\nTracing the execution of LLM applications using Telemetry\nLLM tracing records the paths taken by requests as they propagate through multiple steps or components of an LLM application. For example, when a user interacts with an LLM application, tracing can capture the sequence of operations, such as document retrieval, embedding generation, language model invocation, and response generation to provide a detailed timeline of the request's execution.\nTracing is a helpful tool for understanding how your LLM application works. Phoenix offers comprehensive tracing capabilities that are not tied to any specific LLM vendor or framework. Phoenix accepts traces over the OpenTelemetry protocol (OTLP) and supports first-class instrumentation for a variety of frameworks ( \nLlamaIndex\n, \nLangChain\n,\n DSPy\n),  SDKs (\nOpenAI\n, \nBedrock\n, \nMistral\n, \nVertex\n), and Languages. (Python, Javascript, etc.)\nUsing Phoenix's tracing capabilities can provide important insights into the inner workings of your LLM application. By analyzing the collected trace data, you can identify and address various performance and operational issues and improve the overall reliability and efficiency of your system.\nApplication Latency\n: Identify and address slow invocations of LLMs, Retrievers, and other components within your application, enabling you to optimize performance and responsiveness.\nToken Usage\n: Gain a detailed breakdown of token usage for your LLM calls, allowing you to identify and optimize the most expensive LLM invocations.\nRuntime Exceptions\n: Capture and inspect critical runtime exceptions, such as rate-limiting events, that can help you proactively address and mitigate potential issues.\nRetrieved Documents\n: Inspect the documents retrieved during a Retriever call, including the score and order in which they were returned to provide insight into the retrieval process.\nEmbeddings\n: Examine the embedding text used for retrieval and the underlying embedding model to allow you to validate and refine your embedding strategies.\nLLM Parameters\n: Inspect the parameters used when calling an LLM, such as temperature and system prompts, to ensure optimal configuration and debugging.\nPrompt Templates\n: Understand the prompt templates used during the prompting step and the variables that were applied, allowing you to fine-tune and improve your prompting strategies.\nTool Descriptions\n: View the descriptions and function signatures of the tools your LLM has been given access to in order to better understand and control your LLM’s capabilities.\nLLM Function Calls\n: For LLMs with function call capabilities (e.g., OpenAI), you can inspect the function selection and function messages in the input to the LLM, further improving your ability to debug and optimize your application.\nBy using tracing in Phoenix, you can gain increased visibility into your LLM application, empowering you to identify and address performance bottlenecks, optimize resource utilization, and ensure the overall reliability and effectiveness of your system.\nView the inner workings for your LLM Application\nTo get started, check out the \nQuickstart guide\nAfter that, read through the \nConcepts Section\n to get and understanding of the different components.\nIf you want to learn how to accomplish a particular task, check out the \nHow-To Guides.\n\n\nPrevious\nFAQs: Deployment\nNext\nQuickstart: Tracing\nLast updated \n6 hours ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How does tracing work?\nAnswer: ""}, {'role': 'assistant', 'content': 'Tracing works by instrumenting an application to emit traces for analysis. This can be done manually or automatically through the use of instrumentors. These instrumentors collect spans for the application and export them to a collector for visualization. The spans are then exported by an exporter to a collector, such as the Phoenix server, which receives the spans and helps troubleshoot the application in real time. The traces arrive at the Phoenix collector via the OpenTelemetry Protocol (OTLP) over HTTP.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NDY=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/instrumentation\ntitle: Auto Instrumentation\n\nAuto Instrumentation\nHow OpenInference facilitates automatic instrumentation of applications.\nIn order to make a system observable, it must be \ninstrumented\n: That is, code from the system’s components must emit traces.\nPhoenix natively supports collecting traces generated via OpenInference automatic instrumentation. Phoenix has pre-built auto-instrumentors for \nPython\n and \nTypeScript / JavaScript\n.\n\n\nPython\nLlamaIndex, LangChain, DSPy, OpenAI, Bedrock, Mistral and more\nTypeScript / JavaScript\nOpenAI, LangChainjs\nPrevious\nInstrumenting Prompt Templates and Prompt Variables\nNext\nAuto Instrument: Python\nLast updated \n4 months ago\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/instrumentation/auto-instrument-python\ntitle: Auto Instrument: Python\n\nAuto Instrument: Python\nAutomatically collect traces from frameworks and libraries.\nPhoenix natively supports collecting traces generated via OpenInference automatic instrumentation. The supported instrumentations are:\nPython\nlibrary\nInstrumentation\nVersion\nLlamaIndex\nopeninference-instrumentation-llama-index\nLangChain\nopeninference-instrumentation-langchain\nOpenAI\nopeninference-instrumentation-openai\nMistralAI\nopeninference-instrumentation-mistralai\nVertexAI\nopeninference-instrumentation-vertexai\nDSPy\nopeninference-instrumentation-dspy\nAWS Bedrock\nopeninference-instrumentation-bedrock\nPrevious\nAuto Instrumentation\nNext\nAuto Instrument: TS\nLast updated \n1 month ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is auto-instrumentation?\nAnswer: '}, {'role': 'assistant', 'content': 'Auto-instrumentation is the process of automatically collecting traces from frameworks and libraries within a system to make it observable. This involves emitting traces from the components of the system to enable monitoring and analysis.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/instrumentation\ntitle: Auto Instrumentation\n\nAuto Instrumentation\nHow OpenInference facilitates automatic instrumentation of applications.\nIn order to make a system observable, it must be \ninstrumented\n: That is, code from the system’s components must emit traces.\nPhoenix natively supports collecting traces generated via OpenInference automatic instrumentation. Phoenix has pre-built auto-instrumentors for \nPython\n and \nTypeScript / JavaScript\n.\n\n\nPython\nLlamaIndex, LangChain, DSPy, OpenAI, Bedrock, Mistral and more\nTypeScript / JavaScript\nOpenAI, LangChainjs\nPrevious\nInstrumenting Prompt Templates and Prompt Variables\nNext\nAuto Instrument: Python\nLast updated \n4 months ago\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/instrumentation/auto-instrument-python\ntitle: Auto Instrument: Python\n\nAuto Instrument: Python\nAutomatically collect traces from frameworks and libraries.\nPhoenix natively supports collecting traces generated via OpenInference automatic instrumentation. The supported instrumentations are:\nPython\nlibrary\nInstrumentation\nVersion\nLlamaIndex\nopeninference-instrumentation-llama-index\nLangChain\nopeninference-instrumentation-langchain\nOpenAI\nopeninference-instrumentation-openai\nMistralAI\nopeninference-instrumentation-mistralai\nVertexAI\nopeninference-instrumentation-vertexai\nDSPy\nopeninference-instrumentation-dspy\nAWS Bedrock\nopeninference-instrumentation-bedrock\nPrevious\nAuto Instrumentation\nNext\nAuto Instrument: TS\nLast updated \n1 month ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is auto-instrumentation?\nAnswer: '}, {'role': 'assistant', 'content': 'Auto-instrumentation is the process of automatically collecting traces from frameworks and libraries within a system to make it observable. This involves emitting traces from the components of the system to enable monitoring and analysis.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NDc=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/manual-instrumentation\ntitle: Manual Instrumentation\n\n""span_id""\n:\n \n""0x03f3466720f4bfc7""\n,\n\n\n         \n""trace_state""\n:\n \n""[]""\n\n\n     }\n,\n\n\n     \n""kind""\n:\n \n""SpanKind.INTERNAL""\n,\n\n\n     \n""parent_id""\n:\n \n""0x7eb5df0046c77cd2""\n,\n\n\n     \n""start_time""\n:\n \n""2024-05-08T21:46:11.044464Z""\n,\n\n\n     \n""end_time""\n:\n \n""2024-05-08T21:46:11.465803Z""\n,\n\n\n     \n""status""\n:\n {\n\n\n         \n""status_code""\n:\n \n""OK""\n\n\n     }\n,\n\n\n     \n""attributes""\n:\n {\n\n\n         \n""openinference.span.kind""\n:\n \n""RETRIEVER""\n,\n\n\n         \n""input.value""\n:\n \n""tell me about postal service""\n,\n\n\n         \n""retrieval.documents.0.document.id""\n:\n \n""6d4e27be-1d6d-4084-a619-351a44834f38""\n,\n\n\n         \n""retrieval.documents.0.document.score""\n:\n \n0.7711453293100421\n,\n\n\n         \n""retrieval.documents.0.document.content""\n:\n \n""<document-chunk-1>""\n,\n       \n\n\n         ""retrieval.documents.0.document.metadata"": ""{\\""page_label\\"": \\""7\\"", \\""file_name\\"": \\""/data/101.pdf\\"", \\""file_path\\"": \\""/data/101.pdf\\"", \\""file_type\\"": \\""application/pdf\\"", \\""file_size\\"": 47931, \\""creation_date\\"": \\""2024-04-12\\"", \\""last_modified_date\\"": \\""2024-04-12\\""}"",\n\n\n         \n""retrieval.documents.1.document.id""\n:\n \n""869d9f6d-db9a-43c4-842f-74bd8d505147""\n,\n\n\n         \n""retrieval.documents.1.document.score""\n:\n \n0.7672439175862021\n,\n\n\n         \n""retrieval.documents.1.document.content""\n:\n \n""<document-chunk-2>""\n,\n\n\n         ""retrieval.documents.1.document.metadata"": ""{\\""page_label\\"": \\""6\\"", \\""file_name\\"": \\""/data/101.pdf\\"", \\""file_path\\"": \\""/data/101.pdf\\"", \\""file_type\\"": \\""application/pdf\\"", \\""file_size\\"": 47931, \\""creation_date\\"": \\""2024-04-12\\"", \\""last_modified_date\\"": \\""2024-04-12\\""}"",\n\n\n         \n""retrieval.documents.2.document.id""\n:\n \n""72b5cb6b-464f-4460-b497-cc7c09d1dbef""\n,\n\n\n         \n""retrieval.documents.2.document.score""\n:\n \n0.7647611816897794\n,\n\n\n         \n""retrieval.documents.2.document.content""\n:\n \n""<document-chunk-3>""\n,\n\n\n         ""retrieval.documents.2.document.metadata"": ""{\\""page_label\\"": \\""4\\"", \\""file_name\\"": \\""/data/101.pdf\\"", \\""file_path\\"": \\""/data/101.pdf\\"", \\""file_type\\"": \\""application/pdf\\"", \\""file_size\\"": 47931, \\""creation_date\\"": \\""2024-04-12\\"", \\""last_modified_date\\"": \\""2024-04-12\\""}""\n\n\n     }\n,\n\n\n     \n""events""\n:\n []\n,\n\n\n     \n""links""\n:\n []\n,\n\n\n     \n""resource""\n:\n {\n\n\n         \n""attributes""\n:\n {}\n,\n\n\n         \n""schema_url""\n:\n \n""""\n\n\n     }\n\n\n }\nPrevious\nAuto Instrument: TS\nNext\nInstrument: Python\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/manual-instrumentation\ntitle: Manual Instrumentation\n\nThe reply half may be formatted for response purposes (e.g., contain blocks for completion by the addressee).\\nb. A double card must be folded before mailing and prepared so that the \\naddress on the reply half is on the inside when the double card is originally \\nmailed. The address side of the reply half may be prepared as Business \\nReply Mail, Courtesy Reply Mail, meter reply mail, or as a USPS Returns service label.\\nc. Plain stickers, seals, or a single wire stitch (staple) may be used to fasten the \\nopen edge at the top or bottom once the card is folded if affixed so that the \\ninner surfaces of the cards can be readily examined. Fasteners must be \\naffixed according to the applicable preparation requirements for the price claimed. Any sealing on the left and right sides of the cards, no matter the \\nsealing process used, is not permitted.\\nd. The first half of a double card must be detached when the reply half is \\nmailed for return. \\n6.2.10   Enclosures\\nEnclosures in double postcards are prohibited at card prices. \\n6.3 Nonmachinable Pieces\\n6.3.1   Nonmachinable Letters\\nLetter-size pieces (except card-size pieces) that meet one or more of the \\nnonmachinable characteristics in 1.2 are subject to the nonmachinable \\nsurcharge (see 133.1.7 ). \\n6.3.2   Nonmachinable Flats\\nFlat-size pieces that do not meet the standards in 2.0 are considered parcels, \\nand the mailer must pay the applicable parcel price.  \\n6.4 Parcels \\n[7-9-23]  USPS Ground Advantage \\u2014 Retail parcels are eligible for USPS \\nTracking and Signature Confirmation service. A USPS Ground Advantage \\u2014 \\nRetail parcel is the following:\\na. A mailpiece that exceeds any one of the maximum dimensions for a flat \\n(large envelope). See 2.1.\\nb. A flat-size mailpiece, regardless of thickness, that is rigid or nonrectangular. \\nc. A flat-size mailpiece that is not uniformly thick under 2.4. \\nd.[7-9-23]  A mailpiece that does not exceed 130 inches in combined length \\nand girth.\\n7.0 Additional Physical Standards for Media Mail and Library \\nMail\\nThese standards apply to Media Mail and Library Mail:\\n\\npage_label: 4\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.1Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\n4.0 Additional Physical Standa rds for Priority Mail Express\\nEach piece of Priority Mail Express may not weigh more than 70 pounds. The \\ncombined length and girth of a piece (the length of its longest side plus the \\ndistance around its thickest part) may not exceed 108 inches. Lower size or weight standards apply to Priority Mail Express addressed to certain APO/FPO \\nand DPOs. Priority Mail Express items must be large enough to hold the required \\nmailing labels and indicia on a single optical plane without bending or folding.\\n5.0 Additional Physical St andards for Priority Mail\\nThe maximum weight is 70 pounds. The combined length and girth of a piece \\n(the length of its longest side plus the distance around its thickest part) may not \\nexceed 108 inches. Lower size and weight standards apply for some APO/FPO \\nand DPO mail subject to 703.2.0 , and 703.4.0 , and for Department of State mail \\nsubject to 703.3.0 . \\n[7-9-23] \\n6.0 Additional Physical Standa rds for First-Class Mail and \\nUSPS Ground Advantage \\u2014 Retail\\n[7-9-23]\\n6.1 Maximum Weight\\n6.1.1   First-Class Mail\\nFirst-Class Mail (letters and flats) must not exceed 13 ounces.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is manual-instrumentation?\nAnswer: '}, {'role': 'assistant', 'content': 'Manual instrumentation refers to the process of directly adding code to an application to collect specific tracing data for monitoring and analysis purposes. This involves manually inserting code snippets or functions into the application to track and record information such as span IDs, start and end times, status codes, attributes, and other relevant data points during the execution of the application.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/manual-instrumentation\ntitle: Manual Instrumentation\n\n""span_id""\n:\n \n""0x03f3466720f4bfc7""\n,\n\n\n         \n""trace_state""\n:\n \n""[]""\n\n\n     }\n,\n\n\n     \n""kind""\n:\n \n""SpanKind.INTERNAL""\n,\n\n\n     \n""parent_id""\n:\n \n""0x7eb5df0046c77cd2""\n,\n\n\n     \n""start_time""\n:\n \n""2024-05-08T21:46:11.044464Z""\n,\n\n\n     \n""end_time""\n:\n \n""2024-05-08T21:46:11.465803Z""\n,\n\n\n     \n""status""\n:\n {\n\n\n         \n""status_code""\n:\n \n""OK""\n\n\n     }\n,\n\n\n     \n""attributes""\n:\n {\n\n\n         \n""openinference.span.kind""\n:\n \n""RETRIEVER""\n,\n\n\n         \n""input.value""\n:\n \n""tell me about postal service""\n,\n\n\n         \n""retrieval.documents.0.document.id""\n:\n \n""6d4e27be-1d6d-4084-a619-351a44834f38""\n,\n\n\n         \n""retrieval.documents.0.document.score""\n:\n \n0.7711453293100421\n,\n\n\n         \n""retrieval.documents.0.document.content""\n:\n \n""<document-chunk-1>""\n,\n       \n\n\n         ""retrieval.documents.0.document.metadata"": ""{\\""page_label\\"": \\""7\\"", \\""file_name\\"": \\""/data/101.pdf\\"", \\""file_path\\"": \\""/data/101.pdf\\"", \\""file_type\\"": \\""application/pdf\\"", \\""file_size\\"": 47931, \\""creation_date\\"": \\""2024-04-12\\"", \\""last_modified_date\\"": \\""2024-04-12\\""}"",\n\n\n         \n""retrieval.documents.1.document.id""\n:\n \n""869d9f6d-db9a-43c4-842f-74bd8d505147""\n,\n\n\n         \n""retrieval.documents.1.document.score""\n:\n \n0.7672439175862021\n,\n\n\n         \n""retrieval.documents.1.document.content""\n:\n \n""<document-chunk-2>""\n,\n\n\n         ""retrieval.documents.1.document.metadata"": ""{\\""page_label\\"": \\""6\\"", \\""file_name\\"": \\""/data/101.pdf\\"", \\""file_path\\"": \\""/data/101.pdf\\"", \\""file_type\\"": \\""application/pdf\\"", \\""file_size\\"": 47931, \\""creation_date\\"": \\""2024-04-12\\"", \\""last_modified_date\\"": \\""2024-04-12\\""}"",\n\n\n         \n""retrieval.documents.2.document.id""\n:\n \n""72b5cb6b-464f-4460-b497-cc7c09d1dbef""\n,\n\n\n         \n""retrieval.documents.2.document.score""\n:\n \n0.7647611816897794\n,\n\n\n         \n""retrieval.documents.2.document.content""\n:\n \n""<document-chunk-3>""\n,\n\n\n         ""retrieval.documents.2.document.metadata"": ""{\\""page_label\\"": \\""4\\"", \\""file_name\\"": \\""/data/101.pdf\\"", \\""file_path\\"": \\""/data/101.pdf\\"", \\""file_type\\"": \\""application/pdf\\"", \\""file_size\\"": 47931, \\""creation_date\\"": \\""2024-04-12\\"", \\""last_modified_date\\"": \\""2024-04-12\\""}""\n\n\n     }\n,\n\n\n     \n""events""\n:\n []\n,\n\n\n     \n""links""\n:\n []\n,\n\n\n     \n""resource""\n:\n {\n\n\n         \n""attributes""\n:\n {}\n,\n\n\n         \n""schema_url""\n:\n \n""""\n\n\n     }\n\n\n }\nPrevious\nAuto Instrument: TS\nNext\nInstrument: Python\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/manual-instrumentation\ntitle: Manual Instrumentation\n\nThe reply half may be formatted for response purposes (e.g., contain blocks for completion by the addressee).\\nb. A double card must be folded before mailing and prepared so that the \\naddress on the reply half is on the inside when the double card is originally \\nmailed. The address side of the reply half may be prepared as Business \\nReply Mail, Courtesy Reply Mail, meter reply mail, or as a USPS Returns service label.\\nc. Plain stickers, seals, or a single wire stitch (staple) may be used to fasten the \\nopen edge at the top or bottom once the card is folded if affixed so that the \\ninner surfaces of the cards can be readily examined. Fasteners must be \\naffixed according to the applicable preparation requirements for the price claimed. Any sealing on the left and right sides of the cards, no matter the \\nsealing process used, is not permitted.\\nd. The first half of a double card must be detached when the reply half is \\nmailed for return. \\n6.2.10   Enclosures\\nEnclosures in double postcards are prohibited at card prices. \\n6.3 Nonmachinable Pieces\\n6.3.1   Nonmachinable Letters\\nLetter-size pieces (except card-size pieces) that meet one or more of the \\nnonmachinable characteristics in 1.2 are subject to the nonmachinable \\nsurcharge (see 133.1.7 ). \\n6.3.2   Nonmachinable Flats\\nFlat-size pieces that do not meet the standards in 2.0 are considered parcels, \\nand the mailer must pay the applicable parcel price.  \\n6.4 Parcels \\n[7-9-23]  USPS Ground Advantage \\u2014 Retail parcels are eligible for USPS \\nTracking and Signature Confirmation service. A USPS Ground Advantage \\u2014 \\nRetail parcel is the following:\\na. A mailpiece that exceeds any one of the maximum dimensions for a flat \\n(large envelope). See 2.1.\\nb. A flat-size mailpiece, regardless of thickness, that is rigid or nonrectangular. \\nc. A flat-size mailpiece that is not uniformly thick under 2.4. \\nd.[7-9-23]  A mailpiece that does not exceed 130 inches in combined length \\nand girth.\\n7.0 Additional Physical Standards for Media Mail and Library \\nMail\\nThese standards apply to Media Mail and Library Mail:\\n\\npage_label: 4\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.1Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\n4.0 Additional Physical Standa rds for Priority Mail Express\\nEach piece of Priority Mail Express may not weigh more than 70 pounds. The \\ncombined length and girth of a piece (the length of its longest side plus the \\ndistance around its thickest part) may not exceed 108 inches. Lower size or weight standards apply to Priority Mail Express addressed to certain APO/FPO \\nand DPOs. Priority Mail Express items must be large enough to hold the required \\nmailing labels and indicia on a single optical plane without bending or folding.\\n5.0 Additional Physical St andards for Priority Mail\\nThe maximum weight is 70 pounds. The combined length and girth of a piece \\n(the length of its longest side plus the distance around its thickest part) may not \\nexceed 108 inches. Lower size and weight standards apply for some APO/FPO \\nand DPO mail subject to 703.2.0 , and 703.4.0 , and for Department of State mail \\nsubject to 703.3.0 . \\n[7-9-23] \\n6.0 Additional Physical Standa rds for First-Class Mail and \\nUSPS Ground Advantage \\u2014 Retail\\n[7-9-23]\\n6.1 Maximum Weight\\n6.1.1   First-Class Mail\\nFirst-Class Mail (letters and flats) must not exceed 13 ounces.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is manual-instrumentation?\nAnswer: '}, {'role': 'assistant', 'content': 'Manual instrumentation refers to the process of directly adding code to an application to collect specific tracing data for monitoring and analysis purposes. This involves manually inserting code snippets or functions into the application to track and record information such as span IDs, start and end times, status codes, attributes, and other relevant data points during the execution of the application.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NDg=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/reference/open-inference\ntitle: OpenInference\n\nOpenInference\nOpenInference is an open standard that encompasses model inference and LLM application tracing.\nFor a in-depth specification of the OpenInference specification, please consult the spec \nhttps://github.com/Arize-ai/openinference\nOpenInference is a set of specifications for model inferences and LLM traces\nOpenInference is a specification that encompass two data models:\nTracing\ncapture the execution of an application that results in invocations of an LLM.\n\nInferences\ndesigned to capture inference logs from a variety of model types and use-cases\nPrevious\nFrequently Asked Questions\nNext\nContribute to Phoenix\nLast updated \n1 month ago\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/instrumentation\ntitle: Auto Instrumentation\n\nAuto Instrumentation\nHow OpenInference facilitates automatic instrumentation of applications.\nIn order to make a system observable, it must be \ninstrumented\n: That is, code from the system’s components must emit traces.\nPhoenix natively supports collecting traces generated via OpenInference automatic instrumentation. Phoenix has pre-built auto-instrumentors for \nPython\n and \nTypeScript / JavaScript\n.\n\n\nPython\nLlamaIndex, LangChain, DSPy, OpenAI, Bedrock, Mistral and more\nTypeScript / JavaScript\nOpenAI, LangChainjs\nPrevious\nInstrumenting Prompt Templates and Prompt Variables\nNext\nAuto Instrument: Python\nLast updated \n4 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Can you tell me about openinference?\nAnswer: '}, {'role': 'assistant', 'content': ""OpenInference is an open standard that covers model inference and LLM application tracing. It includes specifications for capturing the execution of an application resulting in invocations of an LLM, as well as capturing inference logs from various model types and use-cases. Additionally, OpenInference facilitates automatic instrumentation of applications to make the system observable by emitting traces from the system's components.""}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/reference/open-inference\ntitle: OpenInference\n\nOpenInference\nOpenInference is an open standard that encompasses model inference and LLM application tracing.\nFor a in-depth specification of the OpenInference specification, please consult the spec \nhttps://github.com/Arize-ai/openinference\nOpenInference is a set of specifications for model inferences and LLM traces\nOpenInference is a specification that encompass two data models:\nTracing\ncapture the execution of an application that results in invocations of an LLM.\n\nInferences\ndesigned to capture inference logs from a variety of model types and use-cases\nPrevious\nFrequently Asked Questions\nNext\nContribute to Phoenix\nLast updated \n1 month ago\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/instrumentation\ntitle: Auto Instrumentation\n\nAuto Instrumentation\nHow OpenInference facilitates automatic instrumentation of applications.\nIn order to make a system observable, it must be \ninstrumented\n: That is, code from the system’s components must emit traces.\nPhoenix natively supports collecting traces generated via OpenInference automatic instrumentation. Phoenix has pre-built auto-instrumentors for \nPython\n and \nTypeScript / JavaScript\n.\n\n\nPython\nLlamaIndex, LangChain, DSPy, OpenAI, Bedrock, Mistral and more\nTypeScript / JavaScript\nOpenAI, LangChainjs\nPrevious\nInstrumenting Prompt Templates and Prompt Variables\nNext\nAuto Instrument: Python\nLast updated \n4 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Can you tell me about openinference?\nAnswer: '}, {'role': 'assistant', 'content': ""OpenInference is an open standard that covers model inference and LLM application tracing. It includes specifications for capturing the execution of an application resulting in invocations of an LLM, as well as capturing inference logs from various model types and use-cases. Additionally, OpenInference facilitates automatic instrumentation of applications to make the system observable by emitting traces from the system's components.""}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NDk=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/manual-instrumentation/javascript\ntitle: Instrument: TS\n\nInstrument: TS\nWhile Phoenix is heavily a Python-based Observability and Evaluation framework, it supports other languages like TypeScript / JavaScript\nYou can trace your NodeJS application over OpenTelemetry\nPhoenix is written and maintained in Python to make it natively runnable in Python notebooks. However, it can be stood up as a trace collector so that your LLM traces from your NodeJS application (e.g., LlamaIndex.TS, Langchain.js) can be collected. The traces collected by Phoenix can then be downloaded to a Jupyter notebook and used to run evaluations (e.g., \nLLM Evals\n, Ragas).\nGetting Started\nInstrumentation\n is the act of adding observability code to an app yourself.\nIf you’re instrumenting an app, you need to use the OpenTelemetry SDK for your language. You’ll then use the SDK to initialize OpenTelemetry and the API to instrument your code. This will emit telemetry from your app, and any library you installed that also comes with instrumentation.\nPhoenix natively supports automatic instrumentation provided by OpenInference. For more details on OpenInfernce, checkout the \nproject\ninstrumentation setup\nDependencies\nInstall OpenTelemetry API packages:\nCopy\nnpm\n \ninstall\n \n@opentelemetry/api\n \n@opentelemetry/resources\n \n@opentelemetry/semantic-conventions\nInstall OpenInference instrumentation packages. Below is an example of adding instrumentation for OpenAI as well as the semantic conventions for OpenInference.\nCopy\nnpm\n \ninstall\n \n@arizeai/openinference-instrumentation-openai\n \n@arizeai/openinference-semantic-conventions\nInitialize the SDK\nIf you instrument a Node.js application install the \nOpenTelemetry SDK for Node.js\n:\nCopy\nnpm\n \ninstall\n \n@opentelemetry/sdk-node\nBefore any other module in your application is loaded, you must initialize the SDK. If you fail to initialize the SDK or initialize it too late, no-op implementations will be provided to any library that acquires a tracer or meter from the API.\nCopy\n/*instrumentation.ts*/\n\n\nimport\n { NodeSDK } \nfrom\n \n'@opentelemetry/sdk-node'\n;\n\n\nimport\n { ConsoleSpanExporter } \nfrom\n \n'@opentelemetry/sdk-trace-node'\n;\n\n\nimport\n { Resource } \nfrom\n \n'@opentelemetry/resources'\n;\n\n\nimport\n { SemanticResourceAttributes } \nfrom\n \n'@opentelemetry/semantic-conventions'\n;\n\n\n\n\nconst\n \nsdk\n \n=\n \nnew\n \nNodeSDK\n({\n\n\n  resource\n:\n \nnew\n \nResource\n({\n\n\n    [\nSemanticResourceAttributes\n.\nSERVICE_NAME\n]\n:\n \n'yourServiceName'\n,\n\n\n    [\nSemanticResourceAttributes\n.\nSERVICE_VERSION\n]\n:\n \n'1.0'\n,\n\n\n  })\n,\n\n\n  traceExporter\n:\n \nnew\n \nConsoleSpanExporter\n()\n,\n\n\n});\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/setup-tracing/setup-tracing-ts\ntitle: Setup Tracing: TS\n\nSetup Tracing: TS\nSetup OpenTelemetry for TypeScript and Javascript\nSetting up OpenTelemetry for TypeScript and Javascript depends on your deployment and server-side runtime. \nNodeJS\nVercel\nPrevious\nUsing OTEL Python Directly\nNext\nTracing Core Concepts\nLast updated \n8 hours ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is open telemetry?\nAnswer: ""}, {'role': 'assistant', 'content': 'OpenTelemetry is a tool that allows you to trace your NodeJS application for observability purposes. It enables you to add observability code to your application and emit telemetry data, which can be collected and used for evaluations.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/manual-instrumentation/javascript\ntitle: Instrument: TS\n\nInstrument: TS\nWhile Phoenix is heavily a Python-based Observability and Evaluation framework, it supports other languages like TypeScript / JavaScript\nYou can trace your NodeJS application over OpenTelemetry\nPhoenix is written and maintained in Python to make it natively runnable in Python notebooks. However, it can be stood up as a trace collector so that your LLM traces from your NodeJS application (e.g., LlamaIndex.TS, Langchain.js) can be collected. The traces collected by Phoenix can then be downloaded to a Jupyter notebook and used to run evaluations (e.g., \nLLM Evals\n, Ragas).\nGetting Started\nInstrumentation\n is the act of adding observability code to an app yourself.\nIf you’re instrumenting an app, you need to use the OpenTelemetry SDK for your language. You’ll then use the SDK to initialize OpenTelemetry and the API to instrument your code. This will emit telemetry from your app, and any library you installed that also comes with instrumentation.\nPhoenix natively supports automatic instrumentation provided by OpenInference. For more details on OpenInfernce, checkout the \nproject\ninstrumentation setup\nDependencies\nInstall OpenTelemetry API packages:\nCopy\nnpm\n \ninstall\n \n@opentelemetry/api\n \n@opentelemetry/resources\n \n@opentelemetry/semantic-conventions\nInstall OpenInference instrumentation packages. Below is an example of adding instrumentation for OpenAI as well as the semantic conventions for OpenInference.\nCopy\nnpm\n \ninstall\n \n@arizeai/openinference-instrumentation-openai\n \n@arizeai/openinference-semantic-conventions\nInitialize the SDK\nIf you instrument a Node.js application install the \nOpenTelemetry SDK for Node.js\n:\nCopy\nnpm\n \ninstall\n \n@opentelemetry/sdk-node\nBefore any other module in your application is loaded, you must initialize the SDK. If you fail to initialize the SDK or initialize it too late, no-op implementations will be provided to any library that acquires a tracer or meter from the API.\nCopy\n/*instrumentation.ts*/\n\n\nimport\n { NodeSDK } \nfrom\n \n'@opentelemetry/sdk-node'\n;\n\n\nimport\n { ConsoleSpanExporter } \nfrom\n \n'@opentelemetry/sdk-trace-node'\n;\n\n\nimport\n { Resource } \nfrom\n \n'@opentelemetry/resources'\n;\n\n\nimport\n { SemanticResourceAttributes } \nfrom\n \n'@opentelemetry/semantic-conventions'\n;\n\n\n\n\nconst\n \nsdk\n \n=\n \nnew\n \nNodeSDK\n({\n\n\n  resource\n:\n \nnew\n \nResource\n({\n\n\n    [\nSemanticResourceAttributes\n.\nSERVICE_NAME\n]\n:\n \n'yourServiceName'\n,\n\n\n    [\nSemanticResourceAttributes\n.\nSERVICE_VERSION\n]\n:\n \n'1.0'\n,\n\n\n  })\n,\n\n\n  traceExporter\n:\n \nnew\n \nConsoleSpanExporter\n()\n,\n\n\n});\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/setup-tracing/setup-tracing-ts\ntitle: Setup Tracing: TS\n\nSetup Tracing: TS\nSetup OpenTelemetry for TypeScript and Javascript\nSetting up OpenTelemetry for TypeScript and Javascript depends on your deployment and server-side runtime. \nNodeJS\nVercel\nPrevious\nUsing OTEL Python Directly\nNext\nTracing Core Concepts\nLast updated \n8 hours ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is open telemetry?\nAnswer: ""}, {'role': 'assistant', 'content': 'OpenTelemetry is a tool that allows you to trace your NodeJS application for observability purposes. It enables you to add observability code to your application and emit telemetry data, which can be collected and used for evaluations.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NTA=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing/openai-node-sdk\ntitle: OpenAI Node SDK\n\nOpenAI Node SDK\nThis module provides automatic instrumentation for the \nOpenAI Node.js SDK\n. which may be used in conjunction with \n@opentelemetry/sdk-trace-node\n.\nInstall\nCopy\nnpm\n \ninstall\n \n--save\n \n@arizeai/openinference-instrumentation-openai\nSetup\nTo load the OpenAI instrumentation, specify it in the registerInstrumentations call along with any additional instrumentation you wish to enable.\nCopy\nimport\n { NodeTracerProvider } \nfrom\n \n""@opentelemetry/sdk-trace-node""\n;\n\n\nimport\n {\n\n\n  OpenAIInstrumentation\n,\n\n\n} \nfrom\n \n""@arizeai/openinference-instrumentation-openai""\n;\n\n\nimport\n { registerInstrumentations } \nfrom\n \n""@opentelemetry/instrumentation""\n;\n\n\n\n\nconst\n \nprovider\n \n=\n \nnew\n \nNodeTracerProvider\n();\n\n\nprovider\n.register\n();\n\n\n\n\nregisterInstrumentations\n({\n\n\n  instrumentations\n:\n [\nnew\n \nOpenAIInstrumentation\n()]\n,\n\n\n});\nResources\nExample project\nOpenInference package\nWorking examples\n\n\nPrevious\nInstructor\nNext\nLangChain.js\nLast updated \n21 days ago\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/instrumentation\ntitle: Auto Instrumentation\n\nAuto Instrumentation\nHow OpenInference facilitates automatic instrumentation of applications.\nIn order to make a system observable, it must be \ninstrumented\n: That is, code from the system’s components must emit traces.\nPhoenix natively supports collecting traces generated via OpenInference automatic instrumentation. Phoenix has pre-built auto-instrumentors for \nPython\n and \nTypeScript / JavaScript\n.\n\n\nPython\nLlamaIndex, LangChain, DSPy, OpenAI, Bedrock, Mistral and more\nTypeScript / JavaScript\nOpenAI, LangChainjs\nPrevious\nInstrumenting Prompt Templates and Prompt Variables\nNext\nAuto Instrument: Python\nLast updated \n4 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What providers are supported with openinference?\nAnswer: '}, {'role': 'assistant', 'content': 'NodeTracerProvider'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing/openai-node-sdk\ntitle: OpenAI Node SDK\n\nOpenAI Node SDK\nThis module provides automatic instrumentation for the \nOpenAI Node.js SDK\n. which may be used in conjunction with \n@opentelemetry/sdk-trace-node\n.\nInstall\nCopy\nnpm\n \ninstall\n \n--save\n \n@arizeai/openinference-instrumentation-openai\nSetup\nTo load the OpenAI instrumentation, specify it in the registerInstrumentations call along with any additional instrumentation you wish to enable.\nCopy\nimport\n { NodeTracerProvider } \nfrom\n \n""@opentelemetry/sdk-trace-node""\n;\n\n\nimport\n {\n\n\n  OpenAIInstrumentation\n,\n\n\n} \nfrom\n \n""@arizeai/openinference-instrumentation-openai""\n;\n\n\nimport\n { registerInstrumentations } \nfrom\n \n""@opentelemetry/instrumentation""\n;\n\n\n\n\nconst\n \nprovider\n \n=\n \nnew\n \nNodeTracerProvider\n();\n\n\nprovider\n.register\n();\n\n\n\n\nregisterInstrumentations\n({\n\n\n  instrumentations\n:\n [\nnew\n \nOpenAIInstrumentation\n()]\n,\n\n\n});\nResources\nExample project\nOpenInference package\nWorking examples\n\n\nPrevious\nInstructor\nNext\nLangChain.js\nLast updated \n21 days ago\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/instrumentation\ntitle: Auto Instrumentation\n\nAuto Instrumentation\nHow OpenInference facilitates automatic instrumentation of applications.\nIn order to make a system observable, it must be \ninstrumented\n: That is, code from the system’s components must emit traces.\nPhoenix natively supports collecting traces generated via OpenInference automatic instrumentation. Phoenix has pre-built auto-instrumentors for \nPython\n and \nTypeScript / JavaScript\n.\n\n\nPython\nLlamaIndex, LangChain, DSPy, OpenAI, Bedrock, Mistral and more\nTypeScript / JavaScript\nOpenAI, LangChainjs\nPrevious\nInstrumenting Prompt Templates and Prompt Variables\nNext\nAuto Instrument: Python\nLast updated \n4 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What providers are supported with openinference?\nAnswer: '}, {'role': 'assistant', 'content': 'NodeTracerProvider'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NTE=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/how-to-experiments/using-evaluators\ntitle: Using Evaluators\n\nUsing Evaluators\nLLM Evaluators\nWe provide LLM evaluators out of the box. These evaluators are vendor agnostic and can be instantiated with a Phoenix model wrapper:\nCopy\nfrom\n phoenix\n.\nexperiments\n.\nevaluators \nimport\n HelpfulnessEvaluator\n\n\nfrom\n phoenix\n.\nevals\n.\nmodels \nimport\n OpenAIModel\n\n\n\n\nhelpfulness_evaluator \n=\n \nHelpfulnessEvaluator\n(model\n=\nOpenAIModel\n())\nCode Evaluators\nCode evaluators are functions that evaluate the output of your LLM task that don\'t use another LLM as a judge. An example might be checking for whether or not a given output contains a link - which can be implemented as a RegEx match.\nphoenix.experiments.evaluators\n contains some pre-built code evaluators that can be passed to the \nevaluators\n parameter in experiments.\nPython\nCopy\nfrom\n phoenix\n.\nexperiments \nimport\n run_experiment\n,\n MatchesRegex\n\n\n\n\n# This defines a code evaluator for links\n\n\ncontains_link \n=\n \nMatchesRegex\n(\n\n\n    pattern\n=\nr\n""[-a-zA-Z0-9@:%._\\+~#=]\n{1,256}\n\\.[a-zA-Z0-9()]\n{1,6}\n\\b(\n[-a-zA-Z0-9()@:%_\\+.~#?&//=]\n*\n)\n""\n,\n\n\n    name\n=\n""contains_link""\n\n\n)\nThe above \ncontains_link\n evaluator can then be passed as an evaluator to any experiment you\'d like to run.\nFor a full list of code evaluators, please consult repo or API documentation.\nCustom Evaluators\nThe simplest way to create an evaluator is just to write a Python function. By default, a function of one argument will be passed the \noutput\n of an experiment run. These custom evaluators can either return a \nboolean\n or numeric value which will be recorded as the evaluation score.\nOutput in bounds\nImagine our experiment is testing a \ntask\n that is intended to output a numeric value from 1-100. We can write a simple evaluator to check if the output is within the allowed range:\nCopy\ndef\n \nin_bounds\n(\nx\n):\n\n\n    \nreturn\n \n1\n \n<=\n x \n<=\n \n100\nBy simply passing the \nin_bounds\n function to \nrun_experiment\n, we will automatically generate evaluations for each experiment run for whether or not the output is in the allowed range.\nMore complex evaluations can use additional information. These values can be accessed by defining a function with specific parameter names which are bound to special values:\nParameter name\nDescription\nExample\ninput\nexperiment run input\ndef eval(input): ...\noutput\nexperiment run output\ndef eval(output): ...\nexpected\nexample output\ndef eval(expected): ...\nreference\nalias for \nexpected\ndef eval(reference): ...\nmetadata\nexperiment metadata\ndef eval(metadata): ...\nThese parameters can be used in any combination and any order to write custom complex evaluators!\nEdit Distance\nBelow is an example of using the \neditdistance\n library to calculate how close the output is to the expected value:\nCopy\npip\n \ninstall\n \neditdistance\nCopy\ndef\n \nedit_distance\n(\noutput\n,\n \nexpected\n) \n->\n \nint\n:\n\n\n    \nreturn\n editdistance\n.\neval\n(\n\n\n        json.\ndumps\n(output, sort_keys\n=\nTrue\n), json.\ndumps\n(expected, sort_keys\n=\nTrue\n)\n\n\n    )\nFor even more customization, use the \ncreate_evaluator\n decorator to further customize how your evaluations show up in the Experiments UI.\nPython\nCopy\nfrom\n phoenix\n.\nexperiments\n.\nevaluators \nimport\n create_evaluator\n\n\n\n\n# the decorator can be used to set display properties\n\n\n# `name` corresponds to the metric name shown in the UI\n\n\n# `kind` indicates if the eval was made with a ""CODE"" or ""LLM"" evaluator\n\n\n@create_evaluator\n(name\n=\n""shorter?""\n, kind\n=\n""CODE""\n)\n\n\ndef\n \nwordiness_evaluator\n(\nexpected\n,\n \noutput\n):\n\n\n    reference_length \n=\n \nlen\n(expected.\nsplit\n())\n\n\n    output_length \n=\n \nlen\n(output.\nsplit\n())\n\n\n    \nreturn\n output_length \n<\n reference_length\nThe decorated \nwordiness_evaluator\n can be passed directly into \nrun_experiment\n!\nPrevious\nRun Experiments\nNext\nUse Cases: Experiments\nLast updated \n1 month ago\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/building-your-own-evals\ntitle: Custom Task Evaluation\n\nCustom Task Evaluation\nCustomize Your Own Eval Templates\nThe LLM Evals library is designed to support the building of any custom Eval templates.\nSteps to Building Your Own Eval\nFollow the following steps to easily build your own Eval with Phoenix\n1. Choose a Metric\nTo do that, you must identify what is the \nmetric best suited for your use case\n. Can you use a pre-existing template or do you need to evaluate something unique to your use case?\n2. Build a Golden Dataset\nThen, you need the \ngolden dataset\n. This should be representative of the type of data you expect the LLM eval to see. The golden dataset should have the “ground truth” label so that we can measure performance of the LLM eval template. Often such labels come from human feedback.\nBuilding such a dataset is laborious, but you can often find a standardized one for the most common use cases (as we did in the code above)\nThe Eval inferences are designed or easy benchmarking and pre-set downloadable test inferences. The inferences are pre-tested, many are hand crafted and designed for testing specific Eval tasks.\nCopy\nfrom\n phoenix\n.\nevals \nimport\n download_benchmark_dataset\n\n\n\n\ndf \n=\n \ndownload_benchmark_dataset\n(\n\n\n    task\n=\n""binary-hallucination-classification""\n, dataset_name\n=\n""halueval_qa_data""\n\n\n)\n\n\ndf\n.\nhead\n()\n3. Decide Which LLM to use For Evaluation\nThen you need to decide \nwhich LLM\n you want to use for evaluation. This could be a different LLM from the one you are using for your application. For example, you may be using Llama for your application and GPT-4 for your eval. Often this choice is influenced by questions of cost and accuracy.\n4. Build the Eval Template\nNow comes the core component that we are trying to benchmark and improve: the \neval template\n.\nYou can adjust an existing template or build your own from scratch.\nBe explicit about the following:\nWhat is the input?\n In our example, it is the documents/context that was retrieved and the query from the user.\nWhat are we asking?\n In our example, we’re asking the LLM to tell us if the document was relevant to the query\nWhat are the possible output formats?\n In our example, it is binary relevant/irrelevant, but it can also be multi-class (e.g., fully relevant, partially relevant, not relevant).\nIn order to create a new template all that is needed is the setting of the input string to the Eval function.\nCopy\nMY_CUSTOM_TEMPLATE \n=\n \n\'\'\'\n\n\n    You are evaluating the positivity or negativity of the responses to questions.\n\n\n    [BEGIN DATA]\n\n\n    ************\n\n\n    [Question]: \n{question}\n\n\n    ************\n\n\n    [Response]: \n{response}\n\n\n    [END DATA]\n\n\n\n\n\n\n    Please focus on the tone of the response.\n\n\n    Your answer must be single word, either ""positive"" or ""negative""\n\n\n    \'\'\'\nThe above template shows an example creation of an easy to use string template. The Phoenix Eval templates support both strings and objects.\nCopy\nmodel \n=\n \nOpenAIModel\n(model_name\n=\n""gpt-4""\n,temperature\n=\n0.6\n)\n\n\npositive_eval \n=\n \nllm_classify\n(\n\n\n    dataframe\n=\ndf,\n\n\n    template\n=\n MY_CUSTOM_TEMPLATE,\n\n\n    model\n=\nmodel\n\n\n)\nThe above example shows a use of the custom created template on the df dataframe.\nCopy\n#Phoenix Evals support using either strings or objects as templates\n\n\nMY_CUSTOM_TEMPLATE \n=\n \n"" ...""\n\n\nMY_CUSTOM_TEMPLATE \n=\n \nPromptTemplate\n(\n""This is a test \n{prompt}\n""\n)\n5. Run Eval on your Golden Dataset and Benchmark Performance\nYou now need to run the eval across your golden dataset. Then you can \ngenerate metrics\n (overall accuracy, precision, recall, F1, etc.) to determine the benchmark. It is important to look at more than just overall accuracy. We’ll discuss that below in more detail.\nPrevious\nEvaluators\nNext\nHow to: Evals\nLast updated \n1 month ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I create a custom evaluator?\nAnswer: '}, {'role': 'assistant', 'content': 'To create a custom evaluator, you can write a Python function that takes the output of an experiment run as an argument. This function can return a boolean or numeric value, which will be recorded as the evaluation score. You can customize the evaluator further by using specific parameter names that are bound to special values like input, output, expected, reference, or metadata. These parameters can be used in any combination and order to write complex evaluators. Additionally, you can use libraries like editdistance to calculate how close the output is to the expected value. To enhance the customization of your evaluations in the Experiments UI, you can use the create_evaluator decorator to set display properties for your custom evaluator.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/how-to-experiments/using-evaluators\ntitle: Using Evaluators\n\nUsing Evaluators\nLLM Evaluators\nWe provide LLM evaluators out of the box. These evaluators are vendor agnostic and can be instantiated with a Phoenix model wrapper:\nCopy\nfrom\n phoenix\n.\nexperiments\n.\nevaluators \nimport\n HelpfulnessEvaluator\n\n\nfrom\n phoenix\n.\nevals\n.\nmodels \nimport\n OpenAIModel\n\n\n\n\nhelpfulness_evaluator \n=\n \nHelpfulnessEvaluator\n(model\n=\nOpenAIModel\n())\nCode Evaluators\nCode evaluators are functions that evaluate the output of your LLM task that don\'t use another LLM as a judge. An example might be checking for whether or not a given output contains a link - which can be implemented as a RegEx match.\nphoenix.experiments.evaluators\n contains some pre-built code evaluators that can be passed to the \nevaluators\n parameter in experiments.\nPython\nCopy\nfrom\n phoenix\n.\nexperiments \nimport\n run_experiment\n,\n MatchesRegex\n\n\n\n\n# This defines a code evaluator for links\n\n\ncontains_link \n=\n \nMatchesRegex\n(\n\n\n    pattern\n=\nr\n""[-a-zA-Z0-9@:%._\\+~#=]\n{1,256}\n\\.[a-zA-Z0-9()]\n{1,6}\n\\b(\n[-a-zA-Z0-9()@:%_\\+.~#?&//=]\n*\n)\n""\n,\n\n\n    name\n=\n""contains_link""\n\n\n)\nThe above \ncontains_link\n evaluator can then be passed as an evaluator to any experiment you\'d like to run.\nFor a full list of code evaluators, please consult repo or API documentation.\nCustom Evaluators\nThe simplest way to create an evaluator is just to write a Python function. By default, a function of one argument will be passed the \noutput\n of an experiment run. These custom evaluators can either return a \nboolean\n or numeric value which will be recorded as the evaluation score.\nOutput in bounds\nImagine our experiment is testing a \ntask\n that is intended to output a numeric value from 1-100. We can write a simple evaluator to check if the output is within the allowed range:\nCopy\ndef\n \nin_bounds\n(\nx\n):\n\n\n    \nreturn\n \n1\n \n<=\n x \n<=\n \n100\nBy simply passing the \nin_bounds\n function to \nrun_experiment\n, we will automatically generate evaluations for each experiment run for whether or not the output is in the allowed range.\nMore complex evaluations can use additional information. These values can be accessed by defining a function with specific parameter names which are bound to special values:\nParameter name\nDescription\nExample\ninput\nexperiment run input\ndef eval(input): ...\noutput\nexperiment run output\ndef eval(output): ...\nexpected\nexample output\ndef eval(expected): ...\nreference\nalias for \nexpected\ndef eval(reference): ...\nmetadata\nexperiment metadata\ndef eval(metadata): ...\nThese parameters can be used in any combination and any order to write custom complex evaluators!\nEdit Distance\nBelow is an example of using the \neditdistance\n library to calculate how close the output is to the expected value:\nCopy\npip\n \ninstall\n \neditdistance\nCopy\ndef\n \nedit_distance\n(\noutput\n,\n \nexpected\n) \n->\n \nint\n:\n\n\n    \nreturn\n editdistance\n.\neval\n(\n\n\n        json.\ndumps\n(output, sort_keys\n=\nTrue\n), json.\ndumps\n(expected, sort_keys\n=\nTrue\n)\n\n\n    )\nFor even more customization, use the \ncreate_evaluator\n decorator to further customize how your evaluations show up in the Experiments UI.\nPython\nCopy\nfrom\n phoenix\n.\nexperiments\n.\nevaluators \nimport\n create_evaluator\n\n\n\n\n# the decorator can be used to set display properties\n\n\n# `name` corresponds to the metric name shown in the UI\n\n\n# `kind` indicates if the eval was made with a ""CODE"" or ""LLM"" evaluator\n\n\n@create_evaluator\n(name\n=\n""shorter?""\n, kind\n=\n""CODE""\n)\n\n\ndef\n \nwordiness_evaluator\n(\nexpected\n,\n \noutput\n):\n\n\n    reference_length \n=\n \nlen\n(expected.\nsplit\n())\n\n\n    output_length \n=\n \nlen\n(output.\nsplit\n())\n\n\n    \nreturn\n output_length \n<\n reference_length\nThe decorated \nwordiness_evaluator\n can be passed directly into \nrun_experiment\n!\nPrevious\nRun Experiments\nNext\nUse Cases: Experiments\nLast updated \n1 month ago\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/building-your-own-evals\ntitle: Custom Task Evaluation\n\nCustom Task Evaluation\nCustomize Your Own Eval Templates\nThe LLM Evals library is designed to support the building of any custom Eval templates.\nSteps to Building Your Own Eval\nFollow the following steps to easily build your own Eval with Phoenix\n1. Choose a Metric\nTo do that, you must identify what is the \nmetric best suited for your use case\n. Can you use a pre-existing template or do you need to evaluate something unique to your use case?\n2. Build a Golden Dataset\nThen, you need the \ngolden dataset\n. This should be representative of the type of data you expect the LLM eval to see. The golden dataset should have the “ground truth” label so that we can measure performance of the LLM eval template. Often such labels come from human feedback.\nBuilding such a dataset is laborious, but you can often find a standardized one for the most common use cases (as we did in the code above)\nThe Eval inferences are designed or easy benchmarking and pre-set downloadable test inferences. The inferences are pre-tested, many are hand crafted and designed for testing specific Eval tasks.\nCopy\nfrom\n phoenix\n.\nevals \nimport\n download_benchmark_dataset\n\n\n\n\ndf \n=\n \ndownload_benchmark_dataset\n(\n\n\n    task\n=\n""binary-hallucination-classification""\n, dataset_name\n=\n""halueval_qa_data""\n\n\n)\n\n\ndf\n.\nhead\n()\n3. Decide Which LLM to use For Evaluation\nThen you need to decide \nwhich LLM\n you want to use for evaluation. This could be a different LLM from the one you are using for your application. For example, you may be using Llama for your application and GPT-4 for your eval. Often this choice is influenced by questions of cost and accuracy.\n4. Build the Eval Template\nNow comes the core component that we are trying to benchmark and improve: the \neval template\n.\nYou can adjust an existing template or build your own from scratch.\nBe explicit about the following:\nWhat is the input?\n In our example, it is the documents/context that was retrieved and the query from the user.\nWhat are we asking?\n In our example, we’re asking the LLM to tell us if the document was relevant to the query\nWhat are the possible output formats?\n In our example, it is binary relevant/irrelevant, but it can also be multi-class (e.g., fully relevant, partially relevant, not relevant).\nIn order to create a new template all that is needed is the setting of the input string to the Eval function.\nCopy\nMY_CUSTOM_TEMPLATE \n=\n \n\'\'\'\n\n\n    You are evaluating the positivity or negativity of the responses to questions.\n\n\n    [BEGIN DATA]\n\n\n    ************\n\n\n    [Question]: \n{question}\n\n\n    ************\n\n\n    [Response]: \n{response}\n\n\n    [END DATA]\n\n\n\n\n\n\n    Please focus on the tone of the response.\n\n\n    Your answer must be single word, either ""positive"" or ""negative""\n\n\n    \'\'\'\nThe above template shows an example creation of an easy to use string template. The Phoenix Eval templates support both strings and objects.\nCopy\nmodel \n=\n \nOpenAIModel\n(model_name\n=\n""gpt-4""\n,temperature\n=\n0.6\n)\n\n\npositive_eval \n=\n \nllm_classify\n(\n\n\n    dataframe\n=\ndf,\n\n\n    template\n=\n MY_CUSTOM_TEMPLATE,\n\n\n    model\n=\nmodel\n\n\n)\nThe above example shows a use of the custom created template on the df dataframe.\nCopy\n#Phoenix Evals support using either strings or objects as templates\n\n\nMY_CUSTOM_TEMPLATE \n=\n \n"" ...""\n\n\nMY_CUSTOM_TEMPLATE \n=\n \nPromptTemplate\n(\n""This is a test \n{prompt}\n""\n)\n5. Run Eval on your Golden Dataset and Benchmark Performance\nYou now need to run the eval across your golden dataset. Then you can \ngenerate metrics\n (overall accuracy, precision, recall, F1, etc.) to determine the benchmark. It is important to look at more than just overall accuracy. We’ll discuss that below in more detail.\nPrevious\nEvaluators\nNext\nHow to: Evals\nLast updated \n1 month ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I create a custom evaluator?\nAnswer: '}, {'role': 'assistant', 'content': 'To create a custom evaluator, you can write a Python function that takes the output of an experiment run as an argument. This function can return a boolean or numeric value, which will be recorded as the evaluation score. You can customize the evaluator further by using specific parameter names that are bound to special values like input, output, expected, reference, or metadata. These parameters can be used in any combination and order to write complex evaluators. Additionally, you can use libraries like editdistance to calculate how close the output is to the expected value. To enhance the customization of your evaluations in the Experiments UI, you can use the create_evaluator decorator to set display properties for your custom evaluator.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NTI=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/setup\ntitle: Setup\n\nSetup\nHow to get started with Phoenix\nHow to run in different environments\nHosted Phoenix by Arize\nIn a python notebook\nFrom the command line\nIn a Container\nHow to configure phoenix\nAvailable ports to run phoenix\nCustomize phoenix using environment variables\nPrevious\nExamples\nNext\nEnvironments\nLast updated \n8 hours ago\n\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Where can I get help with Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'You can get help with Phoenix by joining the Phoenix Slack community.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/setup\ntitle: Setup\n\nSetup\nHow to get started with Phoenix\nHow to run in different environments\nHosted Phoenix by Arize\nIn a python notebook\nFrom the command line\nIn a Container\nHow to configure phoenix\nAvailable ports to run phoenix\nCustomize phoenix using environment variables\nPrevious\nExamples\nNext\nEnvironments\nLast updated \n8 hours ago\n\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Where can I get help with Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'You can get help with Phoenix by joining the Phoenix Slack community.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NTM=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing/dspy\ntitle: DSPy\n\nDSPyInstrumentor\n().\ninstrument\n()\nRun DSPy\nNow run invoke your compiled DSPy module. Your traces should appear inside of Phoenix.\nCopy\nclass\n \nBasicQA\n(\ndspy\n.\nSignature\n):\n\n\n    \n""""""Answer questions with short factoid answers.""""""\n\n\n\n\n    question \n=\n dspy\n.\nInputField\n()\n\n\n    answer \n=\n dspy\n.\nOutputField\n(desc\n=\n""often between 1 and 5 words""\n)\n\n\n\n\n\n\nif\n \n__name__\n \n==\n \n""__main__""\n:\n\n\n    turbo \n=\n dspy\n.\nOpenAI\n(model\n=\n""gpt-3.5-turbo""\n)\n\n\n\n\n    dspy\n.\nsettings\n.\nconfigure\n(lm\n=\nturbo)\n\n\n\n\n    \nwith\n \nusing_attributes\n(\n\n\n        session_id\n=\n""my-test-session""\n,\n\n\n        user_id\n=\n""my-test-user""\n,\n\n\n        metadata\n=\n{\n\n\n            \n""test-int""\n: \n1\n,\n\n\n            \n""test-str""\n: \n""string""\n,\n\n\n            \n""test-list""\n: [\n1\n, \n2\n, \n3\n],\n\n\n            \n""test-dict""\n: {\n\n\n                \n""key-1""\n: \n""val-1""\n,\n\n\n                \n""key-2""\n: \n""val-2""\n,\n\n\n            },\n\n\n        },\n\n\n        tags\n=\n[\n""tag-1""\n, \n""tag-2""\n],\n\n\n        prompt_template_version\n=\n""v1.0""\n,\n\n\n        prompt_template_variables\n=\n{\n\n\n            \n""city""\n: \n""Johannesburg""\n,\n\n\n            \n""date""\n: \n""July 11th""\n,\n\n\n        },\n\n\n    ):\n\n\n        \n# Define the predictor.\n\n\n        generate_answer \n=\n dspy\n.\nPredict\n(BasicQA)\n\n\n\n\n        \n# Call the predictor on a particular input.\n\n\n        pred \n=\n \ngenerate_answer\n(\n\n\n            question\n=\n""What is the capital of the united states?""\n  \n# noqa: E501\n\n\n        )\n  \n# noqa: E501\n\n\n        \nprint\n(f\n""Predicted Answer: \n{pred.answer}\n""\n)\nObserve\nNow that you have tracing setup, all predictions will be streamed to your running Phoenix for observability and evaluation.\nResources\nExample notebook\nOpenInference package\nWorking examples\nPrevious\nGroq\nNext\nVertexAI\nLast updated \n15 days ago\n\nsource: https://docs.arize.com/phoenix/reference/frequently-asked-questions\ntitle: Frequently Asked Questions\n\nFrequently Asked Questions\nCan I use Azure OpenAI?\nYes, in fact this is probably the preferred way to interact with OpenAI if your enterprise requires data privacy. Getting the parameters right for Azure can be a bit tricky so check out the \nmodels section for details.\nCan I use Phoenix locally from a remote Jupyter instance?\nYes, you can use either of the two methods below.\n1. Via ngrok (Preferred)\nInstall pyngrok on the remote machine using the command \npip install pyngrok\n.\nCreate a free account\n on ngrok and verify your email. Find \'Your Authtoken\' on the \ndashboard\n.\nIn jupyter notebook, after launching phoenix set its port number as the \nport\n parameter in the code below. \nPreferably use a default port\n for phoenix so that you won\'t have to set up ngrok tunnel every time for a new port, simply restarting phoenix will work on the same ngrok URL.\nCopy\nimport\n getpass\n\n\nfrom\n pyngrok \nimport\n ngrok\n,\n conf\n\n\nprint\n(\n""Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth""\n)\n\n\nconf\n.\nget_default\n().\nauth_token \n=\n getpass\n.\ngetpass\n()\n\n\nport \n=\n \n37689\n\n\n# Open a ngrok tunnel to the HTTP server\n\n\npublic_url \n=\n ngrok\n.\nconnect\n(port).\npublic_url\n\n\nprint\n(\n"" * ngrok tunnel \\""\n{}\n\\"" -> \\""http://127.0.0.1:\n{}\n\\""""\n.\nformat\n(public_url, port))\n""Visit Site"" using the newly printed \npublic_url\n and ignore warnings, if any.\nNOTE:\nNgrok free account does not allow more than 3 tunnels over a single ngrok agent session. Tackle this error by checking active URL tunnels using \nngrok.get_tunnels()\n and close the required URL tunnel using \nngrok.disconnect(public_url)\n.\n2. Via SSH\nThis assumes you have already set up ssh on both the local machine and the remote server.\nIf you are accessing a remote jupyter notebook from a local machine, you can also access the phoenix app by forwarding a local port to the remote server via ssh. In this particular case of using phoenix on a remote server, it is recommended that you use a default port for launching phoenix, say \nDEFAULT_PHOENIX_PORT\n.\nLaunch the phoenix app from jupyter notebook.\nIn a new terminal or command prompt, forward a local port of your choice from 49152 to 65535 (say \n52362\n) using the command below. Remote user of the remote host must have sufficient port-forwarding/admin privileges.\nCopy\nssh\n \n-L\n \n52362:localhost:\n<\nDEFAULT_PHOENIX_POR\nT\n>\n \n<\nREMOTE_USE\nR\n>\n@\n<\nREMOTE_HOS\nT\n>\nIf successful, visit \nlocalhost:52362\n to access phoenix locally.\nIf you are abruptly unable to access phoenix, check whether the ssh connection is still alive by inspecting the terminal. You can also try increasing the ssh timeout settings.\nClosing ssh tunnel:\nSimply run \nexit\n in the terminal/command prompt where you ran the port forwarding command.\nHow can I configure the backend to send the data to the phoenix UI in another container? \nIf you are working on an API whose endpoints perform RAG, but would like the phoenix server not to be launched as another thread. \nYou can do this by configuring the following the environment variable PHOENIX_COLLECTOR_ENDPOINT to point to the server running in a different process or container. https://docs.arize.com/phoenix/environments\nCan I use an older version of LlamaIndex?\nYes you can! You will have to be using \narize-phoenix>3.0.0\n and downgrade \nopeninference-instrumentation-llama-index<1.0.0\nRunning on SageMaker\nWith SageMaker notebooks, phoenix leverages the \njupyter-server-proy\n to host the server under \nproxy/6006.\nNote, that phoenix will automatically try to detect that you are running in SageMaker but you can declare the notebook runtime via a parameter to \nlaunch_app\n or an environment variable\nCopy\nimport\n os\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Who can I ask for help?\nAnswer: '}, {'role': 'assistant', 'content': 'You can ask for help by reaching out to the support team or community forums associated with the platform or tool you are using.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing/dspy\ntitle: DSPy\n\nDSPyInstrumentor\n().\ninstrument\n()\nRun DSPy\nNow run invoke your compiled DSPy module. Your traces should appear inside of Phoenix.\nCopy\nclass\n \nBasicQA\n(\ndspy\n.\nSignature\n):\n\n\n    \n""""""Answer questions with short factoid answers.""""""\n\n\n\n\n    question \n=\n dspy\n.\nInputField\n()\n\n\n    answer \n=\n dspy\n.\nOutputField\n(desc\n=\n""often between 1 and 5 words""\n)\n\n\n\n\n\n\nif\n \n__name__\n \n==\n \n""__main__""\n:\n\n\n    turbo \n=\n dspy\n.\nOpenAI\n(model\n=\n""gpt-3.5-turbo""\n)\n\n\n\n\n    dspy\n.\nsettings\n.\nconfigure\n(lm\n=\nturbo)\n\n\n\n\n    \nwith\n \nusing_attributes\n(\n\n\n        session_id\n=\n""my-test-session""\n,\n\n\n        user_id\n=\n""my-test-user""\n,\n\n\n        metadata\n=\n{\n\n\n            \n""test-int""\n: \n1\n,\n\n\n            \n""test-str""\n: \n""string""\n,\n\n\n            \n""test-list""\n: [\n1\n, \n2\n, \n3\n],\n\n\n            \n""test-dict""\n: {\n\n\n                \n""key-1""\n: \n""val-1""\n,\n\n\n                \n""key-2""\n: \n""val-2""\n,\n\n\n            },\n\n\n        },\n\n\n        tags\n=\n[\n""tag-1""\n, \n""tag-2""\n],\n\n\n        prompt_template_version\n=\n""v1.0""\n,\n\n\n        prompt_template_variables\n=\n{\n\n\n            \n""city""\n: \n""Johannesburg""\n,\n\n\n            \n""date""\n: \n""July 11th""\n,\n\n\n        },\n\n\n    ):\n\n\n        \n# Define the predictor.\n\n\n        generate_answer \n=\n dspy\n.\nPredict\n(BasicQA)\n\n\n\n\n        \n# Call the predictor on a particular input.\n\n\n        pred \n=\n \ngenerate_answer\n(\n\n\n            question\n=\n""What is the capital of the united states?""\n  \n# noqa: E501\n\n\n        )\n  \n# noqa: E501\n\n\n        \nprint\n(f\n""Predicted Answer: \n{pred.answer}\n""\n)\nObserve\nNow that you have tracing setup, all predictions will be streamed to your running Phoenix for observability and evaluation.\nResources\nExample notebook\nOpenInference package\nWorking examples\nPrevious\nGroq\nNext\nVertexAI\nLast updated \n15 days ago\n\nsource: https://docs.arize.com/phoenix/reference/frequently-asked-questions\ntitle: Frequently Asked Questions\n\nFrequently Asked Questions\nCan I use Azure OpenAI?\nYes, in fact this is probably the preferred way to interact with OpenAI if your enterprise requires data privacy. Getting the parameters right for Azure can be a bit tricky so check out the \nmodels section for details.\nCan I use Phoenix locally from a remote Jupyter instance?\nYes, you can use either of the two methods below.\n1. Via ngrok (Preferred)\nInstall pyngrok on the remote machine using the command \npip install pyngrok\n.\nCreate a free account\n on ngrok and verify your email. Find \'Your Authtoken\' on the \ndashboard\n.\nIn jupyter notebook, after launching phoenix set its port number as the \nport\n parameter in the code below. \nPreferably use a default port\n for phoenix so that you won\'t have to set up ngrok tunnel every time for a new port, simply restarting phoenix will work on the same ngrok URL.\nCopy\nimport\n getpass\n\n\nfrom\n pyngrok \nimport\n ngrok\n,\n conf\n\n\nprint\n(\n""Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth""\n)\n\n\nconf\n.\nget_default\n().\nauth_token \n=\n getpass\n.\ngetpass\n()\n\n\nport \n=\n \n37689\n\n\n# Open a ngrok tunnel to the HTTP server\n\n\npublic_url \n=\n ngrok\n.\nconnect\n(port).\npublic_url\n\n\nprint\n(\n"" * ngrok tunnel \\""\n{}\n\\"" -> \\""http://127.0.0.1:\n{}\n\\""""\n.\nformat\n(public_url, port))\n""Visit Site"" using the newly printed \npublic_url\n and ignore warnings, if any.\nNOTE:\nNgrok free account does not allow more than 3 tunnels over a single ngrok agent session. Tackle this error by checking active URL tunnels using \nngrok.get_tunnels()\n and close the required URL tunnel using \nngrok.disconnect(public_url)\n.\n2. Via SSH\nThis assumes you have already set up ssh on both the local machine and the remote server.\nIf you are accessing a remote jupyter notebook from a local machine, you can also access the phoenix app by forwarding a local port to the remote server via ssh. In this particular case of using phoenix on a remote server, it is recommended that you use a default port for launching phoenix, say \nDEFAULT_PHOENIX_PORT\n.\nLaunch the phoenix app from jupyter notebook.\nIn a new terminal or command prompt, forward a local port of your choice from 49152 to 65535 (say \n52362\n) using the command below. Remote user of the remote host must have sufficient port-forwarding/admin privileges.\nCopy\nssh\n \n-L\n \n52362:localhost:\n<\nDEFAULT_PHOENIX_POR\nT\n>\n \n<\nREMOTE_USE\nR\n>\n@\n<\nREMOTE_HOS\nT\n>\nIf successful, visit \nlocalhost:52362\n to access phoenix locally.\nIf you are abruptly unable to access phoenix, check whether the ssh connection is still alive by inspecting the terminal. You can also try increasing the ssh timeout settings.\nClosing ssh tunnel:\nSimply run \nexit\n in the terminal/command prompt where you ran the port forwarding command.\nHow can I configure the backend to send the data to the phoenix UI in another container? \nIf you are working on an API whose endpoints perform RAG, but would like the phoenix server not to be launched as another thread. \nYou can do this by configuring the following the environment variable PHOENIX_COLLECTOR_ENDPOINT to point to the server running in a different process or container. https://docs.arize.com/phoenix/environments\nCan I use an older version of LlamaIndex?\nYes you can! You will have to be using \narize-phoenix>3.0.0\n and downgrade \nopeninference-instrumentation-llama-index<1.0.0\nRunning on SageMaker\nWith SageMaker notebooks, phoenix leverages the \njupyter-server-proy\n to host the server under \nproxy/6006.\nNote, that phoenix will automatically try to detect that you are running in SageMaker but you can declare the notebook runtime via a parameter to \nlaunch_app\n or an environment variable\nCopy\nimport\n os\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Who can I ask for help?\nAnswer: '}, {'role': 'assistant', 'content': 'You can ask for help by reaching out to the support team or community forums associated with the platform or tool you are using.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NTQ=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing/llamaindex\ntitle: LlamaIndex\n\nfrom\n llama_index\n.\ncore \nimport\n set_global_handler\n\n\n\n\nset_global_handler\n(\n""arize_phoenix""\n)\n\n\n\n\n# Run all of your LlamaIndex applications as usual and traces\n\n\n# will be collected and displayed in Phoenix.\nIf you are using an older version of llamaIndex (pre-0.10), you can still use phoenix. You will have to be using \narize-phoenix>3.0.0\n and downgrade \nopeninference-instrumentation-llama-index<1.0.0\nCopy\n# Phoenix can display in real time the traces automatically\n\n\n# collected from your LlamaIndex application.\n\n\nimport\n phoenix \nas\n px\n\n\n# Look for a URL in the output to open the App in a browser.\n\n\npx\n.\nlaunch_app\n()\n\n\n# The App is initially empty, but as you proceed with the steps below,\n\n\n# traces will appear automatically as your LlamaIndex application runs.\n\n\n\n\nimport\n llama_index\n\n\nllama_index\n.\nset_global_handler\n(\n""arize_phoenix""\n)\n\n\n\n\n# Run all of your LlamaIndex applications as usual and traces\n\n\n# will be collected and displayed in Phoenix.\nBy adding the callback to the callback manager of LlamaIndex, we\'ve created a one-way data connection between your LLM application and Phoenix Server.\nTo view the traces in Phoenix, simply open the UI in your browser.\nCopy\npx\n.\nactive_session\n().\nurl\nFor a fully working example of tracing with LlamaIndex, checkout our colab notebook.\nPrevious\nOpenAI\nNext\nLangChain\nLast updated \n27 days ago\n\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing/langchain.js\ntitle: LangChain.js\n\nLangChain.js\nThis module provides automatic instrumentation for LangChain.js, more specifically, the @langchain/core module. which may be used in conjunction with @opentelemetry/sdk-trace-node.\nInstall\nCopy\nnpm\n \ninstall\n \n--save\n \n@arizeai/openinference-instrumentation-langchain\nSetup\nTo load the LangChain instrumentation, manually instrument the \n@langchain/core/callbacks/manager\n module. The callbacks manager must be manually instrumented due to the non-traditional module structure in \n@langchain/core\n. Additional instrumentations can be registered as usual in the registerInstrumentations function.\nCopy\nimport\n { NodeTracerProvider } \nfrom\n \n""@opentelemetry/sdk-trace-node""\n;\n\n\nimport\n { \n\n\n  LangChainInstrumentation \n\n\n} \nfrom\n \n""@arizeai/openinference-instrumentation-langchain""\n;\n\n\nimport\n \n*\n \nas\n CallbackManagerModule \nfrom\n \n""@langchain/core/callbacks/manager""\n;\n\n\n\n\nconst\n \nprovider\n \n=\n \nnew\n \nNodeTracerProvider\n();\n\n\nprovider\n.register\n();\n\n\n\n\nconst\n \nlcInstrumentation\n \n=\n \nnew\n \nLangChainInstrumentation\n();\n\n\n// LangChain must be manually instrumented as it doesn\'t have \n\n\n// a traditional module structure\n\n\nlcInstrumentation\n.manuallyInstrument\n(CallbackManagerModule);\n\n\nResources\nExample project\nOpenInference package\nPrevious\nOpenAI Node SDK\nNext\nConcepts: Tracing\nLast updated \n20 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: If I am using both llamaindex and langchain in the same application, can I set up instrumentation for both of them?\nAnswer: '}, {'role': 'assistant', 'content': 'Yes, you can set up instrumentation for both LlamaIndex and LangChain in the same application. Each module provides instructions on how to instrument them within your application. By following the provided guidelines for each module, you can successfully set up instrumentation for both LlamaIndex and LangChain simultaneously.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing/llamaindex\ntitle: LlamaIndex\n\nfrom\n llama_index\n.\ncore \nimport\n set_global_handler\n\n\n\n\nset_global_handler\n(\n""arize_phoenix""\n)\n\n\n\n\n# Run all of your LlamaIndex applications as usual and traces\n\n\n# will be collected and displayed in Phoenix.\nIf you are using an older version of llamaIndex (pre-0.10), you can still use phoenix. You will have to be using \narize-phoenix>3.0.0\n and downgrade \nopeninference-instrumentation-llama-index<1.0.0\nCopy\n# Phoenix can display in real time the traces automatically\n\n\n# collected from your LlamaIndex application.\n\n\nimport\n phoenix \nas\n px\n\n\n# Look for a URL in the output to open the App in a browser.\n\n\npx\n.\nlaunch_app\n()\n\n\n# The App is initially empty, but as you proceed with the steps below,\n\n\n# traces will appear automatically as your LlamaIndex application runs.\n\n\n\n\nimport\n llama_index\n\n\nllama_index\n.\nset_global_handler\n(\n""arize_phoenix""\n)\n\n\n\n\n# Run all of your LlamaIndex applications as usual and traces\n\n\n# will be collected and displayed in Phoenix.\nBy adding the callback to the callback manager of LlamaIndex, we\'ve created a one-way data connection between your LLM application and Phoenix Server.\nTo view the traces in Phoenix, simply open the UI in your browser.\nCopy\npx\n.\nactive_session\n().\nurl\nFor a fully working example of tracing with LlamaIndex, checkout our colab notebook.\nPrevious\nOpenAI\nNext\nLangChain\nLast updated \n27 days ago\n\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing/langchain.js\ntitle: LangChain.js\n\nLangChain.js\nThis module provides automatic instrumentation for LangChain.js, more specifically, the @langchain/core module. which may be used in conjunction with @opentelemetry/sdk-trace-node.\nInstall\nCopy\nnpm\n \ninstall\n \n--save\n \n@arizeai/openinference-instrumentation-langchain\nSetup\nTo load the LangChain instrumentation, manually instrument the \n@langchain/core/callbacks/manager\n module. The callbacks manager must be manually instrumented due to the non-traditional module structure in \n@langchain/core\n. Additional instrumentations can be registered as usual in the registerInstrumentations function.\nCopy\nimport\n { NodeTracerProvider } \nfrom\n \n""@opentelemetry/sdk-trace-node""\n;\n\n\nimport\n { \n\n\n  LangChainInstrumentation \n\n\n} \nfrom\n \n""@arizeai/openinference-instrumentation-langchain""\n;\n\n\nimport\n \n*\n \nas\n CallbackManagerModule \nfrom\n \n""@langchain/core/callbacks/manager""\n;\n\n\n\n\nconst\n \nprovider\n \n=\n \nnew\n \nNodeTracerProvider\n();\n\n\nprovider\n.register\n();\n\n\n\n\nconst\n \nlcInstrumentation\n \n=\n \nnew\n \nLangChainInstrumentation\n();\n\n\n// LangChain must be manually instrumented as it doesn\'t have \n\n\n// a traditional module structure\n\n\nlcInstrumentation\n.manuallyInstrument\n(CallbackManagerModule);\n\n\nResources\nExample project\nOpenInference package\nPrevious\nOpenAI Node SDK\nNext\nConcepts: Tracing\nLast updated \n20 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: If I am using both llamaindex and langchain in the same application, can I set up instrumentation for both of them?\nAnswer: '}, {'role': 'assistant', 'content': 'Yes, you can set up instrumentation for both LlamaIndex and LangChain in the same application. Each module provides instructions on how to instrument them within your application. By following the provided guidelines for each module, you can successfully set up instrumentation for both LlamaIndex and LangChain simultaneously.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NTU=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/extract-data-from-spans\ntitle: Querying Spans\n\n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n)\nSpecial Separators\nIf a different separator is desired, say \n\\n************\\n\n, it can be specified as follows.\nCopy\nquery \n=\n \nSpanQuery\n().\nconcat\n(\n\n\n    \n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n).\nwith_concat_separator\n(\n\n\n    separator\n=\n""\\n************\\n""\n,\n\n\n)\nUsing Parent ID as Index\nThis is useful for joining a span to its parent span. To do that we would first index the child span by selecting its parent ID and renaming it as \nspan_id\n. This works because \nspan_id\n is a special column name: whichever column having that name will become the index of the output DataFrame.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n\n\n    span_id\n=\n""parent_id""\n,\n\n\n    output\n=\n""output.value,\n\n\n)\nJoining a Span to Its Parent\nTo do this, we would provide two queries to Phoenix which will return two simultaneous dataframes that can be joined together by pandas. The \nquery_for_child_spans\n uses \nparent_id\n as index as shown in \nUsing Parent ID as Index\n, and \npx.Client().query_spans()\n returns a list of dataframes when multiple queries are given.\nCopy\nimport\n pandas \nas\n pd\n\n\n\n\npd\n.\nconcatenate\n(\n\n\n    px.\nClient\n().\nquery_spans\n(\n\n\n        query_for_parent_spans,\n\n\n        query_for_child_spans,\n\n\n    ),\n\n\n    axis\n=\n1\n,        \n# joining on the row indices\n\n\n    join\n=\n""inner""\n,  \n# inner-join by the indices of the dataframes\n\n\n)\nHow to use Data for Evaluation\nExtract the Input and Output from LLM Spans\nTo learn more about extracting span attributes, see \nExtracting Span Attributes\n.\nCopy\nfrom\n phoenix\n.\ntrace\n.\ndsl \nimport\n SpanQuery\n\n\n\n\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""span_kind == \'LLM\'""\n,\n\n\n).\nselect\n(\n\n\n    input\n=\n""input.value""\n,\n\n\n    output\n=\n""output.value,\n\n\n)\n\n\n\n\n# The Phoenix Client can take this query and return a dataframe.\n\n\npx\n.\nClient\n().\nquery_spans\n(query)\nRetrieval (RAG) Relevance Evaluations\nTo extract the dataframe input for \nRetrieval (RAG) Relevance evaluations\n, we can apply the query described in the \nExample\n, or leverage the \nhelper\n function implementing the same query.\nQ&A on Retrieved Data Evaluations\nTo extract the dataframe input to the \nQ&A on Retrieved Data evaluations\n, we can use a \nhelper\n function or use the following query (which is what\'s inside the helper function). This query applies techniques described in the \nAdvanced Usage\n section.\nCopy\nimport\n pandas \nas\n pd\n\n\nfrom\n phoenix\n.\ntrace\n.\ndsl \nimport\n SpanQuery\n\n\n\n\nquery_for_root_span \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""parent_id is None""\n,   \n# Filter for root spans\n\n\n).\nselect\n(\n\n\n    input\n=\n""input.value""\n,   \n# Input contains the user\'s question\n\n\n    output\n=\n""output.value""\n, \n# Output contains the LLM\'s answer\n\n\n)\n\n\n\n\nquery_for_retrieved_documents \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""span_kind == \'RETRIEVER\'""\n,  \n# Filter for RETRIEVER span\n\n\n).\nselect\n(\n\n\n    \n# Rename parent_id as span_id. This turns the parent_id\n\n\n    \n# values into the index of the output dataframe.\n\n\n    span_id\n=\n""parent_id""\n,\n\n\n).\nconcat\n(\n\n\n    \n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n)\n\n\n\n\n# Perform an inner join on the two sets of spans.\n\n\npd\n.\nconcat\n(\n\n\n    px.\nClient\n().\nquery_spans\n(\n\n\n        query_for_root_span,\n\n\n        query_for_retrieved_documents,\n\n\n    ),\n\n\n    axis\n=\n1\n,\n\n\n    join\n=\n""inner""\n,\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/extract-data-from-spans\ntitle: Querying Spans\n\nQuerying Spans\nSpan queries help you extract data from your traces into DataFrames for evaluation\nHow to Run a Query\nYou can query for data from the traces collected in Phoenix using the \nClient\n. \n\nTo simply get DataFrames of spans, you can simply ask for a DataFrame. Each row of the DataFrame with be a span that matches the filter criteria and time range passed in. If you leave the parameters blank, you will get all the spans.\nCopy\nimport\n phoenix \nas\n px\n\n\n\n\n# You can query for spans with the same filter conditions as in the UI\n\n\npx\n.\nClient\n().\nget_spans_dataframe\n(\n""span_kind == \'CHAIN\'""\n)\n\nYou can also query for data using our query DSL (domain specific language). Below is an example of how to pull all retriever spans and select the input value. The output of this query is a DataFrame that contains the input values for all retriever spans.\nCopy\nimport\n phoenix \nas\n px\n\n\nfrom\n phoenix\n.\ntrace\n.\ndsl \nimport\n SpanQuery\n\n\n\n\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n# Filter for the `RETRIEVER` span kind.\n\n\n    \n# The filter condition is a string of valid Python boolean expression.\n\n\n    \n""span_kind == \'RETRIEVER\'""\n,\n\n\n).\nselect\n(\n\n\n    \n# Extract the span attribute `input.value` which contains the query for the\n\n\n    \n# retriever. Rename it as the `input` column in the output dataframe.\n\n\n    input\n=\n""input.value""\n,\n\n\n)\n\n\n\n\n# The Phoenix Client can take this query and return the dataframe.\n\n\npx\n.\nClient\n().\nquery_spans\n(query)\nDataFrame Index\n\nBy default, the result DataFrame is indexed by \nspan_id\n, and if \n.explode()\n is used, the index from the exploded list is added to create a multi-index on the result DataFrame. For the special \nretrieval.documents\n span attribute, the added index is renamed as \ndocument_position\n.\nHow to Specify a Time Range\nBy default, all queries will collect all spans that are in your Phoenix instance. If you\'d like to focus on most recent spans, you can pull spans based on time frames using \nstart_time\n and \nend_time\n.\nCopy\nimport\n phoenix \nas\n px\n\n\nfrom\n phoenix\n.\ntrace\n.\ndsl \nimport\n SpanQuery\n\n\nfrom\n datetime \nimport\n datetime\n,\n timedelta\n\n\n\n\n# Initiate Phoenix client\n\n\npx_client \n=\n px\n.\nClient\n()\n\n\n\n\n# Get spans from the last 7 days only\n\n\nstart \n=\n datetime\n.\nnow\n()\n \n-\n \ntimedelta\n(days\n=\n7\n)\n\n\n\n\n# Get spans to exclude the last 24 hours\n\n\nend \n=\n datetime\n.\nnow\n()\n \n-\n \ntimedelta\n(days\n=\n1\n)\n\n\n\n\nphoenix_df \n=\n px_client\n.\nquery_spans\n(start_time\n=\nstart, end_time\n=\nend)\nHow to Specify a Project\nBy default all queries are executed against the default project or the project set via the \nPHOENIX_PROJECT_NAME\n environment variable. If you choose to pull from a different project, all methods on the \nClient\n have an optional parameter named \nproject_name\nCopy\nimport\n phoenix \nas\n px\n\n\nfrom\n phoenix\n.\ntrace\n.\ndsl \nimport\n SpanQuery\n\n\n\n\n# Get spans from a project\n\n\npx\n.\nClient\n().\nget_spans_dataframe\n(project_name\n=\n""<my-project>""\n)\n\n\n\n\n# Using the query DSL\n\n\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n""span_kind == \'CHAIN\'""\n).\nselect\n(input\n=\n""input.value""\n)\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Is there any document for the search query syntax on phoenix UI? Is it possible to search for values in children spans?\nAnswer: '}, {'role': 'assistant', 'content': 'Yes, there is documentation available for querying spans in Phoenix UI. You can search for values in children spans by specifying the desired separator or using the parent ID as an index to join a span with its parent span. This allows for extracting data from spans into DataFrames for evaluation purposes.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/extract-data-from-spans\ntitle: Querying Spans\n\n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n)\nSpecial Separators\nIf a different separator is desired, say \n\\n************\\n\n, it can be specified as follows.\nCopy\nquery \n=\n \nSpanQuery\n().\nconcat\n(\n\n\n    \n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n).\nwith_concat_separator\n(\n\n\n    separator\n=\n""\\n************\\n""\n,\n\n\n)\nUsing Parent ID as Index\nThis is useful for joining a span to its parent span. To do that we would first index the child span by selecting its parent ID and renaming it as \nspan_id\n. This works because \nspan_id\n is a special column name: whichever column having that name will become the index of the output DataFrame.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n\n\n    span_id\n=\n""parent_id""\n,\n\n\n    output\n=\n""output.value,\n\n\n)\nJoining a Span to Its Parent\nTo do this, we would provide two queries to Phoenix which will return two simultaneous dataframes that can be joined together by pandas. The \nquery_for_child_spans\n uses \nparent_id\n as index as shown in \nUsing Parent ID as Index\n, and \npx.Client().query_spans()\n returns a list of dataframes when multiple queries are given.\nCopy\nimport\n pandas \nas\n pd\n\n\n\n\npd\n.\nconcatenate\n(\n\n\n    px.\nClient\n().\nquery_spans\n(\n\n\n        query_for_parent_spans,\n\n\n        query_for_child_spans,\n\n\n    ),\n\n\n    axis\n=\n1\n,        \n# joining on the row indices\n\n\n    join\n=\n""inner""\n,  \n# inner-join by the indices of the dataframes\n\n\n)\nHow to use Data for Evaluation\nExtract the Input and Output from LLM Spans\nTo learn more about extracting span attributes, see \nExtracting Span Attributes\n.\nCopy\nfrom\n phoenix\n.\ntrace\n.\ndsl \nimport\n SpanQuery\n\n\n\n\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""span_kind == \'LLM\'""\n,\n\n\n).\nselect\n(\n\n\n    input\n=\n""input.value""\n,\n\n\n    output\n=\n""output.value,\n\n\n)\n\n\n\n\n# The Phoenix Client can take this query and return a dataframe.\n\n\npx\n.\nClient\n().\nquery_spans\n(query)\nRetrieval (RAG) Relevance Evaluations\nTo extract the dataframe input for \nRetrieval (RAG) Relevance evaluations\n, we can apply the query described in the \nExample\n, or leverage the \nhelper\n function implementing the same query.\nQ&A on Retrieved Data Evaluations\nTo extract the dataframe input to the \nQ&A on Retrieved Data evaluations\n, we can use a \nhelper\n function or use the following query (which is what\'s inside the helper function). This query applies techniques described in the \nAdvanced Usage\n section.\nCopy\nimport\n pandas \nas\n pd\n\n\nfrom\n phoenix\n.\ntrace\n.\ndsl \nimport\n SpanQuery\n\n\n\n\nquery_for_root_span \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""parent_id is None""\n,   \n# Filter for root spans\n\n\n).\nselect\n(\n\n\n    input\n=\n""input.value""\n,   \n# Input contains the user\'s question\n\n\n    output\n=\n""output.value""\n, \n# Output contains the LLM\'s answer\n\n\n)\n\n\n\n\nquery_for_retrieved_documents \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""span_kind == \'RETRIEVER\'""\n,  \n# Filter for RETRIEVER span\n\n\n).\nselect\n(\n\n\n    \n# Rename parent_id as span_id. This turns the parent_id\n\n\n    \n# values into the index of the output dataframe.\n\n\n    span_id\n=\n""parent_id""\n,\n\n\n).\nconcat\n(\n\n\n    \n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n)\n\n\n\n\n# Perform an inner join on the two sets of spans.\n\n\npd\n.\nconcat\n(\n\n\n    px.\nClient\n().\nquery_spans\n(\n\n\n        query_for_root_span,\n\n\n        query_for_retrieved_documents,\n\n\n    ),\n\n\n    axis\n=\n1\n,\n\n\n    join\n=\n""inner""\n,\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/extract-data-from-spans\ntitle: Querying Spans\n\nQuerying Spans\nSpan queries help you extract data from your traces into DataFrames for evaluation\nHow to Run a Query\nYou can query for data from the traces collected in Phoenix using the \nClient\n. \n\nTo simply get DataFrames of spans, you can simply ask for a DataFrame. Each row of the DataFrame with be a span that matches the filter criteria and time range passed in. If you leave the parameters blank, you will get all the spans.\nCopy\nimport\n phoenix \nas\n px\n\n\n\n\n# You can query for spans with the same filter conditions as in the UI\n\n\npx\n.\nClient\n().\nget_spans_dataframe\n(\n""span_kind == \'CHAIN\'""\n)\n\nYou can also query for data using our query DSL (domain specific language). Below is an example of how to pull all retriever spans and select the input value. The output of this query is a DataFrame that contains the input values for all retriever spans.\nCopy\nimport\n phoenix \nas\n px\n\n\nfrom\n phoenix\n.\ntrace\n.\ndsl \nimport\n SpanQuery\n\n\n\n\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n# Filter for the `RETRIEVER` span kind.\n\n\n    \n# The filter condition is a string of valid Python boolean expression.\n\n\n    \n""span_kind == \'RETRIEVER\'""\n,\n\n\n).\nselect\n(\n\n\n    \n# Extract the span attribute `input.value` which contains the query for the\n\n\n    \n# retriever. Rename it as the `input` column in the output dataframe.\n\n\n    input\n=\n""input.value""\n,\n\n\n)\n\n\n\n\n# The Phoenix Client can take this query and return the dataframe.\n\n\npx\n.\nClient\n().\nquery_spans\n(query)\nDataFrame Index\n\nBy default, the result DataFrame is indexed by \nspan_id\n, and if \n.explode()\n is used, the index from the exploded list is added to create a multi-index on the result DataFrame. For the special \nretrieval.documents\n span attribute, the added index is renamed as \ndocument_position\n.\nHow to Specify a Time Range\nBy default, all queries will collect all spans that are in your Phoenix instance. If you\'d like to focus on most recent spans, you can pull spans based on time frames using \nstart_time\n and \nend_time\n.\nCopy\nimport\n phoenix \nas\n px\n\n\nfrom\n phoenix\n.\ntrace\n.\ndsl \nimport\n SpanQuery\n\n\nfrom\n datetime \nimport\n datetime\n,\n timedelta\n\n\n\n\n# Initiate Phoenix client\n\n\npx_client \n=\n px\n.\nClient\n()\n\n\n\n\n# Get spans from the last 7 days only\n\n\nstart \n=\n datetime\n.\nnow\n()\n \n-\n \ntimedelta\n(days\n=\n7\n)\n\n\n\n\n# Get spans to exclude the last 24 hours\n\n\nend \n=\n datetime\n.\nnow\n()\n \n-\n \ntimedelta\n(days\n=\n1\n)\n\n\n\n\nphoenix_df \n=\n px_client\n.\nquery_spans\n(start_time\n=\nstart, end_time\n=\nend)\nHow to Specify a Project\nBy default all queries are executed against the default project or the project set via the \nPHOENIX_PROJECT_NAME\n environment variable. If you choose to pull from a different project, all methods on the \nClient\n have an optional parameter named \nproject_name\nCopy\nimport\n phoenix \nas\n px\n\n\nfrom\n phoenix\n.\ntrace\n.\ndsl \nimport\n SpanQuery\n\n\n\n\n# Get spans from a project\n\n\npx\n.\nClient\n().\nget_spans_dataframe\n(project_name\n=\n""<my-project>""\n)\n\n\n\n\n# Using the query DSL\n\n\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n""span_kind == \'CHAIN\'""\n).\nselect\n(input\n=\n""input.value""\n)\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Is there any document for the search query syntax on phoenix UI? Is it possible to search for values in children spans?\nAnswer: '}, {'role': 'assistant', 'content': 'Yes, there is documentation available for querying spans in Phoenix UI. You can search for values in children spans by specifying the desired separator or using the parent ID as an index to join a span with its parent span. This allows for extracting data from spans into DataFrames for evaluation purposes.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NTY=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/inferences/how-to-inferences\ntitle: How-to: Inferences\n\nHow-to: Inferences\nImport your data\nImport Inference data (CV, NLP)\nImport Prompt and Response (LLM) data\nImport Retrieval data\nImport Corpus (Vector Store) data\nManage the App\nDefine your dataset(s)\nHow to launch the app\nHow to view the UI\nHow to close the app\nHow to export data\nHow to export your data for labeling, evaluation, or fine-tuning\nHow to export embeddings\nHow to export a cluster\nHow to export all clusters\nHow to generate embeddings\n\n\nPrevious\nQuickstart: Inferences\nNext\nImport Your Data\nLast updated \n4 months ago\n\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nNot more than 4-1/4 inches high, or more than 6 inches long, or greater than \\n0.016 inch thick.\\nd. Not more than 3.5 ounces (Charge flat-size prices for First-Class Mail \\ncard-type pieces over 3.5 ounces.)\\n\\n  Instruction: Based on the above documents, provide a detailed answer for the user question below.\\n  Answer \\""don\'t know\\"" if not present in the document.\\n  "",\n\n\n       \n""llm.input_messages.1.message.role""\n:\n \n""user""\n,\n\n\n       \n""llm.input_messages.1.message.content""\n:\n \n""Hello""\n,\n\n\n       \n""llm.model_name""\n:\n \n""gpt-4-turbo-preview""\n,\n\n\n       \n""llm.invocation_parameters""\n:\n \n""{\\""temperature\\"": 0.1, \\""model\\"": \\""gpt-4-turbo-preview\\""}""\n,\n\n\n       \n""output.value""\n:\n \n""How are you?""\n }\n,\n\n\n   \n""events""\n:\n []\n,\n\n\n   \n""links""\n:\n []\n,\n\n\n   \n""resource""\n:\n {\n\n\n       \n""attributes""\n:\n {}\n,\n\n\n       \n""schema_url""\n:\n \n""""\n\n\n   }\n\n\n}\nSpans can be nested, as is implied by the presence of a parent span ID: child spans represent sub-operations. This allows spans to more accurately capture the work done in an application.\nTraces\nA trace records the paths taken by requests (made by an application or end-user) as they propagate through multiple steps.\nWithout tracing, it is challenging to pinpoint the cause of performance problems in a system.\nIt improves the visibility of our application or system’s health and lets us debug behavior that is difficult to reproduce locally. Tracing is essential for LLM applications, which commonly have nondeterministic problems or are too complicated to reproduce locally.\nTracing makes debugging and understanding LLM applications less daunting by breaking down what happens within a request as it flows through a system.\nA trace is made of one or more spans. The first span represents the root span. Each root span represents a request from start to finish. The spans underneath the parent provide a more in-depth context of what occurs during a request (or what steps make up a request).\nProjects\nA \nproject\n is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces. Projects can be useful for various use-cases such as separating out environments, logging traces for evaluation runs, etc. To learn more about how to setup projects,  see the \nhow-to guide.\nSpan Kind\nWhen a span is created,  it is created as one of the following: Chain, Retriever, Reranker, LLM, Embedding, Agent, or Tool. \nCHAIN\nA Chain is a starting point or a link between different LLM application steps. For example, a Chain span could be used to represent the beginning of a request to an LLM application or the glue code that passes context from a retriever to and LLM call.\nRETRIEVER\nA Retriever is a span that represents a data retrieval step. For example, a Retriever span could be used to represent a call to a vector store or a database.\nRERANKER\nA Reranker is a span that represents the reranking of a set of input documents. For example, a cross-encoder may be used to compute the input documents\' relevance scores with respect to a user query, and the top K documents with the highest scores are then returned by the Reranker.\nLLM\nAn LLM is a span that represents a call to an LLM. For example, an LLM span could be used to represent a call to OpenAI or Llama.\nEMBEDDING\nAn Embedding is a span that represents a call to an LLM for an embedding. For example, an Embedding span could be used to represent a call OpenAI to get an ada-2 embedding for retrieval.\nTOOL\nA Tool is a span that represents a call to an external tool such as a calculator or a weather API.\nAGENT\nA span that encompasses calls to LLMs and Tools. An agent describes a reasoning block that acts on tools using the guidance of an LLM.\n\nSpan Attributes\nAttributes are key-value pairs that contain metadata that you can use to annotate a span to carry information about the operation it is tracking.\nFor example, if a span invokes an LLM, you can capture the model name, the invocation parameters, the token count, and so on.\nAttributes have the following rules:\nKeys must be non-null string values\nValues must be a non-null string, boolean, floating point value, integer, or an array of these values Additionally, there are Semantic Attributes, which are known naming conventions for metadata that is typically present in common operations. It\'s helpful to use semantic attribute naming wherever possible so that common kinds of metadata are standardized across systems. See \nsemantic conventions\n for more information.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Is there a limit on the amount of messages we can send to the UI? \nAnswer: '}, {'role': 'assistant', 'content': 'There is no specific information provided in the context regarding a limit on the amount of messages that can be sent to the UI.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/inferences/how-to-inferences\ntitle: How-to: Inferences\n\nHow-to: Inferences\nImport your data\nImport Inference data (CV, NLP)\nImport Prompt and Response (LLM) data\nImport Retrieval data\nImport Corpus (Vector Store) data\nManage the App\nDefine your dataset(s)\nHow to launch the app\nHow to view the UI\nHow to close the app\nHow to export data\nHow to export your data for labeling, evaluation, or fine-tuning\nHow to export embeddings\nHow to export a cluster\nHow to export all clusters\nHow to generate embeddings\n\n\nPrevious\nQuickstart: Inferences\nNext\nImport Your Data\nLast updated \n4 months ago\n\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nNot more than 4-1/4 inches high, or more than 6 inches long, or greater than \\n0.016 inch thick.\\nd. Not more than 3.5 ounces (Charge flat-size prices for First-Class Mail \\ncard-type pieces over 3.5 ounces.)\\n\\n  Instruction: Based on the above documents, provide a detailed answer for the user question below.\\n  Answer \\""don\'t know\\"" if not present in the document.\\n  "",\n\n\n       \n""llm.input_messages.1.message.role""\n:\n \n""user""\n,\n\n\n       \n""llm.input_messages.1.message.content""\n:\n \n""Hello""\n,\n\n\n       \n""llm.model_name""\n:\n \n""gpt-4-turbo-preview""\n,\n\n\n       \n""llm.invocation_parameters""\n:\n \n""{\\""temperature\\"": 0.1, \\""model\\"": \\""gpt-4-turbo-preview\\""}""\n,\n\n\n       \n""output.value""\n:\n \n""How are you?""\n }\n,\n\n\n   \n""events""\n:\n []\n,\n\n\n   \n""links""\n:\n []\n,\n\n\n   \n""resource""\n:\n {\n\n\n       \n""attributes""\n:\n {}\n,\n\n\n       \n""schema_url""\n:\n \n""""\n\n\n   }\n\n\n}\nSpans can be nested, as is implied by the presence of a parent span ID: child spans represent sub-operations. This allows spans to more accurately capture the work done in an application.\nTraces\nA trace records the paths taken by requests (made by an application or end-user) as they propagate through multiple steps.\nWithout tracing, it is challenging to pinpoint the cause of performance problems in a system.\nIt improves the visibility of our application or system’s health and lets us debug behavior that is difficult to reproduce locally. Tracing is essential for LLM applications, which commonly have nondeterministic problems or are too complicated to reproduce locally.\nTracing makes debugging and understanding LLM applications less daunting by breaking down what happens within a request as it flows through a system.\nA trace is made of one or more spans. The first span represents the root span. Each root span represents a request from start to finish. The spans underneath the parent provide a more in-depth context of what occurs during a request (or what steps make up a request).\nProjects\nA \nproject\n is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces. Projects can be useful for various use-cases such as separating out environments, logging traces for evaluation runs, etc. To learn more about how to setup projects,  see the \nhow-to guide.\nSpan Kind\nWhen a span is created,  it is created as one of the following: Chain, Retriever, Reranker, LLM, Embedding, Agent, or Tool. \nCHAIN\nA Chain is a starting point or a link between different LLM application steps. For example, a Chain span could be used to represent the beginning of a request to an LLM application or the glue code that passes context from a retriever to and LLM call.\nRETRIEVER\nA Retriever is a span that represents a data retrieval step. For example, a Retriever span could be used to represent a call to a vector store or a database.\nRERANKER\nA Reranker is a span that represents the reranking of a set of input documents. For example, a cross-encoder may be used to compute the input documents\' relevance scores with respect to a user query, and the top K documents with the highest scores are then returned by the Reranker.\nLLM\nAn LLM is a span that represents a call to an LLM. For example, an LLM span could be used to represent a call to OpenAI or Llama.\nEMBEDDING\nAn Embedding is a span that represents a call to an LLM for an embedding. For example, an Embedding span could be used to represent a call OpenAI to get an ada-2 embedding for retrieval.\nTOOL\nA Tool is a span that represents a call to an external tool such as a calculator or a weather API.\nAGENT\nA span that encompasses calls to LLMs and Tools. An agent describes a reasoning block that acts on tools using the guidance of an LLM.\n\nSpan Attributes\nAttributes are key-value pairs that contain metadata that you can use to annotate a span to carry information about the operation it is tracking.\nFor example, if a span invokes an LLM, you can capture the model name, the invocation parameters, the token count, and so on.\nAttributes have the following rules:\nKeys must be non-null string values\nValues must be a non-null string, boolean, floating point value, integer, or an array of these values Additionally, there are Semantic Attributes, which are known naming conventions for metadata that is typically present in common operations. It\'s helpful to use semantic attribute naming wherever possible so that common kinds of metadata are standardized across systems. See \nsemantic conventions\n for more information.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Is there a limit on the amount of messages we can send to the UI? \nAnswer: '}, {'role': 'assistant', 'content': 'There is no specific information provided in the context regarding a limit on the amount of messages that can be sent to the UI.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NTc=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/evaluation-models\ntitle: Evaluation Models\n\nEvaluation Models\narize-phoenix-evals\n supports a large set of foundation models for Evals such as:\nOpenAI\nVertex AI\nAzure Open AI\nAnthropic\nMixtral/Mistral\nAWS Bedrock\nFalcon\nCode Llama\nLlama3\nDeepseek\nDeberta\nDBRX\nQwen\nAnd many more.\nThere are direct model integrations in Phoenix and indirect model integrations (e.x. local modals) through \nLiteLLM\n.\nDirect Integrations:\nThese integrations are native to the Phoenix Evals package and have better throughput, rate limit and error management.\nVertex AI\nOpenAI\nAzure OpenAI\nAnthropic\nMistral\nPrevious\nOnline Evals\nNext\nOverview: Datasets\nLast updated \n7 days ago\n\nsource: https://docs.arize.com/phoenix/inferences/phoenix-inferences\ntitle: Quickstart: Inferences\n\nQuickstart: Inferences\nObservability for all model types (LLM, NLP, CV, Tabular)\nOverview\nPhoenix Inferences allows you to observe the performance of your model through visualizing all the model’s inferences in one interactive UMAP view.\nThis powerful visualization can be leveraged during EDA to understand model drift, find low performing clusters, uncover retrieval issues, and export data for retraining / fine tuning.\nQuickstart\nThe following Quickstart can be executed in a Jupyter notebook or Google Colab.\nWe will begin by logging just a training set. Then proceed to add a production set for comparison.\nStep 1: Install and load dependencies\nUse \npip\n or \nconda\nto install \narize-phoenix\n.\nCopy\n!pip install arize\n-\nphoenix\n\n\n\n\nimport\n phoenix \nas\n px\nStep 2: Prepare model data\nPhoenix visualizes data taken from pandas dataframe, where each row of the dataframe compasses all the information about each inference (including feature values, prediction, metadata, etc.)\nFor this Quickstart, we will show an example of visualizing the inferences from a computer vision model. See example notebooks for all model types \nhere\n.\nLet’s begin by working with the training set for this model.\nDownload the dataset and load it into a Pandas dataframe.\nCopy\nimport\n pandas \nas\n pd\n\n\n\n\ntrain_df \n=\n pd\n.\nread_parquet\n(\n\n\n    ""http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/cv/human-actions/human_actions_training.parquet""\n\n\n)\nPreview the dataframe with \ntrain_df.head()\n and note that each row contains all the data specific to this CV model for each inference.\nCopy\ntrain_df.head()\nStep 3: Define a Schema\nBefore we can log these inferences, we need to define a Schema object to describe them.\nThe Schema object informs Phoenix of the fields that the columns of the dataframe should map to.\nHere we define a Schema to describe our particular CV training set:\nCopy\n# Define Schema to indicate which columns in train_df should map to each field\n\n\ntrain_schema \n=\n px\n.\nSchema\n(\n\n\n    timestamp_column_name\n=\n""prediction_ts""\n,\n\n\n    prediction_label_column_name\n=\n""predicted_action""\n,\n\n\n    actual_label_column_name\n=\n""actual_action""\n,\n\n\n    embedding_feature_column_names\n=\n{\n\n\n        \n""image_embedding""\n: px.\nEmbeddingColumnNames\n(\n\n\n            vector_column_name\n=\n""image_vector""\n,\n\n\n            link_to_data_column_name\n=\n""url""\n,\n\n\n        ),\n\n\n    },\n\n\n)\nImportant\n:\n The fields used in a Schema will \nvary\n depending on the model type that you are working with.\nFor examples on how Schema are defined for other model types (NLP, tabular, LLM-based applications), see example notebooks under \nEmbedding Analysis\n and \nStructured Data Analysis\n.\nStep 4: Wrap into Inference object\nWrap your \ntrain_df\n and schema \ntrain_schema\n into a Phoenix \ninference\n object:\nCopy\ntrain_ds \n=\n \nInference\n(dataframe\n=\ntrain_df, schema\n=\ntrain_schema, name\n=\n""training""\n)\nStep 5: Launch Phoenix!\nWe are now ready to launch Phoenix with our Inferences!\nHere, we are passing \ntrain_ds\n as the \nprimary\n inferences, as we are only visualizing one inference set (see Step 6 for adding additional inference sets).\nCopy\nsession \n=\n px\n.\nlaunch_app\n(primary\n=\ntrain_ds)\nRunning this will fire up a Phoenix visualization. Follow in the instructions in the output to view Phoenix in a browser, or in-line in your notebook:\nCopy\n🌍 To view the Phoenix app in your browser, visit https://x0u0hsyy843-496ff2e9c6d22116-6060-colab.googleusercontent.com/\n\n\n📺 To view the Phoenix app in a notebook, run `px.active_session().view()`\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What model types are supported by Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Phoenix supports model types such as LLM, NLP, CV, and Tabular.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/evaluation-models\ntitle: Evaluation Models\n\nEvaluation Models\narize-phoenix-evals\n supports a large set of foundation models for Evals such as:\nOpenAI\nVertex AI\nAzure Open AI\nAnthropic\nMixtral/Mistral\nAWS Bedrock\nFalcon\nCode Llama\nLlama3\nDeepseek\nDeberta\nDBRX\nQwen\nAnd many more.\nThere are direct model integrations in Phoenix and indirect model integrations (e.x. local modals) through \nLiteLLM\n.\nDirect Integrations:\nThese integrations are native to the Phoenix Evals package and have better throughput, rate limit and error management.\nVertex AI\nOpenAI\nAzure OpenAI\nAnthropic\nMistral\nPrevious\nOnline Evals\nNext\nOverview: Datasets\nLast updated \n7 days ago\n\nsource: https://docs.arize.com/phoenix/inferences/phoenix-inferences\ntitle: Quickstart: Inferences\n\nQuickstart: Inferences\nObservability for all model types (LLM, NLP, CV, Tabular)\nOverview\nPhoenix Inferences allows you to observe the performance of your model through visualizing all the model’s inferences in one interactive UMAP view.\nThis powerful visualization can be leveraged during EDA to understand model drift, find low performing clusters, uncover retrieval issues, and export data for retraining / fine tuning.\nQuickstart\nThe following Quickstart can be executed in a Jupyter notebook or Google Colab.\nWe will begin by logging just a training set. Then proceed to add a production set for comparison.\nStep 1: Install and load dependencies\nUse \npip\n or \nconda\nto install \narize-phoenix\n.\nCopy\n!pip install arize\n-\nphoenix\n\n\n\n\nimport\n phoenix \nas\n px\nStep 2: Prepare model data\nPhoenix visualizes data taken from pandas dataframe, where each row of the dataframe compasses all the information about each inference (including feature values, prediction, metadata, etc.)\nFor this Quickstart, we will show an example of visualizing the inferences from a computer vision model. See example notebooks for all model types \nhere\n.\nLet’s begin by working with the training set for this model.\nDownload the dataset and load it into a Pandas dataframe.\nCopy\nimport\n pandas \nas\n pd\n\n\n\n\ntrain_df \n=\n pd\n.\nread_parquet\n(\n\n\n    ""http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/cv/human-actions/human_actions_training.parquet""\n\n\n)\nPreview the dataframe with \ntrain_df.head()\n and note that each row contains all the data specific to this CV model for each inference.\nCopy\ntrain_df.head()\nStep 3: Define a Schema\nBefore we can log these inferences, we need to define a Schema object to describe them.\nThe Schema object informs Phoenix of the fields that the columns of the dataframe should map to.\nHere we define a Schema to describe our particular CV training set:\nCopy\n# Define Schema to indicate which columns in train_df should map to each field\n\n\ntrain_schema \n=\n px\n.\nSchema\n(\n\n\n    timestamp_column_name\n=\n""prediction_ts""\n,\n\n\n    prediction_label_column_name\n=\n""predicted_action""\n,\n\n\n    actual_label_column_name\n=\n""actual_action""\n,\n\n\n    embedding_feature_column_names\n=\n{\n\n\n        \n""image_embedding""\n: px.\nEmbeddingColumnNames\n(\n\n\n            vector_column_name\n=\n""image_vector""\n,\n\n\n            link_to_data_column_name\n=\n""url""\n,\n\n\n        ),\n\n\n    },\n\n\n)\nImportant\n:\n The fields used in a Schema will \nvary\n depending on the model type that you are working with.\nFor examples on how Schema are defined for other model types (NLP, tabular, LLM-based applications), see example notebooks under \nEmbedding Analysis\n and \nStructured Data Analysis\n.\nStep 4: Wrap into Inference object\nWrap your \ntrain_df\n and schema \ntrain_schema\n into a Phoenix \ninference\n object:\nCopy\ntrain_ds \n=\n \nInference\n(dataframe\n=\ntrain_df, schema\n=\ntrain_schema, name\n=\n""training""\n)\nStep 5: Launch Phoenix!\nWe are now ready to launch Phoenix with our Inferences!\nHere, we are passing \ntrain_ds\n as the \nprimary\n inferences, as we are only visualizing one inference set (see Step 6 for adding additional inference sets).\nCopy\nsession \n=\n px\n.\nlaunch_app\n(primary\n=\ntrain_ds)\nRunning this will fire up a Phoenix visualization. Follow in the instructions in the output to view Phoenix in a browser, or in-line in your notebook:\nCopy\n🌍 To view the Phoenix app in your browser, visit https://x0u0hsyy843-496ff2e9c6d22116-6060-colab.googleusercontent.com/\n\n\n📺 To view the Phoenix app in a notebook, run `px.active_session().view()`\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What model types are supported by Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Phoenix supports model types such as LLM, NLP, CV, and Tabular.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NTg=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing/langchain\ntitle: LangChain\n\nLangChain\nHow to use the python LangChainInstrumentor to trace LangChain and LangGraph\nPhoenix has first-class support for \nLangChain\n applications.\nLaunch Phoenix\nNotebook\nCommand Line\nDocker\napp.phoenix.arize.com\nInstall packages:\nCopy\npip\n \ninstall\n \narize-phoenix\n \nopentelemetry-sdk\n \nopentelemetry-exporter-otlp\nLaunch Phoenix:\nCopy\nimport\n phoenix \nas\n px\n\n\npx\n.\nlaunch_app\n()\nConnect your notebook to Phoenix:\nCopy\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n()\n\n\nspan_exporter \n=\n \nOTLPSpanExporter\n(\n""http://localhost:6006/v1/traces""\n)\n\n\nspan_processor \n=\n \nSimpleSpanProcessor\n(span_exporter)\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor)\n\n\ntrace_api\n.\nset_tracer_provider\n(tracer_provider)\nBy default, notebook instances do not have persistent storage, so your traces will disappear after the notebook is closed. See \nPersistence\n or use one of the other deployment options to retain traces.\nLaunch your local Phoenix instance:\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \nserve\nFor details on customizing a local terminal deployment, see \nTerminal Setup\n.\nInstall packages:\nCopy\npip\n \ninstall\n \nopentelemetry-sdk\n \nopentelemetry-exporter-otlp\nConnect your application to your instance using:\nCopy\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n()\n\n\nspan_exporter \n=\n \nOTLPSpanExporter\n(\n""http://localhost:6006/v1/traces""\n)\n\n\nspan_processor \n=\n \nSimpleSpanProcessor\n(span_exporter)\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor)\n\n\ntrace_api\n.\nset_tracer_provider\n(tracer_provider)\nSee \nQuickstart: Deployment\n for more details\nPull latest Phoenix image from \nDocker Hub\n:\nCopy\ndocker\n \npull\n \narizephoenix/phoenix:latest\nRun your containerized instance:\nCopy\ndocker\n \nrun\n \n-p\n \n6006:6006\n \narizephoenix/phoenix:latest\nThis will expose the Phoenix on \nlocalhost:6006\nInstall packages:\nCopy\npip\n \ninstall\n \nopentelemetry-sdk\n \nopentelemetry-exporter-otlp\nConnect your application to your instance using:\nCopy\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n()\n\n\nspan_exporter \n=\n \nOTLPSpanExporter\n(\n""http://localhost:6006/v1/traces""\n)\n\n\nspan_processor \n=\n \nSimpleSpanProcessor\n(span_exporter)\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor)\n\n\ntrace_api\n.\nset_tracer_provider\n(tracer_provider)\nFor more info on using Phoenix with Docker, see \nDocker\nIf you don\'t want to host an instance of Phoenix yourself or use a notebook instance, you can use a persistent instance provided on our site. Sign up for an Arize Phoenix account at\nhttps://app.phoenix.arize.com/login\nInstall packages:\nCopy\npip\n \ninstall\n \nopentelemetry-sdk\n \nopentelemetry-exporter-otlp\nConnect your application to your cloud instance:\nCopy\nimport\n os\n\n\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\nresources \nimport\n Resource\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing/langchain.js\ntitle: LangChain.js\n\nLangChain.js\nThis module provides automatic instrumentation for LangChain.js, more specifically, the @langchain/core module. which may be used in conjunction with @opentelemetry/sdk-trace-node.\nInstall\nCopy\nnpm\n \ninstall\n \n--save\n \n@arizeai/openinference-instrumentation-langchain\nSetup\nTo load the LangChain instrumentation, manually instrument the \n@langchain/core/callbacks/manager\n module. The callbacks manager must be manually instrumented due to the non-traditional module structure in \n@langchain/core\n. Additional instrumentations can be registered as usual in the registerInstrumentations function.\nCopy\nimport\n { NodeTracerProvider } \nfrom\n \n""@opentelemetry/sdk-trace-node""\n;\n\n\nimport\n { \n\n\n  LangChainInstrumentation \n\n\n} \nfrom\n \n""@arizeai/openinference-instrumentation-langchain""\n;\n\n\nimport\n \n*\n \nas\n CallbackManagerModule \nfrom\n \n""@langchain/core/callbacks/manager""\n;\n\n\n\n\nconst\n \nprovider\n \n=\n \nnew\n \nNodeTracerProvider\n();\n\n\nprovider\n.register\n();\n\n\n\n\nconst\n \nlcInstrumentation\n \n=\n \nnew\n \nLangChainInstrumentation\n();\n\n\n// LangChain must be manually instrumented as it doesn\'t have \n\n\n// a traditional module structure\n\n\nlcInstrumentation\n.manuallyInstrument\n(CallbackManagerModule);\n\n\nResources\nExample project\nOpenInference package\nPrevious\nOpenAI Node SDK\nNext\nConcepts: Tracing\nLast updated \n20 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can you make sure that chains are going to be nested? \nAnswer: '}, {'role': 'assistant', 'content': 'Manually instrument the LangChain module to ensure that chains are nested.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing/langchain\ntitle: LangChain\n\nLangChain\nHow to use the python LangChainInstrumentor to trace LangChain and LangGraph\nPhoenix has first-class support for \nLangChain\n applications.\nLaunch Phoenix\nNotebook\nCommand Line\nDocker\napp.phoenix.arize.com\nInstall packages:\nCopy\npip\n \ninstall\n \narize-phoenix\n \nopentelemetry-sdk\n \nopentelemetry-exporter-otlp\nLaunch Phoenix:\nCopy\nimport\n phoenix \nas\n px\n\n\npx\n.\nlaunch_app\n()\nConnect your notebook to Phoenix:\nCopy\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n()\n\n\nspan_exporter \n=\n \nOTLPSpanExporter\n(\n""http://localhost:6006/v1/traces""\n)\n\n\nspan_processor \n=\n \nSimpleSpanProcessor\n(span_exporter)\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor)\n\n\ntrace_api\n.\nset_tracer_provider\n(tracer_provider)\nBy default, notebook instances do not have persistent storage, so your traces will disappear after the notebook is closed. See \nPersistence\n or use one of the other deployment options to retain traces.\nLaunch your local Phoenix instance:\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \nserve\nFor details on customizing a local terminal deployment, see \nTerminal Setup\n.\nInstall packages:\nCopy\npip\n \ninstall\n \nopentelemetry-sdk\n \nopentelemetry-exporter-otlp\nConnect your application to your instance using:\nCopy\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n()\n\n\nspan_exporter \n=\n \nOTLPSpanExporter\n(\n""http://localhost:6006/v1/traces""\n)\n\n\nspan_processor \n=\n \nSimpleSpanProcessor\n(span_exporter)\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor)\n\n\ntrace_api\n.\nset_tracer_provider\n(tracer_provider)\nSee \nQuickstart: Deployment\n for more details\nPull latest Phoenix image from \nDocker Hub\n:\nCopy\ndocker\n \npull\n \narizephoenix/phoenix:latest\nRun your containerized instance:\nCopy\ndocker\n \nrun\n \n-p\n \n6006:6006\n \narizephoenix/phoenix:latest\nThis will expose the Phoenix on \nlocalhost:6006\nInstall packages:\nCopy\npip\n \ninstall\n \nopentelemetry-sdk\n \nopentelemetry-exporter-otlp\nConnect your application to your instance using:\nCopy\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n()\n\n\nspan_exporter \n=\n \nOTLPSpanExporter\n(\n""http://localhost:6006/v1/traces""\n)\n\n\nspan_processor \n=\n \nSimpleSpanProcessor\n(span_exporter)\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor)\n\n\ntrace_api\n.\nset_tracer_provider\n(tracer_provider)\nFor more info on using Phoenix with Docker, see \nDocker\nIf you don\'t want to host an instance of Phoenix yourself or use a notebook instance, you can use a persistent instance provided on our site. Sign up for an Arize Phoenix account at\nhttps://app.phoenix.arize.com/login\nInstall packages:\nCopy\npip\n \ninstall\n \nopentelemetry-sdk\n \nopentelemetry-exporter-otlp\nConnect your application to your cloud instance:\nCopy\nimport\n os\n\n\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\nresources \nimport\n Resource\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing/langchain.js\ntitle: LangChain.js\n\nLangChain.js\nThis module provides automatic instrumentation for LangChain.js, more specifically, the @langchain/core module. which may be used in conjunction with @opentelemetry/sdk-trace-node.\nInstall\nCopy\nnpm\n \ninstall\n \n--save\n \n@arizeai/openinference-instrumentation-langchain\nSetup\nTo load the LangChain instrumentation, manually instrument the \n@langchain/core/callbacks/manager\n module. The callbacks manager must be manually instrumented due to the non-traditional module structure in \n@langchain/core\n. Additional instrumentations can be registered as usual in the registerInstrumentations function.\nCopy\nimport\n { NodeTracerProvider } \nfrom\n \n""@opentelemetry/sdk-trace-node""\n;\n\n\nimport\n { \n\n\n  LangChainInstrumentation \n\n\n} \nfrom\n \n""@arizeai/openinference-instrumentation-langchain""\n;\n\n\nimport\n \n*\n \nas\n CallbackManagerModule \nfrom\n \n""@langchain/core/callbacks/manager""\n;\n\n\n\n\nconst\n \nprovider\n \n=\n \nnew\n \nNodeTracerProvider\n();\n\n\nprovider\n.register\n();\n\n\n\n\nconst\n \nlcInstrumentation\n \n=\n \nnew\n \nLangChainInstrumentation\n();\n\n\n// LangChain must be manually instrumented as it doesn\'t have \n\n\n// a traditional module structure\n\n\nlcInstrumentation\n.manuallyInstrument\n(CallbackManagerModule);\n\n\nResources\nExample project\nOpenInference package\nPrevious\nOpenAI Node SDK\nNext\nConcepts: Tracing\nLast updated \n20 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can you make sure that chains are going to be nested? \nAnswer: '}, {'role': 'assistant', 'content': 'Manually instrument the LangChain module to ensure that chains are nested.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NTk=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/building-your-own-evals\ntitle: Custom Task Evaluation\n\nCustom Task Evaluation\nCustomize Your Own Eval Templates\nThe LLM Evals library is designed to support the building of any custom Eval templates.\nSteps to Building Your Own Eval\nFollow the following steps to easily build your own Eval with Phoenix\n1. Choose a Metric\nTo do that, you must identify what is the \nmetric best suited for your use case\n. Can you use a pre-existing template or do you need to evaluate something unique to your use case?\n2. Build a Golden Dataset\nThen, you need the \ngolden dataset\n. This should be representative of the type of data you expect the LLM eval to see. The golden dataset should have the “ground truth” label so that we can measure performance of the LLM eval template. Often such labels come from human feedback.\nBuilding such a dataset is laborious, but you can often find a standardized one for the most common use cases (as we did in the code above)\nThe Eval inferences are designed or easy benchmarking and pre-set downloadable test inferences. The inferences are pre-tested, many are hand crafted and designed for testing specific Eval tasks.\nCopy\nfrom\n phoenix\n.\nevals \nimport\n download_benchmark_dataset\n\n\n\n\ndf \n=\n \ndownload_benchmark_dataset\n(\n\n\n    task\n=\n""binary-hallucination-classification""\n, dataset_name\n=\n""halueval_qa_data""\n\n\n)\n\n\ndf\n.\nhead\n()\n3. Decide Which LLM to use For Evaluation\nThen you need to decide \nwhich LLM\n you want to use for evaluation. This could be a different LLM from the one you are using for your application. For example, you may be using Llama for your application and GPT-4 for your eval. Often this choice is influenced by questions of cost and accuracy.\n4. Build the Eval Template\nNow comes the core component that we are trying to benchmark and improve: the \neval template\n.\nYou can adjust an existing template or build your own from scratch.\nBe explicit about the following:\nWhat is the input?\n In our example, it is the documents/context that was retrieved and the query from the user.\nWhat are we asking?\n In our example, we’re asking the LLM to tell us if the document was relevant to the query\nWhat are the possible output formats?\n In our example, it is binary relevant/irrelevant, but it can also be multi-class (e.g., fully relevant, partially relevant, not relevant).\nIn order to create a new template all that is needed is the setting of the input string to the Eval function.\nCopy\nMY_CUSTOM_TEMPLATE \n=\n \n\'\'\'\n\n\n    You are evaluating the positivity or negativity of the responses to questions.\n\n\n    [BEGIN DATA]\n\n\n    ************\n\n\n    [Question]: \n{question}\n\n\n    ************\n\n\n    [Response]: \n{response}\n\n\n    [END DATA]\n\n\n\n\n\n\n    Please focus on the tone of the response.\n\n\n    Your answer must be single word, either ""positive"" or ""negative""\n\n\n    \'\'\'\nThe above template shows an example creation of an easy to use string template. The Phoenix Eval templates support both strings and objects.\nCopy\nmodel \n=\n \nOpenAIModel\n(model_name\n=\n""gpt-4""\n,temperature\n=\n0.6\n)\n\n\npositive_eval \n=\n \nllm_classify\n(\n\n\n    dataframe\n=\ndf,\n\n\n    template\n=\n MY_CUSTOM_TEMPLATE,\n\n\n    model\n=\nmodel\n\n\n)\nThe above example shows a use of the custom created template on the df dataframe.\nCopy\n#Phoenix Evals support using either strings or objects as templates\n\n\nMY_CUSTOM_TEMPLATE \n=\n \n"" ...""\n\n\nMY_CUSTOM_TEMPLATE \n=\n \nPromptTemplate\n(\n""This is a test \n{prompt}\n""\n)\n5. Run Eval on your Golden Dataset and Benchmark Performance\nYou now need to run the eval across your golden dataset. Then you can \ngenerate metrics\n (overall accuracy, precision, recall, F1, etc.) to determine the benchmark. It is important to look at more than just overall accuracy. We’ll discuss that below in more detail.\nPrevious\nEvaluators\nNext\nHow to: Evals\nLast updated \n1 month ago\n\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/how-to-experiments\ntitle: How-to: Experiments\n\nHow-to: Experiments\nHow to run experiments\nHow to upload a Dataset\nHow to run a custom task\nHow to configure evaluators\nHow to run the experiment\nHow to use evaluators \nLLM Evaluators\nCode Evaluators\nCustom Evaluators\nPrevious\nExporting Datasets\nNext\nRun Experiments\nLast updated \n1 month ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Explain the process of setting up custom evaluations in Arize Phoenix for a specific LLM task. How can the results from these evaluations inform model fine-tuning and dataset adjustments?\nAnswer: '}, {'role': 'assistant', 'content': ""To set up custom evaluations in Arize Phoenix for a specific LLM task, you first need to choose a metric that best suits your use case. Then, you should build a golden dataset that is representative of the data the LLM evaluation will encounter, with ground truth labels for performance measurement. Next, decide which LLM to use for evaluation, which can differ from the one used in your application. After that, build the evaluation template by specifying the input, the question being asked, and the possible output formats.\n\nOnce the custom evaluation template is set up, you can run the evaluation on your golden dataset to generate metrics such as overall accuracy, precision, recall, and F1 score. These metrics provide insights into the performance of the LLM model and can inform decisions regarding model fine-tuning and dataset adjustments. By analyzing these evaluation results, you can identify areas where the model may need improvement, adjust the dataset to better align with the model's requirements, and iteratively refine the LLM for better performance.""}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/building-your-own-evals\ntitle: Custom Task Evaluation\n\nCustom Task Evaluation\nCustomize Your Own Eval Templates\nThe LLM Evals library is designed to support the building of any custom Eval templates.\nSteps to Building Your Own Eval\nFollow the following steps to easily build your own Eval with Phoenix\n1. Choose a Metric\nTo do that, you must identify what is the \nmetric best suited for your use case\n. Can you use a pre-existing template or do you need to evaluate something unique to your use case?\n2. Build a Golden Dataset\nThen, you need the \ngolden dataset\n. This should be representative of the type of data you expect the LLM eval to see. The golden dataset should have the “ground truth” label so that we can measure performance of the LLM eval template. Often such labels come from human feedback.\nBuilding such a dataset is laborious, but you can often find a standardized one for the most common use cases (as we did in the code above)\nThe Eval inferences are designed or easy benchmarking and pre-set downloadable test inferences. The inferences are pre-tested, many are hand crafted and designed for testing specific Eval tasks.\nCopy\nfrom\n phoenix\n.\nevals \nimport\n download_benchmark_dataset\n\n\n\n\ndf \n=\n \ndownload_benchmark_dataset\n(\n\n\n    task\n=\n""binary-hallucination-classification""\n, dataset_name\n=\n""halueval_qa_data""\n\n\n)\n\n\ndf\n.\nhead\n()\n3. Decide Which LLM to use For Evaluation\nThen you need to decide \nwhich LLM\n you want to use for evaluation. This could be a different LLM from the one you are using for your application. For example, you may be using Llama for your application and GPT-4 for your eval. Often this choice is influenced by questions of cost and accuracy.\n4. Build the Eval Template\nNow comes the core component that we are trying to benchmark and improve: the \neval template\n.\nYou can adjust an existing template or build your own from scratch.\nBe explicit about the following:\nWhat is the input?\n In our example, it is the documents/context that was retrieved and the query from the user.\nWhat are we asking?\n In our example, we’re asking the LLM to tell us if the document was relevant to the query\nWhat are the possible output formats?\n In our example, it is binary relevant/irrelevant, but it can also be multi-class (e.g., fully relevant, partially relevant, not relevant).\nIn order to create a new template all that is needed is the setting of the input string to the Eval function.\nCopy\nMY_CUSTOM_TEMPLATE \n=\n \n\'\'\'\n\n\n    You are evaluating the positivity or negativity of the responses to questions.\n\n\n    [BEGIN DATA]\n\n\n    ************\n\n\n    [Question]: \n{question}\n\n\n    ************\n\n\n    [Response]: \n{response}\n\n\n    [END DATA]\n\n\n\n\n\n\n    Please focus on the tone of the response.\n\n\n    Your answer must be single word, either ""positive"" or ""negative""\n\n\n    \'\'\'\nThe above template shows an example creation of an easy to use string template. The Phoenix Eval templates support both strings and objects.\nCopy\nmodel \n=\n \nOpenAIModel\n(model_name\n=\n""gpt-4""\n,temperature\n=\n0.6\n)\n\n\npositive_eval \n=\n \nllm_classify\n(\n\n\n    dataframe\n=\ndf,\n\n\n    template\n=\n MY_CUSTOM_TEMPLATE,\n\n\n    model\n=\nmodel\n\n\n)\nThe above example shows a use of the custom created template on the df dataframe.\nCopy\n#Phoenix Evals support using either strings or objects as templates\n\n\nMY_CUSTOM_TEMPLATE \n=\n \n"" ...""\n\n\nMY_CUSTOM_TEMPLATE \n=\n \nPromptTemplate\n(\n""This is a test \n{prompt}\n""\n)\n5. Run Eval on your Golden Dataset and Benchmark Performance\nYou now need to run the eval across your golden dataset. Then you can \ngenerate metrics\n (overall accuracy, precision, recall, F1, etc.) to determine the benchmark. It is important to look at more than just overall accuracy. We’ll discuss that below in more detail.\nPrevious\nEvaluators\nNext\nHow to: Evals\nLast updated \n1 month ago\n\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/how-to-experiments\ntitle: How-to: Experiments\n\nHow-to: Experiments\nHow to run experiments\nHow to upload a Dataset\nHow to run a custom task\nHow to configure evaluators\nHow to run the experiment\nHow to use evaluators \nLLM Evaluators\nCode Evaluators\nCustom Evaluators\nPrevious\nExporting Datasets\nNext\nRun Experiments\nLast updated \n1 month ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Explain the process of setting up custom evaluations in Arize Phoenix for a specific LLM task. How can the results from these evaluations inform model fine-tuning and dataset adjustments?\nAnswer: '}, {'role': 'assistant', 'content': ""To set up custom evaluations in Arize Phoenix for a specific LLM task, you first need to choose a metric that best suits your use case. Then, you should build a golden dataset that is representative of the data the LLM evaluation will encounter, with ground truth labels for performance measurement. Next, decide which LLM to use for evaluation, which can differ from the one used in your application. After that, build the evaluation template by specifying the input, the question being asked, and the possible output formats.\n\nOnce the custom evaluation template is set up, you can run the evaluation on your golden dataset to generate metrics such as overall accuracy, precision, recall, and F1 score. These metrics provide insights into the performance of the LLM model and can inform decisions regarding model fine-tuning and dataset adjustments. By analyzing these evaluation results, you can identify areas where the model may need improvement, adjust the dataset to better align with the model's requirements, and iteratively refine the LLM for better performance.""}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NjA=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/quickstart\ntitle: Quickstart\n\nQuickstart\nThe easiest way to run Phoenix is locally in your own computer. To launch Phoenix, use the following steps.\nInstall\nUsing pip\nUsing conda\nDocker\napp.phoenix.arize.com\nCopy\npip\n \ninstall\n \narize-phoenix\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix\nPhoenix server images are available via \nDocker Hub\n and can be used via \ndocker compose \nor if you simply want a long-running phoenix instance to share with your team.\nCopy\ndocker\n \npull\n \narizephoenix/phoenix:latest\nIf you don\'t want to host an instance of Phoenix yourself or use a notebook instance, you can use a persistent instance provided on our site. Sign up for an Arize Phoenix account at \nhttps://app.phoenix.arize.com/login\nFor more details, see \nHosted Phoenix\nLaunch Phoenix\nLaunching phoenix can be done in many ways depending on your use-case.\nCommand Line\nDocker\nNotebook\napp.phoenix.arize.com\nLaunch your local Phoenix instance using:\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \nserve\nFor details on customizing a local terminal deployment, see \nTerminal Setup\n.\nLaunch the phoenix docker image using:\nCopy\ndocker\n \nrun\n \n-p\n \n6006:6006\n \n-p\n \n4317:4317\n \narizephoenix/phoenix:latest\nThis will expose the Phoenix UI and REST API on \nlocalhost:6006\n and exposes the gRPC endpoint for spans on \nlocalhost:4317\nFor more details on customizing a docker deployment, see \nDocker\nWithin your notebook, launch Phoenix using:\nCopy\nimport\n phoenix \nas\n px\n\n\npx\n.\nlaunch_app\n()\nBy default, notebook instances do not have persistent storage, so your traces will disappear after the notebook is closed. See \nPersistence\n or use one of the other deployment options to retain traces.\nHosted Phoenix instances are always online. Nothing more to do here!\nConnect your App\nTo collect traces from your application, you must configure an OpenTelemetry TracerProvider to send traces to Phoenix. The \nregister\n utility from the \nphoenix.otel\n module streamlines this process.\nPython\nTypeScript\nNotebook\napp.phoenix.arize.com\nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your instance using:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n  \nYou do not have to use phoenix.otel to connect to your phoenix instance, you can use OpenTelemetry itself to initialize  your OTEL connection. See\nUsing OTEL Python Directly\nSee \nSetup Tracing: Python\n for more details on configuration and setup\nFor setting up tracing and OpenTelemetry with TypeScript, see \nSetup Tracing: TS\nConnect your notebook to Phoenix:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n \nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your cloud instance using:\nCopy\nimport\n os\n\n\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# Add Phoenix API Key for tracing\n\n\nos\n.\nenviron\n[\n""OTEL_EXPORTER_OTLP_HEADERS""\n]\n \n=\n f\n""api_key=\n{PHOENIX_API_KEY}\n""\n\n\n\n\n# configure the Phoenix tracer\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""https://app.phoenix.arize.com/v1/traces""\n,\n\n\n)\n \nYour Phoenix API key can be found on the Keys section of your dashboard.\nNext Steps\nTrace\n a running application\nRun \nevaluations\n on traces\nTest changes to you prompts, models, and application via \nexperiments\nPrevious\nArize Phoenix\nNext\nHosted Phoenix\nLast updated \n9 hours ago\n\nsource: https://docs.arize.com/phoenix/setup\ntitle: Setup\n\nSetup\nHow to get started with Phoenix\nHow to run in different environments\nHosted Phoenix by Arize\nIn a python notebook\nFrom the command line\nIn a Container\nHow to configure phoenix\nAvailable ports to run phoenix\nCustomize phoenix using environment variables\nPrevious\nExamples\nNext\nEnvironments\nLast updated \n8 hours ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What configuration steps are needed to set up Arize Phoenix for a new project?\nAnswer: '}, {'role': 'assistant', 'content': 'To set up Arize Phoenix for a new project, you need to install the necessary packages using pip or conda, connect your application to your Phoenix instance by configuring an OpenTelemetry TracerProvider, and ensure that your traces are sent to Phoenix using the appropriate endpoint. Additionally, you can customize the configuration by setting environment variables and specifying the project name for your application.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/quickstart\ntitle: Quickstart\n\nQuickstart\nThe easiest way to run Phoenix is locally in your own computer. To launch Phoenix, use the following steps.\nInstall\nUsing pip\nUsing conda\nDocker\napp.phoenix.arize.com\nCopy\npip\n \ninstall\n \narize-phoenix\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix\nPhoenix server images are available via \nDocker Hub\n and can be used via \ndocker compose \nor if you simply want a long-running phoenix instance to share with your team.\nCopy\ndocker\n \npull\n \narizephoenix/phoenix:latest\nIf you don\'t want to host an instance of Phoenix yourself or use a notebook instance, you can use a persistent instance provided on our site. Sign up for an Arize Phoenix account at \nhttps://app.phoenix.arize.com/login\nFor more details, see \nHosted Phoenix\nLaunch Phoenix\nLaunching phoenix can be done in many ways depending on your use-case.\nCommand Line\nDocker\nNotebook\napp.phoenix.arize.com\nLaunch your local Phoenix instance using:\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \nserve\nFor details on customizing a local terminal deployment, see \nTerminal Setup\n.\nLaunch the phoenix docker image using:\nCopy\ndocker\n \nrun\n \n-p\n \n6006:6006\n \n-p\n \n4317:4317\n \narizephoenix/phoenix:latest\nThis will expose the Phoenix UI and REST API on \nlocalhost:6006\n and exposes the gRPC endpoint for spans on \nlocalhost:4317\nFor more details on customizing a docker deployment, see \nDocker\nWithin your notebook, launch Phoenix using:\nCopy\nimport\n phoenix \nas\n px\n\n\npx\n.\nlaunch_app\n()\nBy default, notebook instances do not have persistent storage, so your traces will disappear after the notebook is closed. See \nPersistence\n or use one of the other deployment options to retain traces.\nHosted Phoenix instances are always online. Nothing more to do here!\nConnect your App\nTo collect traces from your application, you must configure an OpenTelemetry TracerProvider to send traces to Phoenix. The \nregister\n utility from the \nphoenix.otel\n module streamlines this process.\nPython\nTypeScript\nNotebook\napp.phoenix.arize.com\nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your instance using:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n  \nYou do not have to use phoenix.otel to connect to your phoenix instance, you can use OpenTelemetry itself to initialize  your OTEL connection. See\nUsing OTEL Python Directly\nSee \nSetup Tracing: Python\n for more details on configuration and setup\nFor setting up tracing and OpenTelemetry with TypeScript, see \nSetup Tracing: TS\nConnect your notebook to Phoenix:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n \nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your cloud instance using:\nCopy\nimport\n os\n\n\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# Add Phoenix API Key for tracing\n\n\nos\n.\nenviron\n[\n""OTEL_EXPORTER_OTLP_HEADERS""\n]\n \n=\n f\n""api_key=\n{PHOENIX_API_KEY}\n""\n\n\n\n\n# configure the Phoenix tracer\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""https://app.phoenix.arize.com/v1/traces""\n,\n\n\n)\n \nYour Phoenix API key can be found on the Keys section of your dashboard.\nNext Steps\nTrace\n a running application\nRun \nevaluations\n on traces\nTest changes to you prompts, models, and application via \nexperiments\nPrevious\nArize Phoenix\nNext\nHosted Phoenix\nLast updated \n9 hours ago\n\nsource: https://docs.arize.com/phoenix/setup\ntitle: Setup\n\nSetup\nHow to get started with Phoenix\nHow to run in different environments\nHosted Phoenix by Arize\nIn a python notebook\nFrom the command line\nIn a Container\nHow to configure phoenix\nAvailable ports to run phoenix\nCustomize phoenix using environment variables\nPrevious\nExamples\nNext\nEnvironments\nLast updated \n8 hours ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What configuration steps are needed to set up Arize Phoenix for a new project?\nAnswer: '}, {'role': 'assistant', 'content': 'To set up Arize Phoenix for a new project, you need to install the necessary packages using pip or conda, connect your application to your Phoenix instance by configuring an OpenTelemetry TracerProvider, and ensure that your traces are sent to Phoenix using the appropriate endpoint. Additionally, you can customize the configuration by setting environment variables and specifying the project name for your application.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NjE=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/integrations/arize/bring-production-data-to-notebook-for-eda-or-retraining\ntitle: Export Data from Arize to Phoenix\n\nExport Data from Arize to Phoenix\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nOftentimes, the team that notices an issue in their model, for example a prompt/response LLM model, may not be the same team that continues the investigations or kicks off retraining workflows.\nTo help connect teams and workflows, Phoenix enables continued analysis of production data from \nArize\n in a notebook environment for fine tuning workflows.\nFor example, a user may have noticed in \nArize\n that this prompt template is not performing well.\nWith a few lines of Python code, users can export this data into Phoenix for further analysis. This allows team members, such as data scientists, who may not have access to production data today, an easy way to access relevant product data for further analysis in an environment they are familiar with.\nThey can then easily augment and fine tune the data and verify improved performance, before deploying back to production.\nThere are two ways export data out of \nArize\n for further investigation:\nThe easiest way is to click the export button on the Embeddings and Inferences pages. This will produce a code snippet that you can copy into a Python environment and install Phoenix. This code snippet will include the date range you have selected in the \nArize\n platform, in addition to the inferences you have selected.\nUsers can also query \nArize\n for data directly using the Arize Python export client. We recommend doing this once you're more comfortable with the in-platform export functionality, as you will need to manually enter in the data ranges and data you want to export.\nCopy\nos\n.\nenviron\n[\n'ARIZE_API_KEY'\n]\n \n=\n ARIZE_API_KEY\n\n\n\n\nfrom\n datetime \nimport\n datetime\n\n\n\n\nfrom\n arize\n.\nexporter \nimport\n ArizeExportClient\n\n\nfrom\n arize\n.\nutils\n.\ntypes \nimport\n Environments\n\n\n\n\nclient \n=\n \nArizeExportClient\n()\n\n\n\n\nprimary_df \n=\n client\n.\nexport_model_to_df\n(\n\n\n    space_id\n=\n'U3BhY2U6NzU0'\n,\n\n\n    model_name\n=\n'test_home_prices_LLM'\n,\n\n\n    environment\n=\nEnvironments.PRODUCTION,\n\n\n    start_time\n=\ndatetime.\nfromisoformat\n(\n'2023-02-11T07:00:00.000+00:00'\n),\n\n\n    end_time\n=\ndatetime.\nfromisoformat\n(\n'2023-03-14T00:59:59.999+00:00'\n),\n\n\n)\nTest out this workflow by signing up for a \nfree Arize account\n.\nPrevious\nArize\nNext\nRagas\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/integrations/arize\ntitle: Arize\n\nArize\nPhoenix works hand-in-hand with Arize, it's enterprise counterpart\nShare Embeddings Data with Arize\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nPrevious\nModels\nNext\nExport Data from Arize to Phoenix\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can datasets be exported from Arize Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'Datasets can be exported from Arize Phoenix by clicking the export button on the Embeddings and Inferences pages, which will generate a code snippet to be copied into a Python environment with Phoenix installed. Another way is to query Arize directly using the Arize Python export client, where users can manually enter the data ranges and data they want to export.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/integrations/arize/bring-production-data-to-notebook-for-eda-or-retraining\ntitle: Export Data from Arize to Phoenix\n\nExport Data from Arize to Phoenix\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nOftentimes, the team that notices an issue in their model, for example a prompt/response LLM model, may not be the same team that continues the investigations or kicks off retraining workflows.\nTo help connect teams and workflows, Phoenix enables continued analysis of production data from \nArize\n in a notebook environment for fine tuning workflows.\nFor example, a user may have noticed in \nArize\n that this prompt template is not performing well.\nWith a few lines of Python code, users can export this data into Phoenix for further analysis. This allows team members, such as data scientists, who may not have access to production data today, an easy way to access relevant product data for further analysis in an environment they are familiar with.\nThey can then easily augment and fine tune the data and verify improved performance, before deploying back to production.\nThere are two ways export data out of \nArize\n for further investigation:\nThe easiest way is to click the export button on the Embeddings and Inferences pages. This will produce a code snippet that you can copy into a Python environment and install Phoenix. This code snippet will include the date range you have selected in the \nArize\n platform, in addition to the inferences you have selected.\nUsers can also query \nArize\n for data directly using the Arize Python export client. We recommend doing this once you're more comfortable with the in-platform export functionality, as you will need to manually enter in the data ranges and data you want to export.\nCopy\nos\n.\nenviron\n[\n'ARIZE_API_KEY'\n]\n \n=\n ARIZE_API_KEY\n\n\n\n\nfrom\n datetime \nimport\n datetime\n\n\n\n\nfrom\n arize\n.\nexporter \nimport\n ArizeExportClient\n\n\nfrom\n arize\n.\nutils\n.\ntypes \nimport\n Environments\n\n\n\n\nclient \n=\n \nArizeExportClient\n()\n\n\n\n\nprimary_df \n=\n client\n.\nexport_model_to_df\n(\n\n\n    space_id\n=\n'U3BhY2U6NzU0'\n,\n\n\n    model_name\n=\n'test_home_prices_LLM'\n,\n\n\n    environment\n=\nEnvironments.PRODUCTION,\n\n\n    start_time\n=\ndatetime.\nfromisoformat\n(\n'2023-02-11T07:00:00.000+00:00'\n),\n\n\n    end_time\n=\ndatetime.\nfromisoformat\n(\n'2023-03-14T00:59:59.999+00:00'\n),\n\n\n)\nTest out this workflow by signing up for a \nfree Arize account\n.\nPrevious\nArize\nNext\nRagas\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/integrations/arize\ntitle: Arize\n\nArize\nPhoenix works hand-in-hand with Arize, it's enterprise counterpart\nShare Embeddings Data with Arize\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nPrevious\nModels\nNext\nExport Data from Arize to Phoenix\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can datasets be exported from Arize Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'Datasets can be exported from Arize Phoenix by clicking the export button on the Embeddings and Inferences pages, which will generate a code snippet to be copied into a Python environment with Phoenix installed. Another way is to query Arize directly using the Arize Python export client, where users can manually enter the data ranges and data they want to export.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NjI=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix's tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you'll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they're geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix's search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix's flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix's evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n\nsource: https://docs.arize.com/phoenix/evaluation/evaluation-models\ntitle: Evaluation Models\n\nEvaluation Models\narize-phoenix-evals\n supports a large set of foundation models for Evals such as:\nOpenAI\nVertex AI\nAzure Open AI\nAnthropic\nMixtral/Mistral\nAWS Bedrock\nFalcon\nCode Llama\nLlama3\nDeepseek\nDeberta\nDBRX\nQwen\nAnd many more.\nThere are direct model integrations in Phoenix and indirect model integrations (e.x. local modals) through \nLiteLLM\n.\nDirect Integrations:\nThese integrations are native to the Phoenix Evals package and have better throughput, rate limit and error management.\nVertex AI\nOpenAI\nAzure OpenAI\nAnthropic\nMistral\nPrevious\nOnline Evals\nNext\nOverview: Datasets\nLast updated \n7 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Which LLM providers are supported by Arize Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'Arize Phoenix supports a variety of LLM providers such as OpenAI, Vertex AI, Azure Open AI, Anthropic, Mixtral/Mistral, AWS Bedrock, Falcon, Code Llama, Llama3, Deepseek, Deberta, DBRX, and Qwen, among others.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix's tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you'll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they're geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix's search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix's flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix's evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n\nsource: https://docs.arize.com/phoenix/evaluation/evaluation-models\ntitle: Evaluation Models\n\nEvaluation Models\narize-phoenix-evals\n supports a large set of foundation models for Evals such as:\nOpenAI\nVertex AI\nAzure Open AI\nAnthropic\nMixtral/Mistral\nAWS Bedrock\nFalcon\nCode Llama\nLlama3\nDeepseek\nDeberta\nDBRX\nQwen\nAnd many more.\nThere are direct model integrations in Phoenix and indirect model integrations (e.x. local modals) through \nLiteLLM\n.\nDirect Integrations:\nThese integrations are native to the Phoenix Evals package and have better throughput, rate limit and error management.\nVertex AI\nOpenAI\nAzure OpenAI\nAnthropic\nMistral\nPrevious\nOnline Evals\nNext\nOverview: Datasets\nLast updated \n7 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Which LLM providers are supported by Arize Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'Arize Phoenix supports a variety of LLM providers such as OpenAI, Vertex AI, Azure Open AI, Anthropic, Mixtral/Mistral, AWS Bedrock, Falcon, Code Llama, Llama3, Deepseek, Deberta, DBRX, and Qwen, among others.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NjM=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals\ntitle: How to: Evals\n\nHow to: Evals\nPhoenix Evaluators\nHallucinations\nQ&A on Retrieved Data\nRetrieval (RAG) Relevance\nSummarization \nCode Generation\nToxicity\n \nAI vs Human \nReference (Citation) Eval\nBring Your Own Evaluator\nCategorical evaluator\n (llm_classify)\nNumeric evaluator\n (llm_generate)\nOnline Evals\nRun evaluations via a job to visualize in the UI as traces stream in.\nPrevious\nCustom Task Evaluation\nNext\nUse Phoenix Evaluators\nLast updated \n5 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals\ntitle: Concepts: Evals\n\nConcepts: Evals\nHow to leverage Evals to validate and monitor your application\nLLM as a Judge\nEval Data Types\nEvals With Explanations\nEvaluators\nRetrieval Evaluations\nResponse Evaluations\nEvaluation Results\nEvaluating Traces\nCustom Task Evaluations\nPrevious\nQuickstart: Evals\nNext\nLLM as a Judge\nLast updated \n29 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can users perform custom task evaluations in Arize Phoenix? \nAnswer: '}, {'role': 'assistant', 'content': 'Users can perform custom task evaluations in Arize Phoenix by utilizing the feature called ""Custom Task Evaluation.""'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals\ntitle: How to: Evals\n\nHow to: Evals\nPhoenix Evaluators\nHallucinations\nQ&A on Retrieved Data\nRetrieval (RAG) Relevance\nSummarization \nCode Generation\nToxicity\n \nAI vs Human \nReference (Citation) Eval\nBring Your Own Evaluator\nCategorical evaluator\n (llm_classify)\nNumeric evaluator\n (llm_generate)\nOnline Evals\nRun evaluations via a job to visualize in the UI as traces stream in.\nPrevious\nCustom Task Evaluation\nNext\nUse Phoenix Evaluators\nLast updated \n5 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals\ntitle: Concepts: Evals\n\nConcepts: Evals\nHow to leverage Evals to validate and monitor your application\nLLM as a Judge\nEval Data Types\nEvals With Explanations\nEvaluators\nRetrieval Evaluations\nResponse Evaluations\nEvaluation Results\nEvaluating Traces\nCustom Task Evaluations\nPrevious\nQuickstart: Evals\nNext\nLLM as a Judge\nLast updated \n29 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can users perform custom task evaluations in Arize Phoenix? \nAnswer: '}, {'role': 'assistant', 'content': 'Users can perform custom task evaluations in Arize Phoenix by utilizing the feature called ""Custom Task Evaluation.""'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NjQ=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n\nsource: https://docs.arize.com/phoenix/integrations/arize\ntitle: Arize\n\nArize\nPhoenix works hand-in-hand with Arize, it's enterprise counterpart\nShare Embeddings Data with Arize\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nPrevious\nModels\nNext\nExport Data from Arize to Phoenix\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can users join the Arize Phoenix community for support?\nAnswer: ""}, {'role': 'assistant', 'content': 'Users can join the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers for support.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n\nsource: https://docs.arize.com/phoenix/integrations/arize\ntitle: Arize\n\nArize\nPhoenix works hand-in-hand with Arize, it's enterprise counterpart\nShare Embeddings Data with Arize\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nPrevious\nModels\nNext\nExport Data from Arize to Phoenix\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can users join the Arize Phoenix community for support?\nAnswer: ""}, {'role': 'assistant', 'content': 'Users can join the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers for support.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NjU=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n\nsource: https://docs.arize.com/phoenix/integrations/arize\ntitle: Arize\n\nArize\nPhoenix works hand-in-hand with Arize, it's enterprise counterpart\nShare Embeddings Data with Arize\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nPrevious\nModels\nNext\nExport Data from Arize to Phoenix\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What makes Arize Phoenix vendor-agnostic?\nAnswer: ""}, {'role': 'assistant', 'content': 'Arize Phoenix is vendor-agnostic because it works with OpenTelemetry and OpenInference instrumentation, allowing it to be compatible with various environments and deployment setups regardless of the specific vendors or platforms being used.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n\nsource: https://docs.arize.com/phoenix/integrations/arize\ntitle: Arize\n\nArize\nPhoenix works hand-in-hand with Arize, it's enterprise counterpart\nShare Embeddings Data with Arize\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nPrevious\nModels\nNext\nExport Data from Arize to Phoenix\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What makes Arize Phoenix vendor-agnostic?\nAnswer: ""}, {'role': 'assistant', 'content': 'Arize Phoenix is vendor-agnostic because it works with OpenTelemetry and OpenInference instrumentation, allowing it to be compatible with various environments and deployment setups regardless of the specific vendors or platforms being used.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NjY=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/integrations/ragas\ntitle: Ragas\n\nRagas\nPhoenix and Ragas work hand-in-hand\nBuilding a baseline for a RAG pipeline is not usually difficult, but enhancing it to make it suitable for production and ensuring the quality of your responses is almost always hard. Choosing the right tools and parameters for RAG can itself be challenging when there is an abundance of options available. With Ragas and Phoenix you create a robust workflow for making the right choices while building your RAG and ensuring its quality.\nThe tutorial below covers: \nRagas\n for synthetic test data generation and evaluation\nArize AI’s \nPhoenix\n for tracing, visualization, and cluster analysis\nLlamaIndex\n for building RAG pipelines\n\n\nPrevious\nExport Data from Arize to Phoenix\nNext\nFrequently Asked Questions\nLast updated \n5 months ago\n\nsource: https://docs.arize.com/phoenix/tracing/use-cases-tracing/rag-evaluation\ntitle: Evaluate RAG\n\nEvaluate RAG\nBuilding a RAG pipeline and evaluating it with Phoenix Evals.\nIn this tutorial we will look into building a RAG pipeline and evaluating it with Phoenix Evals.\nIt has the the following sections:\nUnderstanding Retrieval Augmented Generation (RAG).\nBuilding RAG (with the help of a framework such as LlamaIndex).\nEvaluating RAG with Phoenix Evals.\nRetrieval Augmented Generation (RAG)\nLLMs are trained on vast amounts of data, but these will not include your specific data (things like company knowledge bases and documentation). Retrieval-Augmented Generation (RAG) addresses this by dynamically incorporating your data as context during the generation process. This is done not by altering the training data of the LLMs but by allowing the model to access and utilize your data in real-time to provide more tailored and contextually relevant responses.\nIn RAG, your data is loaded and prepared for queries. This process is called indexing. User queries act on this index, which filters your data down to the most relevant context. This context and your query then are sent to the LLM along with a prompt, and the LLM provides a response.\nRAG is a critical component for building applications such a chatbots or agents and you will want to know RAG techniques on how to get data into your application.\nStages within RAG\nThere are five key stages within RAG, which will in turn be a part of any larger RAG application.\nLoading\n: This refers to getting your data from where it lives - whether it\'s text files, PDFs, another website, a database or an API - into your pipeline.\nIndexing\n: This means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\nStoring\n: Once your data is indexed, you will want to store your index, along with any other metadata, to avoid the need to re-index it.\nQuerying\n: For any given indexing strategy there are many ways you can utilize LLMs and data structures to query, including sub-queries, multi-step queries, and hybrid strategies.\nEvaluation\n: A critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures on how accurate, faithful, and fast your responses to queries are.\nBuild a RAG system\nNow that we have understood the stages of RAG, let\'s build a pipeline. We will use \nLlamaIndex\n for RAG and \nPhoenix Evals\n for evaluation.\nCopy\n!pip install \n-\nqq \n""arize-phoenix[experimental,llama-index]>=2.0""\nCopy\n# The nest_asyncio module enables the nesting of asynchronous functions within an already running async loop.\n\n\n# This is necessary because Jupyter notebooks inherently operate in an asynchronous loop.\n\n\n# By applying nest_asyncio, we can run additional async functions within this existing loop without conflicts.\n\n\nimport\n nest_asyncio\n\n\n\n\nnest_asyncio\n.\napply\n()\n\n\n\n\nimport\n os\n\n\nfrom\n getpass \nimport\n getpass\n\n\n\n\nimport\n pandas \nas\n pd\n\n\nimport\n phoenix \nas\n px\n\n\nfrom\n llama_index \nimport\n SimpleDirectoryReader\n,\n VectorStoreIndex\n,\n set_global_handler\n\n\nfrom\n llama_index\n.\nllms \nimport\n OpenAI\n\n\nfrom\n llama_index\n.\nnode_parser \nimport\n SimpleNodeParser\nDuring this tutorial, we will capture all the data we need to evaluate our RAG pipeline using Phoenix Tracing. To enable this, simply start the phoenix application and instrument LlamaIndex.\nCopy\npx\n.\nlaunch_app\n()\nCopy\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace \nimport\n TracerProvider\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\n\n\nendpoint \n=\n \n""http://127.0.0.1:6006/v1/traces""\n\n\ntracer_provider \n=\n \nTracerProvider\n()\n\n\ntracer_provider\n.\nadd_span_processor\n(\nSimpleSpanProcessor\n(\nOTLPSpanExporter\n(endpoint)))\n\n\n\n\nLlamaIndexInstrumentor\n().\ninstrument\n(tracer_provider\n=\ntracer_provider)\nFor this tutorial we will be using OpenAI for creating synthetic data as well as for evaluation.\nCopy\nif\n \nnot\n (openai_api_key \n:=\n os\n.\ngetenv\n(\n""OPENAI_API_KEY""\n)\n)\n:\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Where should I look to learn how to use ragas with Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'You should look into the tutorial that covers Ragas for synthetic test data generation and evaluation, as well as the usage of Phoenix for tracing, visualization, and cluster analysis.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/integrations/ragas\ntitle: Ragas\n\nRagas\nPhoenix and Ragas work hand-in-hand\nBuilding a baseline for a RAG pipeline is not usually difficult, but enhancing it to make it suitable for production and ensuring the quality of your responses is almost always hard. Choosing the right tools and parameters for RAG can itself be challenging when there is an abundance of options available. With Ragas and Phoenix you create a robust workflow for making the right choices while building your RAG and ensuring its quality.\nThe tutorial below covers: \nRagas\n for synthetic test data generation and evaluation\nArize AI’s \nPhoenix\n for tracing, visualization, and cluster analysis\nLlamaIndex\n for building RAG pipelines\n\n\nPrevious\nExport Data from Arize to Phoenix\nNext\nFrequently Asked Questions\nLast updated \n5 months ago\n\nsource: https://docs.arize.com/phoenix/tracing/use-cases-tracing/rag-evaluation\ntitle: Evaluate RAG\n\nEvaluate RAG\nBuilding a RAG pipeline and evaluating it with Phoenix Evals.\nIn this tutorial we will look into building a RAG pipeline and evaluating it with Phoenix Evals.\nIt has the the following sections:\nUnderstanding Retrieval Augmented Generation (RAG).\nBuilding RAG (with the help of a framework such as LlamaIndex).\nEvaluating RAG with Phoenix Evals.\nRetrieval Augmented Generation (RAG)\nLLMs are trained on vast amounts of data, but these will not include your specific data (things like company knowledge bases and documentation). Retrieval-Augmented Generation (RAG) addresses this by dynamically incorporating your data as context during the generation process. This is done not by altering the training data of the LLMs but by allowing the model to access and utilize your data in real-time to provide more tailored and contextually relevant responses.\nIn RAG, your data is loaded and prepared for queries. This process is called indexing. User queries act on this index, which filters your data down to the most relevant context. This context and your query then are sent to the LLM along with a prompt, and the LLM provides a response.\nRAG is a critical component for building applications such a chatbots or agents and you will want to know RAG techniques on how to get data into your application.\nStages within RAG\nThere are five key stages within RAG, which will in turn be a part of any larger RAG application.\nLoading\n: This refers to getting your data from where it lives - whether it\'s text files, PDFs, another website, a database or an API - into your pipeline.\nIndexing\n: This means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\nStoring\n: Once your data is indexed, you will want to store your index, along with any other metadata, to avoid the need to re-index it.\nQuerying\n: For any given indexing strategy there are many ways you can utilize LLMs and data structures to query, including sub-queries, multi-step queries, and hybrid strategies.\nEvaluation\n: A critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures on how accurate, faithful, and fast your responses to queries are.\nBuild a RAG system\nNow that we have understood the stages of RAG, let\'s build a pipeline. We will use \nLlamaIndex\n for RAG and \nPhoenix Evals\n for evaluation.\nCopy\n!pip install \n-\nqq \n""arize-phoenix[experimental,llama-index]>=2.0""\nCopy\n# The nest_asyncio module enables the nesting of asynchronous functions within an already running async loop.\n\n\n# This is necessary because Jupyter notebooks inherently operate in an asynchronous loop.\n\n\n# By applying nest_asyncio, we can run additional async functions within this existing loop without conflicts.\n\n\nimport\n nest_asyncio\n\n\n\n\nnest_asyncio\n.\napply\n()\n\n\n\n\nimport\n os\n\n\nfrom\n getpass \nimport\n getpass\n\n\n\n\nimport\n pandas \nas\n pd\n\n\nimport\n phoenix \nas\n px\n\n\nfrom\n llama_index \nimport\n SimpleDirectoryReader\n,\n VectorStoreIndex\n,\n set_global_handler\n\n\nfrom\n llama_index\n.\nllms \nimport\n OpenAI\n\n\nfrom\n llama_index\n.\nnode_parser \nimport\n SimpleNodeParser\nDuring this tutorial, we will capture all the data we need to evaluate our RAG pipeline using Phoenix Tracing. To enable this, simply start the phoenix application and instrument LlamaIndex.\nCopy\npx\n.\nlaunch_app\n()\nCopy\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace \nimport\n TracerProvider\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\n\n\nendpoint \n=\n \n""http://127.0.0.1:6006/v1/traces""\n\n\ntracer_provider \n=\n \nTracerProvider\n()\n\n\ntracer_provider\n.\nadd_span_processor\n(\nSimpleSpanProcessor\n(\nOTLPSpanExporter\n(endpoint)))\n\n\n\n\nLlamaIndexInstrumentor\n().\ninstrument\n(tracer_provider\n=\ntracer_provider)\nFor this tutorial we will be using OpenAI for creating synthetic data as well as for evaluation.\nCopy\nif\n \nnot\n (openai_api_key \n:=\n os\n.\ngetenv\n(\n""OPENAI_API_KEY""\n)\n)\n:\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Where should I look to learn how to use ragas with Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'You should look into the tutorial that covers Ragas for synthetic test data generation and evaluation, as well as the usage of Phoenix for tracing, visualization, and cluster analysis.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6Njc=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/instrumentation/auto-instrument-ts\ntitle: Auto Instrument: TS\n\nAuto Instrument: TS\nPhoenix natively supports collecting traces generated via OpenInference automatic instrumentation. The supported instrumentations are\nLibrary\nInstrumentation\nVersion\nOpenAI\n@arizeai/openinference-instrumentation-openai\nLangChainJS\n@arizeai/openinference-instrumentation-langchain\nOpenInference JS is fully open-source and maintained on \nGitHub\nInstallation\nOpenInference uses OpenTelemetry Protocol (OTLP) to send traces Phoenix. To use OpenInference, you will need to install the OpenTelemetry SDK and the OpenInference instrumentation for the LLM framework you are using.\nInstall the OpenTelemetry SDK:\nCopy\nnpm install --save @opentelemetry/exporter-trace-otlp-http @opentelemetry/exporter-trace-otlp-proto @opentelemetry/resources @opentelemetry/sdk-trace-node\nInstall the OpenInference instrumentation you would like to use:\nCopy\nnpm\n \ninstall\n \n--save\n \n@arizeai/openinference-instrumentation-openai\nIf you plan on manually instrumenting your application, you will also need to install the OpenInference Semantic Conventions:\nCopy\nnpm\n \ninstall\n \n--save\n \n@arizeai/openinference-semantic-conventions\nThis example instruments OpenAI but you can replace \n@arizeai/openinference-instrumentation-openai\n with the instrumentation(s) of your choosing.\nUsage\nTo load the OpenAI instrumentation, specify it in the registerInstrumentations call along with any additional instrumentation you wish to enable.\nCopy\nconst\n { \nNodeTracerProvider\n } \n=\n \nrequire\n(\n""@opentelemetry/sdk-trace-node""\n);\n\n\nconst\n {\n\n\n  \nOpenAIInstrumentation\n,\n\n\n} \n=\n \nrequire\n(\n""@arizeai/openinference-instrumentation-openai""\n);\n\n\nconst\n { \nregisterInstrumentations\n } \n=\n \nrequire\n(\n""@opentelemetry/instrumentation""\n);\n\n\n\n\nconst\n \nprovider\n \n=\n \nnew\n \nNodeTracerProvider\n();\n\n\nprovider\n.register\n();\n\n\n\n\nregisterInstrumentations\n({\n\n\n  instrumentations\n:\n [\nnew\n \nOpenAIInstrumentation\n()]\n,\n\n\n});\nFor more information on OpenTelemetry Node.js SDK, see the \nOpenTelemetry Node.js SDK documentation\n.\nNote the above instrumentation must run before any other code in your application. This is because the instrumentation will only capture spans for the code that runs after the instrumentation is loaded. Typically this is done by requiring the instrumentation when running your application. \nnode -r ./path/to/instrumentation.js ./path/to/your/app.js\n\n\nPrevious\nAuto Instrument: Python\nNext\nManual Instrumentation\nLast updated \n9 hours ago\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/instrumentation\ntitle: Auto Instrumentation\n\nAuto Instrumentation\nHow OpenInference facilitates automatic instrumentation of applications.\nIn order to make a system observable, it must be \ninstrumented\n: That is, code from the system’s components must emit traces.\nPhoenix natively supports collecting traces generated via OpenInference automatic instrumentation. Phoenix has pre-built auto-instrumentors for \nPython\n and \nTypeScript / JavaScript\n.\n\n\nPython\nLlamaIndex, LangChain, DSPy, OpenAI, Bedrock, Mistral and more\nTypeScript / JavaScript\nOpenAI, LangChainjs\nPrevious\nInstrumenting Prompt Templates and Prompt Variables\nNext\nAuto Instrument: Python\nLast updated \n4 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What steps are needed to set up automatic instrumentation in Arize Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'To set up automatic instrumentation in Arize Phoenix, you need to install the OpenTelemetry SDK and the specific OpenInference instrumentation for the LLM framework you are using. First, install the OpenTelemetry SDK by running the provided npm command. Then, install the OpenInference instrumentation you want to use by running the respective npm command. Finally, load the desired instrumentation by specifying it in the registerInstrumentations call along with any additional instrumentation you wish to enable. Remember that the instrumentation must run before any other code in your application to capture spans effectively.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/instrumentation/auto-instrument-ts\ntitle: Auto Instrument: TS\n\nAuto Instrument: TS\nPhoenix natively supports collecting traces generated via OpenInference automatic instrumentation. The supported instrumentations are\nLibrary\nInstrumentation\nVersion\nOpenAI\n@arizeai/openinference-instrumentation-openai\nLangChainJS\n@arizeai/openinference-instrumentation-langchain\nOpenInference JS is fully open-source and maintained on \nGitHub\nInstallation\nOpenInference uses OpenTelemetry Protocol (OTLP) to send traces Phoenix. To use OpenInference, you will need to install the OpenTelemetry SDK and the OpenInference instrumentation for the LLM framework you are using.\nInstall the OpenTelemetry SDK:\nCopy\nnpm install --save @opentelemetry/exporter-trace-otlp-http @opentelemetry/exporter-trace-otlp-proto @opentelemetry/resources @opentelemetry/sdk-trace-node\nInstall the OpenInference instrumentation you would like to use:\nCopy\nnpm\n \ninstall\n \n--save\n \n@arizeai/openinference-instrumentation-openai\nIf you plan on manually instrumenting your application, you will also need to install the OpenInference Semantic Conventions:\nCopy\nnpm\n \ninstall\n \n--save\n \n@arizeai/openinference-semantic-conventions\nThis example instruments OpenAI but you can replace \n@arizeai/openinference-instrumentation-openai\n with the instrumentation(s) of your choosing.\nUsage\nTo load the OpenAI instrumentation, specify it in the registerInstrumentations call along with any additional instrumentation you wish to enable.\nCopy\nconst\n { \nNodeTracerProvider\n } \n=\n \nrequire\n(\n""@opentelemetry/sdk-trace-node""\n);\n\n\nconst\n {\n\n\n  \nOpenAIInstrumentation\n,\n\n\n} \n=\n \nrequire\n(\n""@arizeai/openinference-instrumentation-openai""\n);\n\n\nconst\n { \nregisterInstrumentations\n } \n=\n \nrequire\n(\n""@opentelemetry/instrumentation""\n);\n\n\n\n\nconst\n \nprovider\n \n=\n \nnew\n \nNodeTracerProvider\n();\n\n\nprovider\n.register\n();\n\n\n\n\nregisterInstrumentations\n({\n\n\n  instrumentations\n:\n [\nnew\n \nOpenAIInstrumentation\n()]\n,\n\n\n});\nFor more information on OpenTelemetry Node.js SDK, see the \nOpenTelemetry Node.js SDK documentation\n.\nNote the above instrumentation must run before any other code in your application. This is because the instrumentation will only capture spans for the code that runs after the instrumentation is loaded. Typically this is done by requiring the instrumentation when running your application. \nnode -r ./path/to/instrumentation.js ./path/to/your/app.js\n\n\nPrevious\nAuto Instrument: Python\nNext\nManual Instrumentation\nLast updated \n9 hours ago\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/instrumentation\ntitle: Auto Instrumentation\n\nAuto Instrumentation\nHow OpenInference facilitates automatic instrumentation of applications.\nIn order to make a system observable, it must be \ninstrumented\n: That is, code from the system’s components must emit traces.\nPhoenix natively supports collecting traces generated via OpenInference automatic instrumentation. Phoenix has pre-built auto-instrumentors for \nPython\n and \nTypeScript / JavaScript\n.\n\n\nPython\nLlamaIndex, LangChain, DSPy, OpenAI, Bedrock, Mistral and more\nTypeScript / JavaScript\nOpenAI, LangChainjs\nPrevious\nInstrumenting Prompt Templates and Prompt Variables\nNext\nAuto Instrument: Python\nLast updated \n4 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What steps are needed to set up automatic instrumentation in Arize Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'To set up automatic instrumentation in Arize Phoenix, you need to install the OpenTelemetry SDK and the specific OpenInference instrumentation for the LLM framework you are using. First, install the OpenTelemetry SDK by running the provided npm command. Then, install the OpenInference instrumentation you want to use by running the respective npm command. Finally, load the desired instrumentation by specifying it in the registerInstrumentations call along with any additional instrumentation you wish to enable. Remember that the instrumentation must run before any other code in your application to capture spans effectively.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6Njg=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/integrations/arize/bring-production-data-to-notebook-for-eda-or-retraining\ntitle: Export Data from Arize to Phoenix\n\nExport Data from Arize to Phoenix\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nOftentimes, the team that notices an issue in their model, for example a prompt/response LLM model, may not be the same team that continues the investigations or kicks off retraining workflows.\nTo help connect teams and workflows, Phoenix enables continued analysis of production data from \nArize\n in a notebook environment for fine tuning workflows.\nFor example, a user may have noticed in \nArize\n that this prompt template is not performing well.\nWith a few lines of Python code, users can export this data into Phoenix for further analysis. This allows team members, such as data scientists, who may not have access to production data today, an easy way to access relevant product data for further analysis in an environment they are familiar with.\nThey can then easily augment and fine tune the data and verify improved performance, before deploying back to production.\nThere are two ways export data out of \nArize\n for further investigation:\nThe easiest way is to click the export button on the Embeddings and Inferences pages. This will produce a code snippet that you can copy into a Python environment and install Phoenix. This code snippet will include the date range you have selected in the \nArize\n platform, in addition to the inferences you have selected.\nUsers can also query \nArize\n for data directly using the Arize Python export client. We recommend doing this once you're more comfortable with the in-platform export functionality, as you will need to manually enter in the data ranges and data you want to export.\nCopy\nos\n.\nenviron\n[\n'ARIZE_API_KEY'\n]\n \n=\n ARIZE_API_KEY\n\n\n\n\nfrom\n datetime \nimport\n datetime\n\n\n\n\nfrom\n arize\n.\nexporter \nimport\n ArizeExportClient\n\n\nfrom\n arize\n.\nutils\n.\ntypes \nimport\n Environments\n\n\n\n\nclient \n=\n \nArizeExportClient\n()\n\n\n\n\nprimary_df \n=\n client\n.\nexport_model_to_df\n(\n\n\n    space_id\n=\n'U3BhY2U6NzU0'\n,\n\n\n    model_name\n=\n'test_home_prices_LLM'\n,\n\n\n    environment\n=\nEnvironments.PRODUCTION,\n\n\n    start_time\n=\ndatetime.\nfromisoformat\n(\n'2023-02-11T07:00:00.000+00:00'\n),\n\n\n    end_time\n=\ndatetime.\nfromisoformat\n(\n'2023-03-14T00:59:59.999+00:00'\n),\n\n\n)\nTest out this workflow by signing up for a \nfree Arize account\n.\nPrevious\nArize\nNext\nRagas\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/inferences/how-to-inferences\ntitle: How-to: Inferences\n\nHow-to: Inferences\nImport your data\nImport Inference data (CV, NLP)\nImport Prompt and Response (LLM) data\nImport Retrieval data\nImport Corpus (Vector Store) data\nManage the App\nDefine your dataset(s)\nHow to launch the app\nHow to view the UI\nHow to close the app\nHow to export data\nHow to export your data for labeling, evaluation, or fine-tuning\nHow to export embeddings\nHow to export a cluster\nHow to export all clusters\nHow to generate embeddings\n\n\nPrevious\nQuickstart: Inferences\nNext\nImport Your Data\nLast updated \n4 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can users import their inference data into Arize Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'Users can import their inference data into Arize Phoenix by following the steps outlined in the documentation.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/integrations/arize/bring-production-data-to-notebook-for-eda-or-retraining\ntitle: Export Data from Arize to Phoenix\n\nExport Data from Arize to Phoenix\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nOftentimes, the team that notices an issue in their model, for example a prompt/response LLM model, may not be the same team that continues the investigations or kicks off retraining workflows.\nTo help connect teams and workflows, Phoenix enables continued analysis of production data from \nArize\n in a notebook environment for fine tuning workflows.\nFor example, a user may have noticed in \nArize\n that this prompt template is not performing well.\nWith a few lines of Python code, users can export this data into Phoenix for further analysis. This allows team members, such as data scientists, who may not have access to production data today, an easy way to access relevant product data for further analysis in an environment they are familiar with.\nThey can then easily augment and fine tune the data and verify improved performance, before deploying back to production.\nThere are two ways export data out of \nArize\n for further investigation:\nThe easiest way is to click the export button on the Embeddings and Inferences pages. This will produce a code snippet that you can copy into a Python environment and install Phoenix. This code snippet will include the date range you have selected in the \nArize\n platform, in addition to the inferences you have selected.\nUsers can also query \nArize\n for data directly using the Arize Python export client. We recommend doing this once you're more comfortable with the in-platform export functionality, as you will need to manually enter in the data ranges and data you want to export.\nCopy\nos\n.\nenviron\n[\n'ARIZE_API_KEY'\n]\n \n=\n ARIZE_API_KEY\n\n\n\n\nfrom\n datetime \nimport\n datetime\n\n\n\n\nfrom\n arize\n.\nexporter \nimport\n ArizeExportClient\n\n\nfrom\n arize\n.\nutils\n.\ntypes \nimport\n Environments\n\n\n\n\nclient \n=\n \nArizeExportClient\n()\n\n\n\n\nprimary_df \n=\n client\n.\nexport_model_to_df\n(\n\n\n    space_id\n=\n'U3BhY2U6NzU0'\n,\n\n\n    model_name\n=\n'test_home_prices_LLM'\n,\n\n\n    environment\n=\nEnvironments.PRODUCTION,\n\n\n    start_time\n=\ndatetime.\nfromisoformat\n(\n'2023-02-11T07:00:00.000+00:00'\n),\n\n\n    end_time\n=\ndatetime.\nfromisoformat\n(\n'2023-03-14T00:59:59.999+00:00'\n),\n\n\n)\nTest out this workflow by signing up for a \nfree Arize account\n.\nPrevious\nArize\nNext\nRagas\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/inferences/how-to-inferences\ntitle: How-to: Inferences\n\nHow-to: Inferences\nImport your data\nImport Inference data (CV, NLP)\nImport Prompt and Response (LLM) data\nImport Retrieval data\nImport Corpus (Vector Store) data\nManage the App\nDefine your dataset(s)\nHow to launch the app\nHow to view the UI\nHow to close the app\nHow to export data\nHow to export your data for labeling, evaluation, or fine-tuning\nHow to export embeddings\nHow to export a cluster\nHow to export all clusters\nHow to generate embeddings\n\n\nPrevious\nQuickstart: Inferences\nNext\nImport Your Data\nLast updated \n4 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can users import their inference data into Arize Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'Users can import their inference data into Arize Phoenix by following the steps outlined in the documentation.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6Njk=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/integrations/arize\ntitle: Arize\n\nArize\nPhoenix works hand-in-hand with Arize, it's enterprise counterpart\nShare Embeddings Data with Arize\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nPrevious\nModels\nNext\nExport Data from Arize to Phoenix\nLast updated \n5 months ago\n\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the key components of a schema in Arize Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'The key components of a schema in Arize Phoenix include embeddings data sharing, exporting data to Phoenix, AI observability, evaluation, visualization of data, performance evaluation, issue tracking, and data export for improvement.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/integrations/arize\ntitle: Arize\n\nArize\nPhoenix works hand-in-hand with Arize, it's enterprise counterpart\nShare Embeddings Data with Arize\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nPrevious\nModels\nNext\nExport Data from Arize to Phoenix\nLast updated \n5 months ago\n\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the key components of a schema in Arize Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'The key components of a schema in Arize Phoenix include embeddings data sharing, exporting data to Phoenix, AI observability, evaluation, visualization of data, performance evaluation, issue tracking, and data export for improvement.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NzA=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/integrations/arize/bring-production-data-to-notebook-for-eda-or-retraining\ntitle: Export Data from Arize to Phoenix\n\nExport Data from Arize to Phoenix\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nOftentimes, the team that notices an issue in their model, for example a prompt/response LLM model, may not be the same team that continues the investigations or kicks off retraining workflows.\nTo help connect teams and workflows, Phoenix enables continued analysis of production data from \nArize\n in a notebook environment for fine tuning workflows.\nFor example, a user may have noticed in \nArize\n that this prompt template is not performing well.\nWith a few lines of Python code, users can export this data into Phoenix for further analysis. This allows team members, such as data scientists, who may not have access to production data today, an easy way to access relevant product data for further analysis in an environment they are familiar with.\nThey can then easily augment and fine tune the data and verify improved performance, before deploying back to production.\nThere are two ways export data out of \nArize\n for further investigation:\nThe easiest way is to click the export button on the Embeddings and Inferences pages. This will produce a code snippet that you can copy into a Python environment and install Phoenix. This code snippet will include the date range you have selected in the \nArize\n platform, in addition to the inferences you have selected.\nUsers can also query \nArize\n for data directly using the Arize Python export client. We recommend doing this once you're more comfortable with the in-platform export functionality, as you will need to manually enter in the data ranges and data you want to export.\nCopy\nos\n.\nenviron\n[\n'ARIZE_API_KEY'\n]\n \n=\n ARIZE_API_KEY\n\n\n\n\nfrom\n datetime \nimport\n datetime\n\n\n\n\nfrom\n arize\n.\nexporter \nimport\n ArizeExportClient\n\n\nfrom\n arize\n.\nutils\n.\ntypes \nimport\n Environments\n\n\n\n\nclient \n=\n \nArizeExportClient\n()\n\n\n\n\nprimary_df \n=\n client\n.\nexport_model_to_df\n(\n\n\n    space_id\n=\n'U3BhY2U6NzU0'\n,\n\n\n    model_name\n=\n'test_home_prices_LLM'\n,\n\n\n    environment\n=\nEnvironments.PRODUCTION,\n\n\n    start_time\n=\ndatetime.\nfromisoformat\n(\n'2023-02-11T07:00:00.000+00:00'\n),\n\n\n    end_time\n=\ndatetime.\nfromisoformat\n(\n'2023-03-14T00:59:59.999+00:00'\n),\n\n\n)\nTest out this workflow by signing up for a \nfree Arize account\n.\nPrevious\nArize\nNext\nRagas\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/inferences/how-to-inferences/export-your-data\ntitle: Export Data\n\nExport Data\nHow to export your data for labeling, evaluation, or fine-tuning\nExporting Embeddings\nEmbeddings can be extremely useful for fine-tuning. There are two ways to export your embeddings from the Phoenix UI.\nExport Selected Clusters\nTo export a cluster (either selected via the lasso tool or via a the cluster list on the right hand panel), click on the export button on the top left of the bottom slide-out.\nExport All Clusters\nTo export all clusters of embeddings as a single dataframe (labeled by cluster), click the \n...\n icon on the top right of the screen and click export. Your data will be available either as a Parquet file or is available back in your notebook via your \nsession\n as a dataframe.\nCopy\nsession \n=\n px\n.\nactive_session\n()\n\n\nsession\n.\nexports\n[\n-\n1\n].\ndataframe\nPrevious\nCorpus Data\nNext\nGenerate Embeddings\nLast updated \n4 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can users export inference data from Arize Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'Users can export inference data from Arize Phoenix by clicking the export button on the Embeddings and Inferences pages. This action will generate a code snippet that can be copied into a Python environment after installing Phoenix. The code snippet will include the selected date range and inferences from the Arize platform. Alternatively, users can also query Arize directly for data using the Arize Python export client, which involves manually entering the data ranges and data to be exported.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/integrations/arize/bring-production-data-to-notebook-for-eda-or-retraining\ntitle: Export Data from Arize to Phoenix\n\nExport Data from Arize to Phoenix\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nOftentimes, the team that notices an issue in their model, for example a prompt/response LLM model, may not be the same team that continues the investigations or kicks off retraining workflows.\nTo help connect teams and workflows, Phoenix enables continued analysis of production data from \nArize\n in a notebook environment for fine tuning workflows.\nFor example, a user may have noticed in \nArize\n that this prompt template is not performing well.\nWith a few lines of Python code, users can export this data into Phoenix for further analysis. This allows team members, such as data scientists, who may not have access to production data today, an easy way to access relevant product data for further analysis in an environment they are familiar with.\nThey can then easily augment and fine tune the data and verify improved performance, before deploying back to production.\nThere are two ways export data out of \nArize\n for further investigation:\nThe easiest way is to click the export button on the Embeddings and Inferences pages. This will produce a code snippet that you can copy into a Python environment and install Phoenix. This code snippet will include the date range you have selected in the \nArize\n platform, in addition to the inferences you have selected.\nUsers can also query \nArize\n for data directly using the Arize Python export client. We recommend doing this once you're more comfortable with the in-platform export functionality, as you will need to manually enter in the data ranges and data you want to export.\nCopy\nos\n.\nenviron\n[\n'ARIZE_API_KEY'\n]\n \n=\n ARIZE_API_KEY\n\n\n\n\nfrom\n datetime \nimport\n datetime\n\n\n\n\nfrom\n arize\n.\nexporter \nimport\n ArizeExportClient\n\n\nfrom\n arize\n.\nutils\n.\ntypes \nimport\n Environments\n\n\n\n\nclient \n=\n \nArizeExportClient\n()\n\n\n\n\nprimary_df \n=\n client\n.\nexport_model_to_df\n(\n\n\n    space_id\n=\n'U3BhY2U6NzU0'\n,\n\n\n    model_name\n=\n'test_home_prices_LLM'\n,\n\n\n    environment\n=\nEnvironments.PRODUCTION,\n\n\n    start_time\n=\ndatetime.\nfromisoformat\n(\n'2023-02-11T07:00:00.000+00:00'\n),\n\n\n    end_time\n=\ndatetime.\nfromisoformat\n(\n'2023-03-14T00:59:59.999+00:00'\n),\n\n\n)\nTest out this workflow by signing up for a \nfree Arize account\n.\nPrevious\nArize\nNext\nRagas\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/inferences/how-to-inferences/export-your-data\ntitle: Export Data\n\nExport Data\nHow to export your data for labeling, evaluation, or fine-tuning\nExporting Embeddings\nEmbeddings can be extremely useful for fine-tuning. There are two ways to export your embeddings from the Phoenix UI.\nExport Selected Clusters\nTo export a cluster (either selected via the lasso tool or via a the cluster list on the right hand panel), click on the export button on the top left of the bottom slide-out.\nExport All Clusters\nTo export all clusters of embeddings as a single dataframe (labeled by cluster), click the \n...\n icon on the top right of the screen and click export. Your data will be available either as a Parquet file or is available back in your notebook via your \nsession\n as a dataframe.\nCopy\nsession \n=\n px\n.\nactive_session\n()\n\n\nsession\n.\nexports\n[\n-\n1\n].\ndataframe\nPrevious\nCorpus Data\nNext\nGenerate Embeddings\nLast updated \n4 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can users export inference data from Arize Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'Users can export inference data from Arize Phoenix by clicking the export button on the Embeddings and Inferences pages. This action will generate a code snippet that can be copied into a Python environment after installing Phoenix. The code snippet will include the selected date range and inferences from the Arize platform. Alternatively, users can also query Arize directly for data using the Arize Python export client, which involves manually entering the data ranges and data to be exported.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NzE=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/inferences/how-to-inferences/export-your-data\ntitle: Export Data\n\nExport Data\nHow to export your data for labeling, evaluation, or fine-tuning\nExporting Embeddings\nEmbeddings can be extremely useful for fine-tuning. There are two ways to export your embeddings from the Phoenix UI.\nExport Selected Clusters\nTo export a cluster (either selected via the lasso tool or via a the cluster list on the right hand panel), click on the export button on the top left of the bottom slide-out.\nExport All Clusters\nTo export all clusters of embeddings as a single dataframe (labeled by cluster), click the \n...\n icon on the top right of the screen and click export. Your data will be available either as a Parquet file or is available back in your notebook via your \nsession\n as a dataframe.\nCopy\nsession \n=\n px\n.\nactive_session\n()\n\n\nsession\n.\nexports\n[\n-\n1\n].\ndataframe\nPrevious\nCorpus Data\nNext\nGenerate Embeddings\nLast updated \n4 months ago\n\nsource: https://docs.arize.com/phoenix/inferences/how-to-inferences\ntitle: How-to: Inferences\n\nHow-to: Inferences\nImport your data\nImport Inference data (CV, NLP)\nImport Prompt and Response (LLM) data\nImport Retrieval data\nImport Corpus (Vector Store) data\nManage the App\nDefine your dataset(s)\nHow to launch the app\nHow to view the UI\nHow to close the app\nHow to export data\nHow to export your data for labeling, evaluation, or fine-tuning\nHow to export embeddings\nHow to export a cluster\nHow to export all clusters\nHow to generate embeddings\n\n\nPrevious\nQuickstart: Inferences\nNext\nImport Your Data\nLast updated \n4 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What formats are supported for exporting inference data?\nAnswer: '}, {'role': 'assistant', 'content': 'Your data can be exported in either a Parquet file format or as a dataframe.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/inferences/how-to-inferences/export-your-data\ntitle: Export Data\n\nExport Data\nHow to export your data for labeling, evaluation, or fine-tuning\nExporting Embeddings\nEmbeddings can be extremely useful for fine-tuning. There are two ways to export your embeddings from the Phoenix UI.\nExport Selected Clusters\nTo export a cluster (either selected via the lasso tool or via a the cluster list on the right hand panel), click on the export button on the top left of the bottom slide-out.\nExport All Clusters\nTo export all clusters of embeddings as a single dataframe (labeled by cluster), click the \n...\n icon on the top right of the screen and click export. Your data will be available either as a Parquet file or is available back in your notebook via your \nsession\n as a dataframe.\nCopy\nsession \n=\n px\n.\nactive_session\n()\n\n\nsession\n.\nexports\n[\n-\n1\n].\ndataframe\nPrevious\nCorpus Data\nNext\nGenerate Embeddings\nLast updated \n4 months ago\n\nsource: https://docs.arize.com/phoenix/inferences/how-to-inferences\ntitle: How-to: Inferences\n\nHow-to: Inferences\nImport your data\nImport Inference data (CV, NLP)\nImport Prompt and Response (LLM) data\nImport Retrieval data\nImport Corpus (Vector Store) data\nManage the App\nDefine your dataset(s)\nHow to launch the app\nHow to view the UI\nHow to close the app\nHow to export data\nHow to export your data for labeling, evaluation, or fine-tuning\nHow to export embeddings\nHow to export a cluster\nHow to export all clusters\nHow to generate embeddings\n\n\nPrevious\nQuickstart: Inferences\nNext\nImport Your Data\nLast updated \n4 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What formats are supported for exporting inference data?\nAnswer: '}, {'role': 'assistant', 'content': 'Your data can be exported in either a Parquet file format or as a dataframe.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NzI=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/llm-traces-1\ntitle: Quickstart: Tracing\n\n# Retrieve and generate using the relevant snippets of the blog.\n\n\nretriever \n=\n vectorstore\n.\nas_retriever\n()\n\n\nprompt \n=\n hub\n.\npull\n(\n""rlm/rag-prompt""\n)\n\n\n\n\ndef\n \nformat_docs\n(\ndocs\n):\n\n\n    \nreturn\n \n""\\n\\n""\n.\njoin\n(doc.page_content \nfor\n doc \nin\n docs)\n\n\n\n\n\n\nrag_chain \n=\n (\n\n\n    \n{\n""context""\n:\n retriever \n|\n format_docs\n,\n \n""question""\n:\n \nRunnablePassthrough\n()}\n\n\n    \n|\n prompt\n\n\n    \n|\n llm\n\n\n    \n|\n \nStrOutputParser\n()\n\n\n)\n\n\n\n\n# Execute the chain\n\n\nresponse \n=\n rag_chain\n.\ninvoke\n(\n""What is Task Decomposition?""\n)\nSee the \nintegration guide\n for details\nCopy\npip\n \ninstall\n \nopenai\nCopy\nimport\n phoenix \nas\n px\n\n\nfrom\n phoenix\n.\ntrace\n.\nopenai \nimport\n OpenAIInstrumentor\n\n\n\n\n# To view traces in Phoenix, you will first have to start a Phoenix server. You can do this by running the following:\n\n\nsession \n=\n px\n.\nlaunch_app\n()\n\n\n\n\n# Initialize OpenAI auto-instrumentation\n\n\nOpenAIInstrumentor\n().\ninstrument\n()\n\n\n\n\nimport\n os\n\n\nfrom\n openai \nimport\n OpenAI\n\n\n\n\n# Initialize an OpenAI client\n\n\nclient \n=\n \nOpenAI\n(api_key\n=\n\'\'\n)\n\n\n\n\n# Define a conversation with a user message\n\n\nconversation \n=\n [\n\n\n    \n{\n""role""\n:\n \n""system""\n,\n \n""content""\n:\n \n""You are a helpful assistant.""\n},\n\n\n    \n{\n""role""\n:\n \n""user""\n,\n \n""content""\n:\n \n""Hello, can you help me with something?""\n}\n\n\n]\n\n\n\n\n# Generate a response from the assistant\n\n\nresponse \n=\n client\n.\nchat\n.\ncompletions\n.\ncreate\n(\n\n\n    model\n=\n""gpt-3.5-turbo""\n,\n\n\n    messages\n=\nconversation,\n\n\n)\n\n\n\n\n# Extract and print the assistant\'s reply\n\n\n# The traces will be available in the Phoenix App for the above messsages\n\n\nassistant_reply \n=\n response\n.\nchoices\n[\n0\n].\nmessage\n.\ncontent\nOnce you\'ve executed a sufficient number of queries (or chats) to your application, you can view the details of the UI by refreshing the browser url\nA detailed view of a trace of a RAG application using LlamaIndex\nExporting Traces from Phoenix\nFrom the App\nCopy\n# You can export a dataframe from the session\n\n\ndf \n=\n px\n.\nClient\n().\nget_spans_dataframe\n()\n\n\n\n\n# Note that you can apply a filter if you would like to export only a sub-set of spans\n\n\ndf \n=\n px\n.\nClient\n().\nget_spans_dataframe\n(\n\'span_kind == ""RETRIEVER""\'\n)\nFor full details on how to export trace data, see \nthe detailed guide\nEvaluating Traces\nIn addition to launching phoenix on LlamaIndex and LangChain, teams can export trace data to a dataframe in order to run LLM Evals on the data.\nLearn more in the \nevals quickstart\n.\nConclusion\nLLM Traces\n are a powerful way to troubleshoot and understand your application and can be leveraged to \nevaluate\n the quality of your application (see also: this \ndeep dive on LLM evaluation\n). For a full list of notebooks that illustrate this in full-color, please check out the \nnotebooks section\n.\nPrevious\nOverview: Tracing\nNext\nIntegrations: Tracing\nLast updated \n6 hours ago\n\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nTraces\nA deep dive into the details of a trace\nSpans\nA span represents a unit of work or operation (think a \nspan\n of time). It tracks specific operations that a request makes, painting a picture of what happened during the time in which that operation was executed.\nA span contains name, time-related data, structured log messages, and other metadata (that is, Attributes) to provide information about the operation it tracks. A span for an LLM execution in JSON format is displayed below\nCopy\n{\n\n\n   \n""name""\n:\n \n""llm""\n,\n\n\n   \n""context""\n:\n {\n\n\n       \n""trace_id""\n:\n \n""0x6c80880dbeb609e2ed41e06a6397a0dd""\n,\n\n\n       \n""span_id""\n:\n \n""0xd9bdedf0df0b7208""\n,\n\n\n       \n""trace_state""\n:\n \n""[]""\n\n\n   }\n,\n\n\n   \n""kind""\n:\n \n""SpanKind.INTERNAL""\n,\n\n\n   \n""parent_id""\n:\n \n""0x7eb5df0046c77cd2""\n,\n\n\n   \n""start_time""\n:\n \n""2024-05-08T21:46:11.480777Z""\n,\n\n\n   \n""end_time""\n:\n \n""2024-05-08T21:46:35.368042Z""\n,\n\n\n   \n""status""\n:\n {\n\n\n       \n""status_code""\n:\n \n""OK""\n\n\n   }\n,\n\n\n   \n""attributes""\n:\n {\n\n\n       \n""openinference.span.kind""\n:\n \n""LLM""\n,\n\n\n       \n""llm.input_messages.0.message.role""\n:\n \n""system""\n,\n\n\n       ""llm.input_messages.0.message.content"": ""\\n  The following is a friendly conversation between a user and an AI assistant.\\n  The assistant is talkative and provides lots of specific details from its context.\\n  If the assistant does not know the answer to a question, it truthfully says it\\n  does not know.\\n\\n  Here are the relevant documents for the context:\\n\\n  page_label: 7\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.4Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\na. No piece may weigh more than 70 pounds.\\nb. The combined length and girth of a piece (the length of its longest side plus \\nthe distance around its thickest part) may not exceed 108 inches.\\nc. Lower size or weight standards apply to mail addressed to certain APOs and \\nFPOs, subject to 703.2.0  and 703.4.0  and for Department of State mail, \\nsubject to 703.3.0 .\\n\\npage_label: 6\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.2.10Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\na. The reply half of a double card must be used for reply only and may not be \\nused to convey a message to the original addressee or to send statements \\nof account. The reply half may be formatted for response purposes (e.g., contain blocks for completion by the addressee).\\nb. A double card must be folded before mailing and prepared so that the \\naddress on the reply half is on the inside when the double card is originally \\nmailed. The address side of the reply half may be prepared as Business \\nReply Mail, Courtesy Reply Mail, meter reply mail, or as a USPS Returns service label.\\nc. Plain stickers, seals, or a single wire stitch (staple) may be used to fasten the \\nopen edge at the top or bottom once the card is folded if affixed so that the \\ninner surfaces of the cards can be readily examined. Fasteners must be \\naffixed according to the applicable preparation requirements for the price claimed. Any sealing on the left and right sides of the cards, no matter the \\nsealing process used, is not permitted.\\nd. The first half of a double card must be detached when the reply half is \\nmailed for return. \\n6.2.10   Enclosures\\nEnclosures in double postcards are prohibited at card prices.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What formats are supported for exporting trace data?\nAnswer: '}, {'role': 'assistant', 'content': 'The formats supported for exporting trace data include exporting a dataframe from the session and applying a filter if you would like to export only a sub-set of spans.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/llm-traces-1\ntitle: Quickstart: Tracing\n\n# Retrieve and generate using the relevant snippets of the blog.\n\n\nretriever \n=\n vectorstore\n.\nas_retriever\n()\n\n\nprompt \n=\n hub\n.\npull\n(\n""rlm/rag-prompt""\n)\n\n\n\n\ndef\n \nformat_docs\n(\ndocs\n):\n\n\n    \nreturn\n \n""\\n\\n""\n.\njoin\n(doc.page_content \nfor\n doc \nin\n docs)\n\n\n\n\n\n\nrag_chain \n=\n (\n\n\n    \n{\n""context""\n:\n retriever \n|\n format_docs\n,\n \n""question""\n:\n \nRunnablePassthrough\n()}\n\n\n    \n|\n prompt\n\n\n    \n|\n llm\n\n\n    \n|\n \nStrOutputParser\n()\n\n\n)\n\n\n\n\n# Execute the chain\n\n\nresponse \n=\n rag_chain\n.\ninvoke\n(\n""What is Task Decomposition?""\n)\nSee the \nintegration guide\n for details\nCopy\npip\n \ninstall\n \nopenai\nCopy\nimport\n phoenix \nas\n px\n\n\nfrom\n phoenix\n.\ntrace\n.\nopenai \nimport\n OpenAIInstrumentor\n\n\n\n\n# To view traces in Phoenix, you will first have to start a Phoenix server. You can do this by running the following:\n\n\nsession \n=\n px\n.\nlaunch_app\n()\n\n\n\n\n# Initialize OpenAI auto-instrumentation\n\n\nOpenAIInstrumentor\n().\ninstrument\n()\n\n\n\n\nimport\n os\n\n\nfrom\n openai \nimport\n OpenAI\n\n\n\n\n# Initialize an OpenAI client\n\n\nclient \n=\n \nOpenAI\n(api_key\n=\n\'\'\n)\n\n\n\n\n# Define a conversation with a user message\n\n\nconversation \n=\n [\n\n\n    \n{\n""role""\n:\n \n""system""\n,\n \n""content""\n:\n \n""You are a helpful assistant.""\n},\n\n\n    \n{\n""role""\n:\n \n""user""\n,\n \n""content""\n:\n \n""Hello, can you help me with something?""\n}\n\n\n]\n\n\n\n\n# Generate a response from the assistant\n\n\nresponse \n=\n client\n.\nchat\n.\ncompletions\n.\ncreate\n(\n\n\n    model\n=\n""gpt-3.5-turbo""\n,\n\n\n    messages\n=\nconversation,\n\n\n)\n\n\n\n\n# Extract and print the assistant\'s reply\n\n\n# The traces will be available in the Phoenix App for the above messsages\n\n\nassistant_reply \n=\n response\n.\nchoices\n[\n0\n].\nmessage\n.\ncontent\nOnce you\'ve executed a sufficient number of queries (or chats) to your application, you can view the details of the UI by refreshing the browser url\nA detailed view of a trace of a RAG application using LlamaIndex\nExporting Traces from Phoenix\nFrom the App\nCopy\n# You can export a dataframe from the session\n\n\ndf \n=\n px\n.\nClient\n().\nget_spans_dataframe\n()\n\n\n\n\n# Note that you can apply a filter if you would like to export only a sub-set of spans\n\n\ndf \n=\n px\n.\nClient\n().\nget_spans_dataframe\n(\n\'span_kind == ""RETRIEVER""\'\n)\nFor full details on how to export trace data, see \nthe detailed guide\nEvaluating Traces\nIn addition to launching phoenix on LlamaIndex and LangChain, teams can export trace data to a dataframe in order to run LLM Evals on the data.\nLearn more in the \nevals quickstart\n.\nConclusion\nLLM Traces\n are a powerful way to troubleshoot and understand your application and can be leveraged to \nevaluate\n the quality of your application (see also: this \ndeep dive on LLM evaluation\n). For a full list of notebooks that illustrate this in full-color, please check out the \nnotebooks section\n.\nPrevious\nOverview: Tracing\nNext\nIntegrations: Tracing\nLast updated \n6 hours ago\n\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nTraces\nA deep dive into the details of a trace\nSpans\nA span represents a unit of work or operation (think a \nspan\n of time). It tracks specific operations that a request makes, painting a picture of what happened during the time in which that operation was executed.\nA span contains name, time-related data, structured log messages, and other metadata (that is, Attributes) to provide information about the operation it tracks. A span for an LLM execution in JSON format is displayed below\nCopy\n{\n\n\n   \n""name""\n:\n \n""llm""\n,\n\n\n   \n""context""\n:\n {\n\n\n       \n""trace_id""\n:\n \n""0x6c80880dbeb609e2ed41e06a6397a0dd""\n,\n\n\n       \n""span_id""\n:\n \n""0xd9bdedf0df0b7208""\n,\n\n\n       \n""trace_state""\n:\n \n""[]""\n\n\n   }\n,\n\n\n   \n""kind""\n:\n \n""SpanKind.INTERNAL""\n,\n\n\n   \n""parent_id""\n:\n \n""0x7eb5df0046c77cd2""\n,\n\n\n   \n""start_time""\n:\n \n""2024-05-08T21:46:11.480777Z""\n,\n\n\n   \n""end_time""\n:\n \n""2024-05-08T21:46:35.368042Z""\n,\n\n\n   \n""status""\n:\n {\n\n\n       \n""status_code""\n:\n \n""OK""\n\n\n   }\n,\n\n\n   \n""attributes""\n:\n {\n\n\n       \n""openinference.span.kind""\n:\n \n""LLM""\n,\n\n\n       \n""llm.input_messages.0.message.role""\n:\n \n""system""\n,\n\n\n       ""llm.input_messages.0.message.content"": ""\\n  The following is a friendly conversation between a user and an AI assistant.\\n  The assistant is talkative and provides lots of specific details from its context.\\n  If the assistant does not know the answer to a question, it truthfully says it\\n  does not know.\\n\\n  Here are the relevant documents for the context:\\n\\n  page_label: 7\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.4Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\na. No piece may weigh more than 70 pounds.\\nb. The combined length and girth of a piece (the length of its longest side plus \\nthe distance around its thickest part) may not exceed 108 inches.\\nc. Lower size or weight standards apply to mail addressed to certain APOs and \\nFPOs, subject to 703.2.0  and 703.4.0  and for Department of State mail, \\nsubject to 703.3.0 .\\n\\npage_label: 6\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.2.10Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\na. The reply half of a double card must be used for reply only and may not be \\nused to convey a message to the original addressee or to send statements \\nof account. The reply half may be formatted for response purposes (e.g., contain blocks for completion by the addressee).\\nb. A double card must be folded before mailing and prepared so that the \\naddress on the reply half is on the inside when the double card is originally \\nmailed. The address side of the reply half may be prepared as Business \\nReply Mail, Courtesy Reply Mail, meter reply mail, or as a USPS Returns service label.\\nc. Plain stickers, seals, or a single wire stitch (staple) may be used to fasten the \\nopen edge at the top or bottom once the card is folded if affixed so that the \\ninner surfaces of the cards can be readily examined. Fasteners must be \\naffixed according to the applicable preparation requirements for the price claimed. Any sealing on the left and right sides of the cards, no matter the \\nsealing process used, is not permitted.\\nd. The first half of a double card must be detached when the reply half is \\nmailed for return. \\n6.2.10   Enclosures\\nEnclosures in double postcards are prohibited at card prices.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What formats are supported for exporting trace data?\nAnswer: '}, {'role': 'assistant', 'content': 'The formats supported for exporting trace data include exporting a dataframe from the session and applying a filter if you would like to export only a sub-set of spans.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NzM=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/setup\ntitle: Setup\n\nSetup\nHow to get started with Phoenix\nHow to run in different environments\nHosted Phoenix by Arize\nIn a python notebook\nFrom the command line\nIn a Container\nHow to configure phoenix\nAvailable ports to run phoenix\nCustomize phoenix using environment variables\nPrevious\nExamples\nNext\nEnvironments\nLast updated \n8 hours ago\n\nsource: https://docs.arize.com/phoenix/quickstart\ntitle: Quickstart\n\nQuickstart\nThe easiest way to run Phoenix is locally in your own computer. To launch Phoenix, use the following steps.\nInstall\nUsing pip\nUsing conda\nDocker\napp.phoenix.arize.com\nCopy\npip\n \ninstall\n \narize-phoenix\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix\nPhoenix server images are available via \nDocker Hub\n and can be used via \ndocker compose \nor if you simply want a long-running phoenix instance to share with your team.\nCopy\ndocker\n \npull\n \narizephoenix/phoenix:latest\nIf you don\'t want to host an instance of Phoenix yourself or use a notebook instance, you can use a persistent instance provided on our site. Sign up for an Arize Phoenix account at \nhttps://app.phoenix.arize.com/login\nFor more details, see \nHosted Phoenix\nLaunch Phoenix\nLaunching phoenix can be done in many ways depending on your use-case.\nCommand Line\nDocker\nNotebook\napp.phoenix.arize.com\nLaunch your local Phoenix instance using:\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \nserve\nFor details on customizing a local terminal deployment, see \nTerminal Setup\n.\nLaunch the phoenix docker image using:\nCopy\ndocker\n \nrun\n \n-p\n \n6006:6006\n \n-p\n \n4317:4317\n \narizephoenix/phoenix:latest\nThis will expose the Phoenix UI and REST API on \nlocalhost:6006\n and exposes the gRPC endpoint for spans on \nlocalhost:4317\nFor more details on customizing a docker deployment, see \nDocker\nWithin your notebook, launch Phoenix using:\nCopy\nimport\n phoenix \nas\n px\n\n\npx\n.\nlaunch_app\n()\nBy default, notebook instances do not have persistent storage, so your traces will disappear after the notebook is closed. See \nPersistence\n or use one of the other deployment options to retain traces.\nHosted Phoenix instances are always online. Nothing more to do here!\nConnect your App\nTo collect traces from your application, you must configure an OpenTelemetry TracerProvider to send traces to Phoenix. The \nregister\n utility from the \nphoenix.otel\n module streamlines this process.\nPython\nTypeScript\nNotebook\napp.phoenix.arize.com\nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your instance using:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n  \nYou do not have to use phoenix.otel to connect to your phoenix instance, you can use OpenTelemetry itself to initialize  your OTEL connection. See\nUsing OTEL Python Directly\nSee \nSetup Tracing: Python\n for more details on configuration and setup\nFor setting up tracing and OpenTelemetry with TypeScript, see \nSetup Tracing: TS\nConnect your notebook to Phoenix:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n \nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your cloud instance using:\nCopy\nimport\n os\n\n\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# Add Phoenix API Key for tracing\n\n\nos\n.\nenviron\n[\n""OTEL_EXPORTER_OTLP_HEADERS""\n]\n \n=\n f\n""api_key=\n{PHOENIX_API_KEY}\n""\n\n\n\n\n# configure the Phoenix tracer\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""https://app.phoenix.arize.com/v1/traces""\n,\n\n\n)\n \nYour Phoenix API key can be found on the Keys section of your dashboard.\nNext Steps\nTrace\n a running application\nRun \nevaluations\n on traces\nTest changes to you prompts, models, and application via \nexperiments\nPrevious\nArize Phoenix\nNext\nHosted Phoenix\nLast updated \n9 hours ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can users create a new session in Arize Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Users can create a new session in Arize Phoenix by launching their local Phoenix instance using Python, running the Phoenix docker image, or launching Phoenix within a notebook.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/setup\ntitle: Setup\n\nSetup\nHow to get started with Phoenix\nHow to run in different environments\nHosted Phoenix by Arize\nIn a python notebook\nFrom the command line\nIn a Container\nHow to configure phoenix\nAvailable ports to run phoenix\nCustomize phoenix using environment variables\nPrevious\nExamples\nNext\nEnvironments\nLast updated \n8 hours ago\n\nsource: https://docs.arize.com/phoenix/quickstart\ntitle: Quickstart\n\nQuickstart\nThe easiest way to run Phoenix is locally in your own computer. To launch Phoenix, use the following steps.\nInstall\nUsing pip\nUsing conda\nDocker\napp.phoenix.arize.com\nCopy\npip\n \ninstall\n \narize-phoenix\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix\nPhoenix server images are available via \nDocker Hub\n and can be used via \ndocker compose \nor if you simply want a long-running phoenix instance to share with your team.\nCopy\ndocker\n \npull\n \narizephoenix/phoenix:latest\nIf you don\'t want to host an instance of Phoenix yourself or use a notebook instance, you can use a persistent instance provided on our site. Sign up for an Arize Phoenix account at \nhttps://app.phoenix.arize.com/login\nFor more details, see \nHosted Phoenix\nLaunch Phoenix\nLaunching phoenix can be done in many ways depending on your use-case.\nCommand Line\nDocker\nNotebook\napp.phoenix.arize.com\nLaunch your local Phoenix instance using:\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \nserve\nFor details on customizing a local terminal deployment, see \nTerminal Setup\n.\nLaunch the phoenix docker image using:\nCopy\ndocker\n \nrun\n \n-p\n \n6006:6006\n \n-p\n \n4317:4317\n \narizephoenix/phoenix:latest\nThis will expose the Phoenix UI and REST API on \nlocalhost:6006\n and exposes the gRPC endpoint for spans on \nlocalhost:4317\nFor more details on customizing a docker deployment, see \nDocker\nWithin your notebook, launch Phoenix using:\nCopy\nimport\n phoenix \nas\n px\n\n\npx\n.\nlaunch_app\n()\nBy default, notebook instances do not have persistent storage, so your traces will disappear after the notebook is closed. See \nPersistence\n or use one of the other deployment options to retain traces.\nHosted Phoenix instances are always online. Nothing more to do here!\nConnect your App\nTo collect traces from your application, you must configure an OpenTelemetry TracerProvider to send traces to Phoenix. The \nregister\n utility from the \nphoenix.otel\n module streamlines this process.\nPython\nTypeScript\nNotebook\napp.phoenix.arize.com\nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your instance using:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n  \nYou do not have to use phoenix.otel to connect to your phoenix instance, you can use OpenTelemetry itself to initialize  your OTEL connection. See\nUsing OTEL Python Directly\nSee \nSetup Tracing: Python\n for more details on configuration and setup\nFor setting up tracing and OpenTelemetry with TypeScript, see \nSetup Tracing: TS\nConnect your notebook to Phoenix:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n \nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your cloud instance using:\nCopy\nimport\n os\n\n\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# Add Phoenix API Key for tracing\n\n\nos\n.\nenviron\n[\n""OTEL_EXPORTER_OTLP_HEADERS""\n]\n \n=\n f\n""api_key=\n{PHOENIX_API_KEY}\n""\n\n\n\n\n# configure the Phoenix tracer\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""https://app.phoenix.arize.com/v1/traces""\n,\n\n\n)\n \nYour Phoenix API key can be found on the Keys section of your dashboard.\nNext Steps\nTrace\n a running application\nRun \nevaluations\n on traces\nTest changes to you prompts, models, and application via \nexperiments\nPrevious\nArize Phoenix\nNext\nHosted Phoenix\nLast updated \n9 hours ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can users create a new session in Arize Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Users can create a new session in Arize Phoenix by launching their local Phoenix instance using Python, running the Phoenix docker image, or launching Phoenix within a notebook.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NzQ=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/llm-evals\ntitle: Overview: Evals\n\nOverview: Evals\nThe standard for evaluating text is human labeling. However, high-quality LLM outputs are becoming cheaper and faster to produce, and human evaluation cannot scale. In this context, evaluating the performance of LLM applications is best tackled by using a separate evaluation LLM. The Phoenix \nLLM Evals library\n is designed for simple, fast, and accurate LLM-based evaluations. \nSimple callback system integration for applying to spans on LangChain and LlamaIndex\nSupport for one-click explanations \nFast on Batches- Async concurrent fast parallel rate limit management for API calls \nCustom dataset support and custom Eval creation support\nPre-tested Evaluations with model benchmarks \nExtensive support for RAG Evals: Benchmarking scripts, retrieval Evals and citation Evals \nThe Problem with LLM Evaluations\nMost evaluation libraries do NOT follow trustworthy benchmarking rigor necessary for production environments. Production LLM Evals need to benchmark both a model \nand\n ""\na prompt template"". (i.e. the Open AI “model” Evals only focuses on evaluating the model, a different use case\n). \nThere is typically difficulty integrating benchmarking, development, production, or the LangChain/LlamaIndex callback system. Evals should process batches of data with optimal speed. \nObligation to use chain abstractions (i.e.\n LangChain shouldn\'t be a prerequisite for obtaining evaluations for pipelines that don\'t utilize it)\n.  \nOur Solution: Phoenix LLM Evals\n1. Support for Pre-Tested Eval Templates & custom eval templates\nPhoenix provides pretested eval templates and convenience functions for a set of common Eval “tasks”. Learn more about pretested templates \nhere\n. This library is split into high-level functions to easily run rigorously \npre-tested functions\n and building blocks to modify and \ncreate your own Evals\n.\n2. Data Science Rigor when Benchmarking Evals for Reproducible Results \nThe Phoenix team is dedicated to testing model and template combinations and is continually improving templates for optimized performance. Find the most up-to-date template on \nGitHub\n.\n3. Designed for Throughput \nPhoenix evals are designed to run as fast as possible on batches of Eval data and maximize the throughput and usage of your API key. The current Phoenix library is 10x faster in throughput than current call-by-call-based approaches integrated into the LLM App Framework Evals.\n4. Run the Same Evals in Different Environments (Notebooks, python pipelines, Langchain/LlamaIndex callbacks)  \nPhoenix Evals are designed to run on dataframes, in Python pipelines, or in LangChain & LlamaIndex callbacks. Evals are also supported in Python pipelines for normal LLM deployments not using LlamaIndex or LangChain. There is also one-click support for Langchain and LlamaIndx support. \n5. Run Evals on Span and Chain Level \n Evals are supported on a span level for LangChain and LlamaIndex. \nPrevious\nFAQs: Tracing\nNext\nQuickstart: Evals\nLast updated \n5 months ago\n\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix\'s tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you\'ll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they\'re geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix\'s search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix\'s flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix\'s evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How does Arize Phoenix Evals help in assessing the performance of LLM applications?\nAnswer: '}, {'role': 'assistant', 'content': 'Arize Phoenix Evals helps in assessing the performance of LLM applications by providing support for pre-tested evaluation templates, custom evaluation templates, and rigorous benchmarking for reproducible results. It is designed to run evaluations as fast as possible on batches of evaluation data, maximizing throughput and API key usage. Additionally, Phoenix Evals allows evaluations to be run in different environments such as notebooks, Python pipelines, and LangChain/LlamaIndex callbacks, supporting evaluations on both span and chain levels.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/llm-evals\ntitle: Overview: Evals\n\nOverview: Evals\nThe standard for evaluating text is human labeling. However, high-quality LLM outputs are becoming cheaper and faster to produce, and human evaluation cannot scale. In this context, evaluating the performance of LLM applications is best tackled by using a separate evaluation LLM. The Phoenix \nLLM Evals library\n is designed for simple, fast, and accurate LLM-based evaluations. \nSimple callback system integration for applying to spans on LangChain and LlamaIndex\nSupport for one-click explanations \nFast on Batches- Async concurrent fast parallel rate limit management for API calls \nCustom dataset support and custom Eval creation support\nPre-tested Evaluations with model benchmarks \nExtensive support for RAG Evals: Benchmarking scripts, retrieval Evals and citation Evals \nThe Problem with LLM Evaluations\nMost evaluation libraries do NOT follow trustworthy benchmarking rigor necessary for production environments. Production LLM Evals need to benchmark both a model \nand\n ""\na prompt template"". (i.e. the Open AI “model” Evals only focuses on evaluating the model, a different use case\n). \nThere is typically difficulty integrating benchmarking, development, production, or the LangChain/LlamaIndex callback system. Evals should process batches of data with optimal speed. \nObligation to use chain abstractions (i.e.\n LangChain shouldn\'t be a prerequisite for obtaining evaluations for pipelines that don\'t utilize it)\n.  \nOur Solution: Phoenix LLM Evals\n1. Support for Pre-Tested Eval Templates & custom eval templates\nPhoenix provides pretested eval templates and convenience functions for a set of common Eval “tasks”. Learn more about pretested templates \nhere\n. This library is split into high-level functions to easily run rigorously \npre-tested functions\n and building blocks to modify and \ncreate your own Evals\n.\n2. Data Science Rigor when Benchmarking Evals for Reproducible Results \nThe Phoenix team is dedicated to testing model and template combinations and is continually improving templates for optimized performance. Find the most up-to-date template on \nGitHub\n.\n3. Designed for Throughput \nPhoenix evals are designed to run as fast as possible on batches of Eval data and maximize the throughput and usage of your API key. The current Phoenix library is 10x faster in throughput than current call-by-call-based approaches integrated into the LLM App Framework Evals.\n4. Run the Same Evals in Different Environments (Notebooks, python pipelines, Langchain/LlamaIndex callbacks)  \nPhoenix Evals are designed to run on dataframes, in Python pipelines, or in LangChain & LlamaIndex callbacks. Evals are also supported in Python pipelines for normal LLM deployments not using LlamaIndex or LangChain. There is also one-click support for Langchain and LlamaIndx support. \n5. Run Evals on Span and Chain Level \n Evals are supported on a span level for LangChain and LlamaIndex. \nPrevious\nFAQs: Tracing\nNext\nQuickstart: Evals\nLast updated \n5 months ago\n\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix\'s tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you\'ll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they\'re geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix\'s search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix\'s flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix\'s evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How does Arize Phoenix Evals help in assessing the performance of LLM applications?\nAnswer: '}, {'role': 'assistant', 'content': 'Arize Phoenix Evals helps in assessing the performance of LLM applications by providing support for pre-tested evaluation templates, custom evaluation templates, and rigorous benchmarking for reproducible results. It is designed to run evaluations as fast as possible on batches of evaluation data, maximizing throughput and API key usage. Additionally, Phoenix Evals allows evaluations to be run in different environments such as notebooks, Python pipelines, and LangChain/LlamaIndex callbacks, supporting evaluations on both span and chain levels.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NzU=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals\ntitle: How to: Evals\n\nHow to: Evals\nPhoenix Evaluators\nHallucinations\nQ&A on Retrieved Data\nRetrieval (RAG) Relevance\nSummarization \nCode Generation\nToxicity\n \nAI vs Human \nReference (Citation) Eval\nBring Your Own Evaluator\nCategorical evaluator\n (llm_classify)\nNumeric evaluator\n (llm_generate)\nOnline Evals\nRun evaluations via a job to visualize in the UI as traces stream in.\nPrevious\nCustom Task Evaluation\nNext\nUse Phoenix Evaluators\nLast updated \n5 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals\ntitle: Concepts: Evals\n\nConcepts: Evals\nHow to leverage Evals to validate and monitor your application\nLLM as a Judge\nEval Data Types\nEvals With Explanations\nEvaluators\nRetrieval Evaluations\nResponse Evaluations\nEvaluation Results\nEvaluating Traces\nCustom Task Evaluations\nPrevious\nQuickstart: Evals\nNext\nLLM as a Judge\nLast updated \n29 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can users create custom evaluations in Arize Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Users can create custom evaluations in Arize Phoenix by utilizing the Custom Task Evaluation feature provided in the platform.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals\ntitle: How to: Evals\n\nHow to: Evals\nPhoenix Evaluators\nHallucinations\nQ&A on Retrieved Data\nRetrieval (RAG) Relevance\nSummarization \nCode Generation\nToxicity\n \nAI vs Human \nReference (Citation) Eval\nBring Your Own Evaluator\nCategorical evaluator\n (llm_classify)\nNumeric evaluator\n (llm_generate)\nOnline Evals\nRun evaluations via a job to visualize in the UI as traces stream in.\nPrevious\nCustom Task Evaluation\nNext\nUse Phoenix Evaluators\nLast updated \n5 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals\ntitle: Concepts: Evals\n\nConcepts: Evals\nHow to leverage Evals to validate and monitor your application\nLLM as a Judge\nEval Data Types\nEvals With Explanations\nEvaluators\nRetrieval Evaluations\nResponse Evaluations\nEvaluation Results\nEvaluating Traces\nCustom Task Evaluations\nPrevious\nQuickstart: Evals\nNext\nLLM as a Judge\nLast updated \n29 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can users create custom evaluations in Arize Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Users can create custom evaluations in Arize Phoenix by utilizing the Custom Task Evaluation feature provided in the platform.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6NzY=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/evaluation-models\ntitle: Evaluation Models\n\nEvaluation Models\narize-phoenix-evals\n supports a large set of foundation models for Evals such as:\nOpenAI\nVertex AI\nAzure Open AI\nAnthropic\nMixtral/Mistral\nAWS Bedrock\nFalcon\nCode Llama\nLlama3\nDeepseek\nDeberta\nDBRX\nQwen\nAnd many more.\nThere are direct model integrations in Phoenix and indirect model integrations (e.x. local modals) through \nLiteLLM\n.\nDirect Integrations:\nThese integrations are native to the Phoenix Evals package and have better throughput, rate limit and error management.\nVertex AI\nOpenAI\nAzure OpenAI\nAnthropic\nMistral\nPrevious\nOnline Evals\nNext\nOverview: Datasets\nLast updated \n7 days ago\n\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals\ntitle: How to: Evals\n\nHow to: Evals\nPhoenix Evaluators\nHallucinations\nQ&A on Retrieved Data\nRetrieval (RAG) Relevance\nSummarization \nCode Generation\nToxicity\n \nAI vs Human \nReference (Citation) Eval\nBring Your Own Evaluator\nCategorical evaluator\n (llm_classify)\nNumeric evaluator\n (llm_generate)\nOnline Evals\nRun evaluations via a job to visualize in the UI as traces stream in.\nPrevious\nCustom Task Evaluation\nNext\nUse Phoenix Evaluators\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Where can users find detailed documentation on using Arize Phoenix Evals?\nAnswer: '}, {'role': 'assistant', 'content': 'Users can find detailed documentation on using Arize Phoenix Evals on the Arize website under the ""Evaluation Models"" and ""How to: Evals"" sections.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/evaluation-models\ntitle: Evaluation Models\n\nEvaluation Models\narize-phoenix-evals\n supports a large set of foundation models for Evals such as:\nOpenAI\nVertex AI\nAzure Open AI\nAnthropic\nMixtral/Mistral\nAWS Bedrock\nFalcon\nCode Llama\nLlama3\nDeepseek\nDeberta\nDBRX\nQwen\nAnd many more.\nThere are direct model integrations in Phoenix and indirect model integrations (e.x. local modals) through \nLiteLLM\n.\nDirect Integrations:\nThese integrations are native to the Phoenix Evals package and have better throughput, rate limit and error management.\nVertex AI\nOpenAI\nAzure OpenAI\nAnthropic\nMistral\nPrevious\nOnline Evals\nNext\nOverview: Datasets\nLast updated \n7 days ago\n\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals\ntitle: How to: Evals\n\nHow to: Evals\nPhoenix Evaluators\nHallucinations\nQ&A on Retrieved Data\nRetrieval (RAG) Relevance\nSummarization \nCode Generation\nToxicity\n \nAI vs Human \nReference (Citation) Eval\nBring Your Own Evaluator\nCategorical evaluator\n (llm_classify)\nNumeric evaluator\n (llm_generate)\nOnline Evals\nRun evaluations via a job to visualize in the UI as traces stream in.\nPrevious\nCustom Task Evaluation\nNext\nUse Phoenix Evaluators\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Where can users find detailed documentation on using Arize Phoenix Evals?\nAnswer: '}, {'role': 'assistant', 'content': 'Users can find detailed documentation on using Arize Phoenix Evals on the Arize website under the ""Evaluation Models"" and ""How to: Evals"" sections.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6Nzc=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals/online-evals\ntitle: Online Evals\n\nOnline Evals\nYou can use cron to run evals client-side as your traces and spans are generated, augmenting your dataset with evaluations in an online manner. View the \nexample in Github\n.\nThis example:\nContinuously queries a LangChain application to send new traces and spans to your Phoenix session\nQueries new spans once per minute and runs evals, including:\nHallucination\nQ&A Correctness\nRelevance\nLogs evaluations back to Phoenix so they appear in the UI\nThe evaluation script is run as a cron job, enabling you to adjust the frequency of the evaluation job:\nCopy\n* * * * * /path/to/python /path/to/run_evals.py\nThe above script can be run periodically to augment Evals in Phoenix.\nPrevious\nBring Your Own Evaluator\nNext\nEvaluation Models\nLast updated \n5 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/evals\ntitle: Quickstart: Evals\n\nQuickstart: Evals\nEvaluate your LLM application with Phoenix\nThis quickstart guide will show you through the basics of evaluating data from your LLM application.\n1. Install Phoenix Evals\nUsing pip\nUsing conda\nCopy\npip install \n""arize-phoenix[evals]""\nCopy\nconda install -c conda-forge arize-phoenix[evals]\n2. Export Data and Launch Phoenix\nExport a dataframe from your Phoenix session that contains traces from your LLM application.\nIf you are interested in a subset of your data, you can export with a custom query. Learn more \nhere\n.\nFor the sake of this guide, we\'ll download some pre-existing trace data collected from a LlamaIndex application (in practice, this data would be collected by \ninstrumenting your LLM application\n with an OpenInference-compatible tracer).\nCopy\nfrom\n urllib\n.\nrequest \nimport\n urlopen\n\n\nfrom\n phoenix\n.\ntrace\n.\ntrace_dataset \nimport\n TraceDataset\n\n\nfrom\n phoenix\n.\ntrace\n.\nutils \nimport\n json_lines_to_df\n\n\n\n\n# Replace with the URL to your trace data\n\n\ntraces_url = ""https://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/context-retrieval/trace.jsonl""\n\n\nwith\n \nurlopen\n(traces_url)\n \nas\n response\n:\n\n\n    lines \n=\n [line\n.\ndecode\n(\n""utf-8""\n)\n \nfor\n line \nin\n response\n.\nreadlines\n()\n]\n\n\ntrace_ds \n=\n \nTraceDataset\n(\njson_lines_to_df\n(lines))\nThen, start Phoenix to view and manage your evaluations.\nCopy\nimport\n phoenix \nas\n px\n\n\nsession \n=\n px\n.\nlaunch_app\n(trace\n=\ntrace_ds)\n\n\nsession\n.\nview\n()\nYou should now see a view like this. If not, toggle the time range to ""All Time"" as the default ""Last 7 Days"" may not adequately represent the time range of the traces.\nA view of the Phoenix UI prior to adding evaluation annotations\n3. Evaluate and Log Results\nSet up evaluators (in this case for hallucinations and Q&A correctness), run the evaluations, and log the results to visualize them in Phoenix.\nCopy\n!pip install openai\n\n\n\n\nfrom\n phoenix\n.\nevals \nimport\n OpenAIModel\n,\n HallucinationEvaluator\n,\n QAEvaluator\n\n\nfrom\n phoenix\n.\nevals \nimport\n run_evals\n\n\nimport\n nest_asyncio\n\n\nnest_asyncio\n.\napply\n()\n  \n# This is needed for concurrency in notebook environments\n\n\n\n\n# Set your OpenAI API key\n\n\napi_key \n=\n \n""your-api-key""\n  \n# Replace with your actual API key\n\n\neval_model \n=\n \nOpenAIModel\n(model\n=\n""gpt-4-turbo-preview""\n, api_key\n=\napi_key)\n\n\n\n\n# Define your evaluators\n\n\nhallucination_evaluator \n=\n \nHallucinationEvaluator\n(eval_model)\n\n\nqa_evaluator \n=\n \nQAEvaluator\n(eval_model)\n\n\n\n\n# Assume \'queries_df\' is your input dataframe \n\n\n# for `hallucination_evaluator` your input df needs to have columns \'output\', \'input\', \'context\'\n\n\n# for `qa_evaluator` your input df needs to have columns \'output\', \'input\', \'reference\'\n\n\nassert\n \nall\n(column \nin\n queries_df.columns \nfor\n column \nin\n [\n\'output\'\n, \n\'input\'\n, \n\'context\'\n, \n\'reference\'\n])\n\n\n\n\n# Run the evaluators, each evaluator will return a dataframe with evaluation results\n\n\n# We upload the evaluation results to Phoenix in the next step\n\n\nhallucination_eval_df\n,\n qa_eval_df \n=\n \nrun_evals\n(\n\n\n    dataframe\n=\nqueries_df,\n\n\n    evaluators\n=\n[hallucination_evaluator, qa_evaluator],\n\n\n    provide_explanation\n=\nTrue\n\n\n)\n\n\n\n\n# Log the evaluations\n\n\nfrom\n phoenix\n.\ntrace \nimport\n SpanEvaluations\n\n\n\n\npx\n.\nClient\n().\nlog_evaluations\n(\n\n\n    \nSpanEvaluations\n(eval_name\n=\n""Hallucination""\n, dataframe\n=\nhallucination_eval_df),\n\n\n    \nSpanEvaluations\n(eval_name\n=\n""QA Correctness""\n, dataframe\n=\nqa_eval_df)\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Is there a way to log_evaluation to hosted phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Yes, there is a way to log evaluations to hosted Phoenix.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals/online-evals\ntitle: Online Evals\n\nOnline Evals\nYou can use cron to run evals client-side as your traces and spans are generated, augmenting your dataset with evaluations in an online manner. View the \nexample in Github\n.\nThis example:\nContinuously queries a LangChain application to send new traces and spans to your Phoenix session\nQueries new spans once per minute and runs evals, including:\nHallucination\nQ&A Correctness\nRelevance\nLogs evaluations back to Phoenix so they appear in the UI\nThe evaluation script is run as a cron job, enabling you to adjust the frequency of the evaluation job:\nCopy\n* * * * * /path/to/python /path/to/run_evals.py\nThe above script can be run periodically to augment Evals in Phoenix.\nPrevious\nBring Your Own Evaluator\nNext\nEvaluation Models\nLast updated \n5 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/evals\ntitle: Quickstart: Evals\n\nQuickstart: Evals\nEvaluate your LLM application with Phoenix\nThis quickstart guide will show you through the basics of evaluating data from your LLM application.\n1. Install Phoenix Evals\nUsing pip\nUsing conda\nCopy\npip install \n""arize-phoenix[evals]""\nCopy\nconda install -c conda-forge arize-phoenix[evals]\n2. Export Data and Launch Phoenix\nExport a dataframe from your Phoenix session that contains traces from your LLM application.\nIf you are interested in a subset of your data, you can export with a custom query. Learn more \nhere\n.\nFor the sake of this guide, we\'ll download some pre-existing trace data collected from a LlamaIndex application (in practice, this data would be collected by \ninstrumenting your LLM application\n with an OpenInference-compatible tracer).\nCopy\nfrom\n urllib\n.\nrequest \nimport\n urlopen\n\n\nfrom\n phoenix\n.\ntrace\n.\ntrace_dataset \nimport\n TraceDataset\n\n\nfrom\n phoenix\n.\ntrace\n.\nutils \nimport\n json_lines_to_df\n\n\n\n\n# Replace with the URL to your trace data\n\n\ntraces_url = ""https://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/context-retrieval/trace.jsonl""\n\n\nwith\n \nurlopen\n(traces_url)\n \nas\n response\n:\n\n\n    lines \n=\n [line\n.\ndecode\n(\n""utf-8""\n)\n \nfor\n line \nin\n response\n.\nreadlines\n()\n]\n\n\ntrace_ds \n=\n \nTraceDataset\n(\njson_lines_to_df\n(lines))\nThen, start Phoenix to view and manage your evaluations.\nCopy\nimport\n phoenix \nas\n px\n\n\nsession \n=\n px\n.\nlaunch_app\n(trace\n=\ntrace_ds)\n\n\nsession\n.\nview\n()\nYou should now see a view like this. If not, toggle the time range to ""All Time"" as the default ""Last 7 Days"" may not adequately represent the time range of the traces.\nA view of the Phoenix UI prior to adding evaluation annotations\n3. Evaluate and Log Results\nSet up evaluators (in this case for hallucinations and Q&A correctness), run the evaluations, and log the results to visualize them in Phoenix.\nCopy\n!pip install openai\n\n\n\n\nfrom\n phoenix\n.\nevals \nimport\n OpenAIModel\n,\n HallucinationEvaluator\n,\n QAEvaluator\n\n\nfrom\n phoenix\n.\nevals \nimport\n run_evals\n\n\nimport\n nest_asyncio\n\n\nnest_asyncio\n.\napply\n()\n  \n# This is needed for concurrency in notebook environments\n\n\n\n\n# Set your OpenAI API key\n\n\napi_key \n=\n \n""your-api-key""\n  \n# Replace with your actual API key\n\n\neval_model \n=\n \nOpenAIModel\n(model\n=\n""gpt-4-turbo-preview""\n, api_key\n=\napi_key)\n\n\n\n\n# Define your evaluators\n\n\nhallucination_evaluator \n=\n \nHallucinationEvaluator\n(eval_model)\n\n\nqa_evaluator \n=\n \nQAEvaluator\n(eval_model)\n\n\n\n\n# Assume \'queries_df\' is your input dataframe \n\n\n# for `hallucination_evaluator` your input df needs to have columns \'output\', \'input\', \'context\'\n\n\n# for `qa_evaluator` your input df needs to have columns \'output\', \'input\', \'reference\'\n\n\nassert\n \nall\n(column \nin\n queries_df.columns \nfor\n column \nin\n [\n\'output\'\n, \n\'input\'\n, \n\'context\'\n, \n\'reference\'\n])\n\n\n\n\n# Run the evaluators, each evaluator will return a dataframe with evaluation results\n\n\n# We upload the evaluation results to Phoenix in the next step\n\n\nhallucination_eval_df\n,\n qa_eval_df \n=\n \nrun_evals\n(\n\n\n    dataframe\n=\nqueries_df,\n\n\n    evaluators\n=\n[hallucination_evaluator, qa_evaluator],\n\n\n    provide_explanation\n=\nTrue\n\n\n)\n\n\n\n\n# Log the evaluations\n\n\nfrom\n phoenix\n.\ntrace \nimport\n SpanEvaluations\n\n\n\n\npx\n.\nClient\n().\nlog_evaluations\n(\n\n\n    \nSpanEvaluations\n(eval_name\n=\n""Hallucination""\n, dataframe\n=\nhallucination_eval_df),\n\n\n    \nSpanEvaluations\n(eval_name\n=\n""QA Correctness""\n, dataframe\n=\nqa_eval_df)\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Is there a way to log_evaluation to hosted phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Yes, there is a way to log evaluations to hosted Phoenix.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6Nzg=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals/retrieval-rag-relevance\ntitle: Retrieval (RAG) Relevance\n\nRetrieval (RAG) Relevance\nWhen To Use RAG Eval Template\nThis Eval evaluates whether a retrieved chunk contains an answer to the query. It\'s extremely useful for evaluating retrieval systems.\nRAG Eval Template\nGoogle Colaboratory\nTry it out!\nCopy\nYou are comparing a reference text to a question \nand\n trying to determine \nif\n the reference text\n\n\ncontains information relevant to answering the question\n.\n Here \nis\n the data\n:\n\n\n    [BEGIN DATA]\n\n\n    \n************\n\n\n    [Question]\n:\n \n{\nquery\n}\n\n\n    \n************\n\n\n    [Reference text]\n:\n \n{\nreference\n}\n\n\n    [END DATA]\n\n\n\n\nCompare the Question above to the Reference text\n.\n You must determine whether the Reference text\n\n\ncontains information that can answer the Question\n.\n Please focus on whether the very specific\n\n\nquestion can be answered by the information \nin\n the Reference text\n.\n\n\nYour response must be single word\n,\n either \n""relevant""\n \nor\n \n""unrelated""\n,\n\n\nand\n should \nnot\n contain \nany\n text \nor\n characters aside \nfrom\n that word\n.\n\n\n""unrelated""\n means that the reference text does \nnot\n contain an answer to the Question\n.\n\n\n""relevant""\n means the reference text contains an answer to the Question\n.\nWe are continually iterating our templates, view the most up-to-date template on GitHub. Last updated on 10/12/2023\nBenchmark Results\nGPT-4 Result\nGPT-3.5 Results\nClaude V2 Results\nGPT 4 Turbo\nHow To Run the Eval\nCopy\nfrom\n phoenix\n.\nevals \nimport\n (\n\n\n    RAG_RELEVANCY_PROMPT_RAILS_MAP\n,\n\n\n    RAG_RELEVANCY_PROMPT_TEMPLATE\n,\n\n\n    OpenAIModel\n,\n\n\n    download_benchmark_dataset\n,\n\n\n    llm_classify\n,\n\n\n)\n\n\n\n\nmodel \n=\n \nOpenAIModel\n(\n\n\n    model_name\n=\n""gpt-4""\n,\n\n\n    temperature\n=\n0.0\n,\n\n\n)\n\n\n\n\n#The rails is used to hold the output to specific values based on the template\n\n\n#It will remove text such as "",,,"" or ""...""\n\n\n#Will ensure the binary value expected from the template is returned\n\n\nrails \n=\n \nlist\n(RAG_RELEVANCY_PROMPT_RAILS_MAP.\nvalues\n())\n\n\nrelevance_classifications \n=\n \nllm_classify\n(\n\n\n    dataframe\n=\ndf,\n\n\n    template\n=\nRAG_RELEVANCY_PROMPT_TEMPLATE,\n\n\n    model\n=\nmodel,\n\n\n    rails\n=\nrails,\n\n\n    provide_explanation\n=\nTrue\n, \n#optional to generate explanations for the value produced by the eval LLM\n\n\n)\nThe above runs the RAG relevancy LLM template against the dataframe df.\nRAG Eval\nGPT-4o\nGPT-4\nGPT-4 Turbo\nGemini Pro\nGPT-3.5\nPalm (Text Bison)\nClaude V2\nPrecision\n0.60\n0.70\n0.68\n0.61\n0.42\n0.53\n0.79\nRecall\n0.77\n0.88\n0.91\n1\n1.0\n1\n0.22\nF1\n0.67\n0.78\n0.78\n0.76\n0.59\n0.69\n0.34\nThroughput\nGPT-4\nGPT-4 Turbo\nGPT-3.5\n100 Samples\n113 Sec\n61 sec\n73 Sec\nPrevious\nQ&A on Retrieved Data\nNext\nSummarization\nLast updated \n2 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/evaluation\ntitle: Evaluators\n\nEvaluators\nEvaluation and benchmarking are crucial concepts in LLM development. To improve the performance of an LLM app (RAG, agents), you must have a way to measure it.\nPhoenix offers key modules to measure the quality of generated results as well as modules to measure retrieval quality.\nResponse Evaluation\n: Does the response match the retrieved context? Does it also match the query?\nRetrieval Evaluation\n: Are the retrieved sources relevant to the query?\nResponse Evaluation\nEvaluation of generated results can be challenging. Unlike traditional ML, the predicted results are not numeric or categorical, making it hard to define quantitative metrics for this problem.\nPhoenix offers \nLLM Evaluations\n, a module designed to measure the quality of results. This module uses a ""gold"" LLM (e.g. GPT-4) to decide whether the generated answer is correct in a variety of ways.\n\nNote that many of these evaluation criteria DO NOT require ground-truth labels. Evaluation can be done simply with a combination of the \ninput\n (query), \noutput\n (response), and \ncontext\n.\nLLM Evals supports the following response evaluation criteria:\nQA Correctness\n - Whether a question was correctly answered by the system based on the retrieved data. In contrast to retrieval Evals that are checks on chunks of data returned, this check is a system level check of a correct Q&A.\nHallucinations\n \n-\n Designed to detect LLM hallucinations relative to retrieved context\nToxicity\n - Identify if the AI response is racist, biased, or toxic\nResponse evaluations are a critical first step to figuring out whether your LLM App is running correctly. Response evaluations can pinpoint specific executions (a.k.a. traces) that are performing badly and can be aggregated up so that you can track how your application is running as a whole.\nEvaluations can be aggregated across executions to be used as KPIs\nRetrieval Evaluation\nPhoenix also provides evaluation of retrieval independently.\nThe concept of retrieval evaluation is not new; given a set of relevance scores for a set of retrieved documents, we can evaluate retrievers using retrieval metrics like \nprecision\n, \nNDCG\n, \nhit rate\n and more.\nLLM Evals supports the following retrieval evaluation criteria:\nRelevance\n - Evaluates whether a retrieved document chunk contains an answer to the query.\nRetrieval Evaluations can be run directly on application traces\nRetrieval is possibly the most important step in any LLM application as poor and/or incorrect retrieval can be the cause of bad response generation. If your application uses RAG to power an LLM, retrieval evals can help you identify the cause of hallucinations and incorrect answers.\nEvaluations\nInferences that contain generative records can be fed into evals to produce evaluations for analysis\nWith Phoenix\'s LLM Evals, evaluation results (or just \nEvaluations\n for short) is data consisting of 3 main columns:\nlabel\n: str [optional] - a classification label for the evaluation (e.g. ""hallucinated"" vs ""factual""). Can be used to calculate percentages (e.g. percent hallucinated) and can be used to filter down your data (e.g. \nEvals[""Hallucinations""].label == ""hallucinated""\n)\nscore\n: number [optional] - a numeric score for the evaluation (e.g. 1 for good, 0 for bad). Scores are great way to sort your data to surface poorly performing examples and can be used to filter your data by a threshold.\nexplanation\n: str [optional] - the reasoning for why the evaluation label or score was given. In the case of LLM evals, this is the evaluation model\'s reasoning. While explanations are optional, they can be extremely useful when trying to understand problematic areas of your application.\nLet\'s take a look at an example list of \nQ&A relevance\n evaluations:\nlabel\nexplanation\nscore\ncorrect\nThe reference text explains that YC was not or...\n1\ncorrect\nTo determine if the answer is correct, we need...\n1\nincorrect\nTo determine if the answer is correct, we must...\n0\ncorrect\nTo determine if the answer is correct, we need...\n1\nThese three columns combined can drive any type of evaluation you can imagine. \nlabel\n provides a way to classify responses, \nscore\n provides a way to assign a numeric assessment, and \nexplanation\n gives you a way to get qualitative feedback.\nEvaluating Traces\nAdding evaluations on traces can highlight problematic areas that require further analysis\nWith Phoenix, evaluations can be ""attached"" to the \nspans\n and \ndocuments\n collected. In order to facilitate this, Phoenix supports the following steps.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Is there a way to reduce NOT_PARSABLE responses during relevance evaluation?\nAnswer: '}, {'role': 'assistant', 'content': 'relevant'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals/retrieval-rag-relevance\ntitle: Retrieval (RAG) Relevance\n\nRetrieval (RAG) Relevance\nWhen To Use RAG Eval Template\nThis Eval evaluates whether a retrieved chunk contains an answer to the query. It\'s extremely useful for evaluating retrieval systems.\nRAG Eval Template\nGoogle Colaboratory\nTry it out!\nCopy\nYou are comparing a reference text to a question \nand\n trying to determine \nif\n the reference text\n\n\ncontains information relevant to answering the question\n.\n Here \nis\n the data\n:\n\n\n    [BEGIN DATA]\n\n\n    \n************\n\n\n    [Question]\n:\n \n{\nquery\n}\n\n\n    \n************\n\n\n    [Reference text]\n:\n \n{\nreference\n}\n\n\n    [END DATA]\n\n\n\n\nCompare the Question above to the Reference text\n.\n You must determine whether the Reference text\n\n\ncontains information that can answer the Question\n.\n Please focus on whether the very specific\n\n\nquestion can be answered by the information \nin\n the Reference text\n.\n\n\nYour response must be single word\n,\n either \n""relevant""\n \nor\n \n""unrelated""\n,\n\n\nand\n should \nnot\n contain \nany\n text \nor\n characters aside \nfrom\n that word\n.\n\n\n""unrelated""\n means that the reference text does \nnot\n contain an answer to the Question\n.\n\n\n""relevant""\n means the reference text contains an answer to the Question\n.\nWe are continually iterating our templates, view the most up-to-date template on GitHub. Last updated on 10/12/2023\nBenchmark Results\nGPT-4 Result\nGPT-3.5 Results\nClaude V2 Results\nGPT 4 Turbo\nHow To Run the Eval\nCopy\nfrom\n phoenix\n.\nevals \nimport\n (\n\n\n    RAG_RELEVANCY_PROMPT_RAILS_MAP\n,\n\n\n    RAG_RELEVANCY_PROMPT_TEMPLATE\n,\n\n\n    OpenAIModel\n,\n\n\n    download_benchmark_dataset\n,\n\n\n    llm_classify\n,\n\n\n)\n\n\n\n\nmodel \n=\n \nOpenAIModel\n(\n\n\n    model_name\n=\n""gpt-4""\n,\n\n\n    temperature\n=\n0.0\n,\n\n\n)\n\n\n\n\n#The rails is used to hold the output to specific values based on the template\n\n\n#It will remove text such as "",,,"" or ""...""\n\n\n#Will ensure the binary value expected from the template is returned\n\n\nrails \n=\n \nlist\n(RAG_RELEVANCY_PROMPT_RAILS_MAP.\nvalues\n())\n\n\nrelevance_classifications \n=\n \nllm_classify\n(\n\n\n    dataframe\n=\ndf,\n\n\n    template\n=\nRAG_RELEVANCY_PROMPT_TEMPLATE,\n\n\n    model\n=\nmodel,\n\n\n    rails\n=\nrails,\n\n\n    provide_explanation\n=\nTrue\n, \n#optional to generate explanations for the value produced by the eval LLM\n\n\n)\nThe above runs the RAG relevancy LLM template against the dataframe df.\nRAG Eval\nGPT-4o\nGPT-4\nGPT-4 Turbo\nGemini Pro\nGPT-3.5\nPalm (Text Bison)\nClaude V2\nPrecision\n0.60\n0.70\n0.68\n0.61\n0.42\n0.53\n0.79\nRecall\n0.77\n0.88\n0.91\n1\n1.0\n1\n0.22\nF1\n0.67\n0.78\n0.78\n0.76\n0.59\n0.69\n0.34\nThroughput\nGPT-4\nGPT-4 Turbo\nGPT-3.5\n100 Samples\n113 Sec\n61 sec\n73 Sec\nPrevious\nQ&A on Retrieved Data\nNext\nSummarization\nLast updated \n2 months ago\n\nsource: https://docs.arize.com/phoenix/evaluation/concepts-evals/evaluation\ntitle: Evaluators\n\nEvaluators\nEvaluation and benchmarking are crucial concepts in LLM development. To improve the performance of an LLM app (RAG, agents), you must have a way to measure it.\nPhoenix offers key modules to measure the quality of generated results as well as modules to measure retrieval quality.\nResponse Evaluation\n: Does the response match the retrieved context? Does it also match the query?\nRetrieval Evaluation\n: Are the retrieved sources relevant to the query?\nResponse Evaluation\nEvaluation of generated results can be challenging. Unlike traditional ML, the predicted results are not numeric or categorical, making it hard to define quantitative metrics for this problem.\nPhoenix offers \nLLM Evaluations\n, a module designed to measure the quality of results. This module uses a ""gold"" LLM (e.g. GPT-4) to decide whether the generated answer is correct in a variety of ways.\n\nNote that many of these evaluation criteria DO NOT require ground-truth labels. Evaluation can be done simply with a combination of the \ninput\n (query), \noutput\n (response), and \ncontext\n.\nLLM Evals supports the following response evaluation criteria:\nQA Correctness\n - Whether a question was correctly answered by the system based on the retrieved data. In contrast to retrieval Evals that are checks on chunks of data returned, this check is a system level check of a correct Q&A.\nHallucinations\n \n-\n Designed to detect LLM hallucinations relative to retrieved context\nToxicity\n - Identify if the AI response is racist, biased, or toxic\nResponse evaluations are a critical first step to figuring out whether your LLM App is running correctly. Response evaluations can pinpoint specific executions (a.k.a. traces) that are performing badly and can be aggregated up so that you can track how your application is running as a whole.\nEvaluations can be aggregated across executions to be used as KPIs\nRetrieval Evaluation\nPhoenix also provides evaluation of retrieval independently.\nThe concept of retrieval evaluation is not new; given a set of relevance scores for a set of retrieved documents, we can evaluate retrievers using retrieval metrics like \nprecision\n, \nNDCG\n, \nhit rate\n and more.\nLLM Evals supports the following retrieval evaluation criteria:\nRelevance\n - Evaluates whether a retrieved document chunk contains an answer to the query.\nRetrieval Evaluations can be run directly on application traces\nRetrieval is possibly the most important step in any LLM application as poor and/or incorrect retrieval can be the cause of bad response generation. If your application uses RAG to power an LLM, retrieval evals can help you identify the cause of hallucinations and incorrect answers.\nEvaluations\nInferences that contain generative records can be fed into evals to produce evaluations for analysis\nWith Phoenix\'s LLM Evals, evaluation results (or just \nEvaluations\n for short) is data consisting of 3 main columns:\nlabel\n: str [optional] - a classification label for the evaluation (e.g. ""hallucinated"" vs ""factual""). Can be used to calculate percentages (e.g. percent hallucinated) and can be used to filter down your data (e.g. \nEvals[""Hallucinations""].label == ""hallucinated""\n)\nscore\n: number [optional] - a numeric score for the evaluation (e.g. 1 for good, 0 for bad). Scores are great way to sort your data to surface poorly performing examples and can be used to filter your data by a threshold.\nexplanation\n: str [optional] - the reasoning for why the evaluation label or score was given. In the case of LLM evals, this is the evaluation model\'s reasoning. While explanations are optional, they can be extremely useful when trying to understand problematic areas of your application.\nLet\'s take a look at an example list of \nQ&A relevance\n evaluations:\nlabel\nexplanation\nscore\ncorrect\nThe reference text explains that YC was not or...\n1\ncorrect\nTo determine if the answer is correct, we need...\n1\nincorrect\nTo determine if the answer is correct, we must...\n0\ncorrect\nTo determine if the answer is correct, we need...\n1\nThese three columns combined can drive any type of evaluation you can imagine. \nlabel\n provides a way to classify responses, \nscore\n provides a way to assign a numeric assessment, and \nexplanation\n gives you a way to get qualitative feedback.\nEvaluating Traces\nAdding evaluations on traces can highlight problematic areas that require further analysis\nWith Phoenix, evaluations can be ""attached"" to the \nspans\n and \ndocuments\n collected. In order to facilitate this, Phoenix supports the following steps.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Is there a way to reduce NOT_PARSABLE responses during relevance evaluation?\nAnswer: '}, {'role': 'assistant', 'content': 'relevant'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6Nzk=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix's tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you'll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they're geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix's search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix's flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix's evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals\ntitle: How to: Evals\n\nHow to: Evals\nPhoenix Evaluators\nHallucinations\nQ&A on Retrieved Data\nRetrieval (RAG) Relevance\nSummarization \nCode Generation\nToxicity\n \nAI vs Human \nReference (Citation) Eval\nBring Your Own Evaluator\nCategorical evaluator\n (llm_classify)\nNumeric evaluator\n (llm_generate)\nOnline Evals\nRun evaluations via a job to visualize in the UI as traces stream in.\nPrevious\nCustom Task Evaluation\nNext\nUse Phoenix Evaluators\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Is there any document for the search query syntax on phoenix UI?\nAnswer: ""}, {'role': 'assistant', 'content': 'Phoenix UI does not have a specific document for search query syntax.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/user-guide\ntitle: User Guide\n\nUser Guide\nLLM observability is complete visibility into every layer of an LLM-based software system: the application, the prompt, and the response.\nPhoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications efficiently.\n🛠️ Develop\nDuring the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.\nTraces for Debugging\nPhoenix's tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.\nQuickstart: Tracing\nExperimentation\nLeverage experiments to measure prompt and model performance. Typically during this early stage, you'll focus on gather a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they're geared towards ensure your application performs correctly.\nRun Experiments\nEvaluation\nEither as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.\nQuickstart: Evals\nPrompt Tracking\nInstrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.\nInstrumenting Prompt Templates and Prompt Variables\nSearch & Retrieval Embeddings Visualizer\nPhoenix's search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.\nQuickstart: Inferences\n🧪 Testing/Staging\nIn the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.\nIterate via Experiments\nWith a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.\nRun Experiments\nEvals Testing\nPhoenix's flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.\nQuickstart: Evals\nCurate Data\nPhoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.\nQuickstart: Datasets\nGuardrails\nAdd guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.\nGuardrails AI\n🚀 Production\nIn production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.\nTraces in Production\nPhoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded. \nEvals for Production\nPhoenix's evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.\nFine-tuning\nPhoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.\nPhoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.\nPrevious\nHosted Phoenix\nNext\nExamples\nLast updated \n15 days ago\n\nsource: https://docs.arize.com/phoenix/evaluation/how-to-evals\ntitle: How to: Evals\n\nHow to: Evals\nPhoenix Evaluators\nHallucinations\nQ&A on Retrieved Data\nRetrieval (RAG) Relevance\nSummarization \nCode Generation\nToxicity\n \nAI vs Human \nReference (Citation) Eval\nBring Your Own Evaluator\nCategorical evaluator\n (llm_classify)\nNumeric evaluator\n (llm_generate)\nOnline Evals\nRun evaluations via a job to visualize in the UI as traces stream in.\nPrevious\nCustom Task Evaluation\nNext\nUse Phoenix Evaluators\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Is there any document for the search query syntax on phoenix UI?\nAnswer: ""}, {'role': 'assistant', 'content': 'Phoenix UI does not have a specific document for search query syntax.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6ODA=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/extract-data-from-spans\ntitle: Querying Spans\n\n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n    score\n=\n""document.score""\n,\n\n\n)\nHow to Apply Filters\nThe \n.where()\n method accepts a string of valid Python boolean expression. The expression can be arbitrarily complex, but restrictions apply, e.g. making function calls are generally disallowed. Below is a conjunction filtering also on whether the input value contains the string \n\'programming\'\n.\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""span_kind == \'RETRIEVER\' and \'programming\' in input.value""\n\n\n)\nFiltering Spans by Evaluation Results\nFiltering spans by evaluation results, e.g. \nscore\n or \nlabel\n, can be done via a special syntax. The name of the evaluation is specified as an indexer on the special keyword \nevals\n. The example below filters for spans with the \nincorrect\n label on their \ncorrectness\n evaluations. (See \nhere\n for how to compute evaluations for traces, and \nhere\n for how to ingest those results back to Phoenix.)\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""evals[\'correctness\'].label == \'incorrect\'""\n\n\n)\nFiltering on Metadata\nmetadata\n is an attribute that is a dictionary and it can be filtered like a dictionary.\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""metadata[""\ntopic\n""] == \'programming\'""\n\n\n)\nFiltering for Substring\nNote that Python strings do not have a \ncontain\n method, and substring search is done with the \nin\n operator.\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""\'programming\' in metadata[""\ntopic\n""]""\n\n\n)\nFiltering for No Evaluations\nGet spans that do not have an evaluation attached yet\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""evals[\'correctness\'].label is None""\n\n\n)\n\n\n# correctness is whatever you named your evaluation metric\nHow to Extract Attributes\nSpan attributes can be selected by simply listing them inside \n.select()\n method.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n\n\n    \n""input.value""\n,\n\n\n    \n""output.value,\n\n\n)\nRenaming Output Columns\nKeyword-argument style can be used to rename the columns in the dataframe. The example below returns two columns named \ninput\n and \noutput\n instead of the original names of the attributes.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n\n\n    input\n=\n""input.value""\n,\n\n\n    output\n=\n""output.value,\n\n\n)\nArbitrary Output Column Names \nIf arbitrary output names are desired, e.g. names with spaces and symbols, we can leverage Python\'s double-asterisk idiom for unpacking a dictionary, as shown below.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n**\n{\n\n\n    \n""Value (Input)""\n: \n""input.value""\n,\n\n\n    \n""Value (Output)""\n: \n""output.value,\n\n\n})\nAdvanced Usage\nConcatenating\nThe document contents can also be concatenated together. The query below concatenates the list of \ndocument.content\n with \n\\n\\n\n (double newlines), which is the default separator. Keyword arguments are necessary to name the output columns, and in this example we name the output column as \nreference\n. (Python\'s double-asterisk unpacking idiom can be used to specify arbitrary output names containing spaces or symbols. See \nhere\n for an example.)\nCopy\nquery \n=\n \nSpanQuery\n().\nconcat\n(\n\n\n    \n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n)\nSpecial Separators\nIf a different separator is desired, say \n\\n************\\n\n, it can be specified as follows.\nCopy\nquery \n=\n \nSpanQuery\n().\nconcat\n(\n\n\n    \n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n).\nwith_concat_separator\n(\n\n\n    separator\n=\n""\\n************\\n""\n,\n\n\n)\nUsing Parent ID as Index\nThis is useful for joining a span to its parent span. To do that we would first index the child span by selecting its parent ID and renaming it as \nspan_id\n. This works because \nspan_id\n is a special column name: whichever column having that name will become the index of the output DataFrame.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n\n\n    span_id\n=\n""parent_id""\n,\n\n\n    output\n=\n""output.value,\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/extract-data-from-spans\ntitle: Querying Spans\n\n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n)\nSpecial Separators\nIf a different separator is desired, say \n\\n************\\n\n, it can be specified as follows.\nCopy\nquery \n=\n \nSpanQuery\n().\nconcat\n(\n\n\n    \n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n).\nwith_concat_separator\n(\n\n\n    separator\n=\n""\\n************\\n""\n,\n\n\n)\nUsing Parent ID as Index\nThis is useful for joining a span to its parent span. To do that we would first index the child span by selecting its parent ID and renaming it as \nspan_id\n. This works because \nspan_id\n is a special column name: whichever column having that name will become the index of the output DataFrame.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n\n\n    span_id\n=\n""parent_id""\n,\n\n\n    output\n=\n""output.value,\n\n\n)\nJoining a Span to Its Parent\nTo do this, we would provide two queries to Phoenix which will return two simultaneous dataframes that can be joined together by pandas. The \nquery_for_child_spans\n uses \nparent_id\n as index as shown in \nUsing Parent ID as Index\n, and \npx.Client().query_spans()\n returns a list of dataframes when multiple queries are given.\nCopy\nimport\n pandas \nas\n pd\n\n\n\n\npd\n.\nconcatenate\n(\n\n\n    px.\nClient\n().\nquery_spans\n(\n\n\n        query_for_parent_spans,\n\n\n        query_for_child_spans,\n\n\n    ),\n\n\n    axis\n=\n1\n,        \n# joining on the row indices\n\n\n    join\n=\n""inner""\n,  \n# inner-join by the indices of the dataframes\n\n\n)\nHow to use Data for Evaluation\nExtract the Input and Output from LLM Spans\nTo learn more about extracting span attributes, see \nExtracting Span Attributes\n.\nCopy\nfrom\n phoenix\n.\ntrace\n.\ndsl \nimport\n SpanQuery\n\n\n\n\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""span_kind == \'LLM\'""\n,\n\n\n).\nselect\n(\n\n\n    input\n=\n""input.value""\n,\n\n\n    output\n=\n""output.value,\n\n\n)\n\n\n\n\n# The Phoenix Client can take this query and return a dataframe.\n\n\npx\n.\nClient\n().\nquery_spans\n(query)\nRetrieval (RAG) Relevance Evaluations\nTo extract the dataframe input for \nRetrieval (RAG) Relevance evaluations\n, we can apply the query described in the \nExample\n, or leverage the \nhelper\n function implementing the same query.\nQ&A on Retrieved Data Evaluations\nTo extract the dataframe input to the \nQ&A on Retrieved Data evaluations\n, we can use a \nhelper\n function or use the following query (which is what\'s inside the helper function). This query applies techniques described in the \nAdvanced Usage\n section.\nCopy\nimport\n pandas \nas\n pd\n\n\nfrom\n phoenix\n.\ntrace\n.\ndsl \nimport\n SpanQuery\n\n\n\n\nquery_for_root_span \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""parent_id is None""\n,   \n# Filter for root spans\n\n\n).\nselect\n(\n\n\n    input\n=\n""input.value""\n,   \n# Input contains the user\'s question\n\n\n    output\n=\n""output.value""\n, \n# Output contains the LLM\'s answer\n\n\n)\n\n\n\n\nquery_for_retrieved_documents \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""span_kind == \'RETRIEVER\'""\n,  \n# Filter for RETRIEVER span\n\n\n).\nselect\n(\n\n\n    \n# Rename parent_id as span_id. This turns the parent_id\n\n\n    \n# values into the index of the output dataframe.\n\n\n    span_id\n=\n""parent_id""\n,\n\n\n).\nconcat\n(\n\n\n    \n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n)\n\n\n\n\n# Perform an inner join on the two sets of spans.\n\n\npd\n.\nconcat\n(\n\n\n    px.\nClient\n().\nquery_spans\n(\n\n\n        query_for_root_span,\n\n\n        query_for_retrieved_documents,\n\n\n    ),\n\n\n    axis\n=\n1\n,\n\n\n    join\n=\n""inner""\n,\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Is it possible to search for values in children spans?\nAnswer: '}, {'role': 'assistant', 'content': 'Yes, it is possible to search for values in children spans by using specific queries and filtering techniques provided in the context information.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/extract-data-from-spans\ntitle: Querying Spans\n\n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n    score\n=\n""document.score""\n,\n\n\n)\nHow to Apply Filters\nThe \n.where()\n method accepts a string of valid Python boolean expression. The expression can be arbitrarily complex, but restrictions apply, e.g. making function calls are generally disallowed. Below is a conjunction filtering also on whether the input value contains the string \n\'programming\'\n.\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""span_kind == \'RETRIEVER\' and \'programming\' in input.value""\n\n\n)\nFiltering Spans by Evaluation Results\nFiltering spans by evaluation results, e.g. \nscore\n or \nlabel\n, can be done via a special syntax. The name of the evaluation is specified as an indexer on the special keyword \nevals\n. The example below filters for spans with the \nincorrect\n label on their \ncorrectness\n evaluations. (See \nhere\n for how to compute evaluations for traces, and \nhere\n for how to ingest those results back to Phoenix.)\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""evals[\'correctness\'].label == \'incorrect\'""\n\n\n)\nFiltering on Metadata\nmetadata\n is an attribute that is a dictionary and it can be filtered like a dictionary.\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""metadata[""\ntopic\n""] == \'programming\'""\n\n\n)\nFiltering for Substring\nNote that Python strings do not have a \ncontain\n method, and substring search is done with the \nin\n operator.\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""\'programming\' in metadata[""\ntopic\n""]""\n\n\n)\nFiltering for No Evaluations\nGet spans that do not have an evaluation attached yet\nCopy\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""evals[\'correctness\'].label is None""\n\n\n)\n\n\n# correctness is whatever you named your evaluation metric\nHow to Extract Attributes\nSpan attributes can be selected by simply listing them inside \n.select()\n method.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n\n\n    \n""input.value""\n,\n\n\n    \n""output.value,\n\n\n)\nRenaming Output Columns\nKeyword-argument style can be used to rename the columns in the dataframe. The example below returns two columns named \ninput\n and \noutput\n instead of the original names of the attributes.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n\n\n    input\n=\n""input.value""\n,\n\n\n    output\n=\n""output.value,\n\n\n)\nArbitrary Output Column Names \nIf arbitrary output names are desired, e.g. names with spaces and symbols, we can leverage Python\'s double-asterisk idiom for unpacking a dictionary, as shown below.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n**\n{\n\n\n    \n""Value (Input)""\n: \n""input.value""\n,\n\n\n    \n""Value (Output)""\n: \n""output.value,\n\n\n})\nAdvanced Usage\nConcatenating\nThe document contents can also be concatenated together. The query below concatenates the list of \ndocument.content\n with \n\\n\\n\n (double newlines), which is the default separator. Keyword arguments are necessary to name the output columns, and in this example we name the output column as \nreference\n. (Python\'s double-asterisk unpacking idiom can be used to specify arbitrary output names containing spaces or symbols. See \nhere\n for an example.)\nCopy\nquery \n=\n \nSpanQuery\n().\nconcat\n(\n\n\n    \n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n)\nSpecial Separators\nIf a different separator is desired, say \n\\n************\\n\n, it can be specified as follows.\nCopy\nquery \n=\n \nSpanQuery\n().\nconcat\n(\n\n\n    \n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n).\nwith_concat_separator\n(\n\n\n    separator\n=\n""\\n************\\n""\n,\n\n\n)\nUsing Parent ID as Index\nThis is useful for joining a span to its parent span. To do that we would first index the child span by selecting its parent ID and renaming it as \nspan_id\n. This works because \nspan_id\n is a special column name: whichever column having that name will become the index of the output DataFrame.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n\n\n    span_id\n=\n""parent_id""\n,\n\n\n    output\n=\n""output.value,\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/extract-data-from-spans\ntitle: Querying Spans\n\n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n)\nSpecial Separators\nIf a different separator is desired, say \n\\n************\\n\n, it can be specified as follows.\nCopy\nquery \n=\n \nSpanQuery\n().\nconcat\n(\n\n\n    \n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n).\nwith_concat_separator\n(\n\n\n    separator\n=\n""\\n************\\n""\n,\n\n\n)\nUsing Parent ID as Index\nThis is useful for joining a span to its parent span. To do that we would first index the child span by selecting its parent ID and renaming it as \nspan_id\n. This works because \nspan_id\n is a special column name: whichever column having that name will become the index of the output DataFrame.\nCopy\nquery \n=\n \nSpanQuery\n().\nselect\n(\n\n\n    span_id\n=\n""parent_id""\n,\n\n\n    output\n=\n""output.value,\n\n\n)\nJoining a Span to Its Parent\nTo do this, we would provide two queries to Phoenix which will return two simultaneous dataframes that can be joined together by pandas. The \nquery_for_child_spans\n uses \nparent_id\n as index as shown in \nUsing Parent ID as Index\n, and \npx.Client().query_spans()\n returns a list of dataframes when multiple queries are given.\nCopy\nimport\n pandas \nas\n pd\n\n\n\n\npd\n.\nconcatenate\n(\n\n\n    px.\nClient\n().\nquery_spans\n(\n\n\n        query_for_parent_spans,\n\n\n        query_for_child_spans,\n\n\n    ),\n\n\n    axis\n=\n1\n,        \n# joining on the row indices\n\n\n    join\n=\n""inner""\n,  \n# inner-join by the indices of the dataframes\n\n\n)\nHow to use Data for Evaluation\nExtract the Input and Output from LLM Spans\nTo learn more about extracting span attributes, see \nExtracting Span Attributes\n.\nCopy\nfrom\n phoenix\n.\ntrace\n.\ndsl \nimport\n SpanQuery\n\n\n\n\nquery \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""span_kind == \'LLM\'""\n,\n\n\n).\nselect\n(\n\n\n    input\n=\n""input.value""\n,\n\n\n    output\n=\n""output.value,\n\n\n)\n\n\n\n\n# The Phoenix Client can take this query and return a dataframe.\n\n\npx\n.\nClient\n().\nquery_spans\n(query)\nRetrieval (RAG) Relevance Evaluations\nTo extract the dataframe input for \nRetrieval (RAG) Relevance evaluations\n, we can apply the query described in the \nExample\n, or leverage the \nhelper\n function implementing the same query.\nQ&A on Retrieved Data Evaluations\nTo extract the dataframe input to the \nQ&A on Retrieved Data evaluations\n, we can use a \nhelper\n function or use the following query (which is what\'s inside the helper function). This query applies techniques described in the \nAdvanced Usage\n section.\nCopy\nimport\n pandas \nas\n pd\n\n\nfrom\n phoenix\n.\ntrace\n.\ndsl \nimport\n SpanQuery\n\n\n\n\nquery_for_root_span \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""parent_id is None""\n,   \n# Filter for root spans\n\n\n).\nselect\n(\n\n\n    input\n=\n""input.value""\n,   \n# Input contains the user\'s question\n\n\n    output\n=\n""output.value""\n, \n# Output contains the LLM\'s answer\n\n\n)\n\n\n\n\nquery_for_retrieved_documents \n=\n \nSpanQuery\n().\nwhere\n(\n\n\n    \n""span_kind == \'RETRIEVER\'""\n,  \n# Filter for RETRIEVER span\n\n\n).\nselect\n(\n\n\n    \n# Rename parent_id as span_id. This turns the parent_id\n\n\n    \n# values into the index of the output dataframe.\n\n\n    span_id\n=\n""parent_id""\n,\n\n\n).\nconcat\n(\n\n\n    \n""retrieval.documents""\n,\n\n\n    reference\n=\n""document.content""\n,\n\n\n)\n\n\n\n\n# Perform an inner join on the two sets of spans.\n\n\npd\n.\nconcat\n(\n\n\n    px.\nClient\n().\nquery_spans\n(\n\n\n        query_for_root_span,\n\n\n        query_for_retrieved_documents,\n\n\n    ),\n\n\n    axis\n=\n1\n,\n\n\n    join\n=\n""inner""\n,\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Is it possible to search for values in children spans?\nAnswer: '}, {'role': 'assistant', 'content': 'Yes, it is possible to search for values in children spans by using specific queries and filtering techniques provided in the context information.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6ODE=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/integrations/arize/bring-production-data-to-notebook-for-eda-or-retraining\ntitle: Export Data from Arize to Phoenix\n\nExport Data from Arize to Phoenix\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nOftentimes, the team that notices an issue in their model, for example a prompt/response LLM model, may not be the same team that continues the investigations or kicks off retraining workflows.\nTo help connect teams and workflows, Phoenix enables continued analysis of production data from \nArize\n in a notebook environment for fine tuning workflows.\nFor example, a user may have noticed in \nArize\n that this prompt template is not performing well.\nWith a few lines of Python code, users can export this data into Phoenix for further analysis. This allows team members, such as data scientists, who may not have access to production data today, an easy way to access relevant product data for further analysis in an environment they are familiar with.\nThey can then easily augment and fine tune the data and verify improved performance, before deploying back to production.\nThere are two ways export data out of \nArize\n for further investigation:\nThe easiest way is to click the export button on the Embeddings and Inferences pages. This will produce a code snippet that you can copy into a Python environment and install Phoenix. This code snippet will include the date range you have selected in the \nArize\n platform, in addition to the inferences you have selected.\nUsers can also query \nArize\n for data directly using the Arize Python export client. We recommend doing this once you're more comfortable with the in-platform export functionality, as you will need to manually enter in the data ranges and data you want to export.\nCopy\nos\n.\nenviron\n[\n'ARIZE_API_KEY'\n]\n \n=\n ARIZE_API_KEY\n\n\n\n\nfrom\n datetime \nimport\n datetime\n\n\n\n\nfrom\n arize\n.\nexporter \nimport\n ArizeExportClient\n\n\nfrom\n arize\n.\nutils\n.\ntypes \nimport\n Environments\n\n\n\n\nclient \n=\n \nArizeExportClient\n()\n\n\n\n\nprimary_df \n=\n client\n.\nexport_model_to_df\n(\n\n\n    space_id\n=\n'U3BhY2U6NzU0'\n,\n\n\n    model_name\n=\n'test_home_prices_LLM'\n,\n\n\n    environment\n=\nEnvironments.PRODUCTION,\n\n\n    start_time\n=\ndatetime.\nfromisoformat\n(\n'2023-02-11T07:00:00.000+00:00'\n),\n\n\n    end_time\n=\ndatetime.\nfromisoformat\n(\n'2023-03-14T00:59:59.999+00:00'\n),\n\n\n)\nTest out this workflow by signing up for a \nfree Arize account\n.\nPrevious\nArize\nNext\nRagas\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/integrations/arize\ntitle: Arize\n\nArize\nPhoenix works hand-in-hand with Arize, it's enterprise counterpart\nShare Embeddings Data with Arize\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nPrevious\nModels\nNext\nExport Data from Arize to Phoenix\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can I export data from Arize to Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'You can export data from Arize to Phoenix by clicking the export button on the Embeddings and Inferences pages in Arize. This action will generate a code snippet that you can copy into a Python environment after installing Phoenix. Alternatively, you can also query Arize for data directly using the Arize Python export client, which involves manually entering the data ranges and specific data you wish to export.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/integrations/arize/bring-production-data-to-notebook-for-eda-or-retraining\ntitle: Export Data from Arize to Phoenix\n\nExport Data from Arize to Phoenix\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nOftentimes, the team that notices an issue in their model, for example a prompt/response LLM model, may not be the same team that continues the investigations or kicks off retraining workflows.\nTo help connect teams and workflows, Phoenix enables continued analysis of production data from \nArize\n in a notebook environment for fine tuning workflows.\nFor example, a user may have noticed in \nArize\n that this prompt template is not performing well.\nWith a few lines of Python code, users can export this data into Phoenix for further analysis. This allows team members, such as data scientists, who may not have access to production data today, an easy way to access relevant product data for further analysis in an environment they are familiar with.\nThey can then easily augment and fine tune the data and verify improved performance, before deploying back to production.\nThere are two ways export data out of \nArize\n for further investigation:\nThe easiest way is to click the export button on the Embeddings and Inferences pages. This will produce a code snippet that you can copy into a Python environment and install Phoenix. This code snippet will include the date range you have selected in the \nArize\n platform, in addition to the inferences you have selected.\nUsers can also query \nArize\n for data directly using the Arize Python export client. We recommend doing this once you're more comfortable with the in-platform export functionality, as you will need to manually enter in the data ranges and data you want to export.\nCopy\nos\n.\nenviron\n[\n'ARIZE_API_KEY'\n]\n \n=\n ARIZE_API_KEY\n\n\n\n\nfrom\n datetime \nimport\n datetime\n\n\n\n\nfrom\n arize\n.\nexporter \nimport\n ArizeExportClient\n\n\nfrom\n arize\n.\nutils\n.\ntypes \nimport\n Environments\n\n\n\n\nclient \n=\n \nArizeExportClient\n()\n\n\n\n\nprimary_df \n=\n client\n.\nexport_model_to_df\n(\n\n\n    space_id\n=\n'U3BhY2U6NzU0'\n,\n\n\n    model_name\n=\n'test_home_prices_LLM'\n,\n\n\n    environment\n=\nEnvironments.PRODUCTION,\n\n\n    start_time\n=\ndatetime.\nfromisoformat\n(\n'2023-02-11T07:00:00.000+00:00'\n),\n\n\n    end_time\n=\ndatetime.\nfromisoformat\n(\n'2023-03-14T00:59:59.999+00:00'\n),\n\n\n)\nTest out this workflow by signing up for a \nfree Arize account\n.\nPrevious\nArize\nNext\nRagas\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/integrations/arize\ntitle: Arize\n\nArize\nPhoenix works hand-in-hand with Arize, it's enterprise counterpart\nShare Embeddings Data with Arize\nEasily share data when you discover interesting insights so your data science team can perform further investigation or kickoff retraining workflows.\nPrevious\nModels\nNext\nExport Data from Arize to Phoenix\nLast updated \n5 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can I export data from Arize to Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'You can export data from Arize to Phoenix by clicking the export button on the Embeddings and Inferences pages in Arize. This action will generate a code snippet that you can copy into a Python environment after installing Phoenix. Alternatively, you can also query Arize for data directly using the Arize Python export client, which involves manually entering the data ranges and specific data you wish to export.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6ODI=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/llm-traces\ntitle: Overview: Tracing\n\nOverview: Tracing\nTracing the execution of LLM applications using Telemetry\nLLM tracing records the paths taken by requests as they propagate through multiple steps or components of an LLM application. For example, when a user interacts with an LLM application, tracing can capture the sequence of operations, such as document retrieval, embedding generation, language model invocation, and response generation to provide a detailed timeline of the request's execution.\nTracing is a helpful tool for understanding how your LLM application works. Phoenix offers comprehensive tracing capabilities that are not tied to any specific LLM vendor or framework. Phoenix accepts traces over the OpenTelemetry protocol (OTLP) and supports first-class instrumentation for a variety of frameworks ( \nLlamaIndex\n, \nLangChain\n,\n DSPy\n),  SDKs (\nOpenAI\n, \nBedrock\n, \nMistral\n, \nVertex\n), and Languages. (Python, Javascript, etc.)\nUsing Phoenix's tracing capabilities can provide important insights into the inner workings of your LLM application. By analyzing the collected trace data, you can identify and address various performance and operational issues and improve the overall reliability and efficiency of your system.\nApplication Latency\n: Identify and address slow invocations of LLMs, Retrievers, and other components within your application, enabling you to optimize performance and responsiveness.\nToken Usage\n: Gain a detailed breakdown of token usage for your LLM calls, allowing you to identify and optimize the most expensive LLM invocations.\nRuntime Exceptions\n: Capture and inspect critical runtime exceptions, such as rate-limiting events, that can help you proactively address and mitigate potential issues.\nRetrieved Documents\n: Inspect the documents retrieved during a Retriever call, including the score and order in which they were returned to provide insight into the retrieval process.\nEmbeddings\n: Examine the embedding text used for retrieval and the underlying embedding model to allow you to validate and refine your embedding strategies.\nLLM Parameters\n: Inspect the parameters used when calling an LLM, such as temperature and system prompts, to ensure optimal configuration and debugging.\nPrompt Templates\n: Understand the prompt templates used during the prompting step and the variables that were applied, allowing you to fine-tune and improve your prompting strategies.\nTool Descriptions\n: View the descriptions and function signatures of the tools your LLM has been given access to in order to better understand and control your LLM’s capabilities.\nLLM Function Calls\n: For LLMs with function call capabilities (e.g., OpenAI), you can inspect the function selection and function messages in the input to the LLM, further improving your ability to debug and optimize your application.\nBy using tracing in Phoenix, you can gain increased visibility into your LLM application, empowering you to identify and address performance bottlenecks, optimize resource utilization, and ensure the overall reliability and effectiveness of your system.\nView the inner workings for your LLM Application\nTo get started, check out the \nQuickstart guide\nAfter that, read through the \nConcepts Section\n to get and understanding of the different components.\nIf you want to learn how to accomplish a particular task, check out the \nHow-To Guides.\n\n\nPrevious\nFAQs: Deployment\nNext\nQuickstart: Tracing\nLast updated \n6 hours ago\n\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/how-to-annotate-traces\ntitle: Annotating Traces\n\nAnnotating Traces\nIn order to improve your LLM application iteratively, it's vital to collect feedback as well as to establish an evaluation pipeline so that you can monitor your application. In Phoenix we capture this type of \nfeedback\n in the form of \nannotations\n.\nPhoenix gives you the ability to annotate traces with feedback from two sources: \nLLM\n in the form of \nevaluations\n and \nHUMAN\n in the form of human \nannotations\n. Phoenix's annotation model is simple yet powerful - given an entity such as a span that is collected, you can assign a \nlabel\n and/or a \nscore\n to that entity. Let's see a few examples of the types of feedback you might want to collect:\nTypes of Feedback\nFeedback from End-users\nHuman feedback allows you to understand how your users are experiencing your application and helps draw attention to problematic traces. Phoenix makes it easy to collect feedback for traces and view it in the context of the trace, as well as filter all your traces based on the feedback annotations you send. Before anything else, you want to know if your users or customers are happy with your product. This can be as straightforward as adding \n👍\n \n👎\n buttons to your application, and logging the result as annotations.\nFor more information on how to wire up your application to collect feedback from your users, see \nCapture Feedback\nEvaluations from LLMs\nWhen you have large amounts of data it can be immensely efficient and valuable to leverage LLM judges via \nevals\n to produce labels and scores to annotate your traces with. Phoenix's \nevals library\n as well as other third-party eval libraries can be leveraged to annotate your spans with evaluations. For details see:\nQuickstart: Evals\nto generate evaluation results\nLog Evaluation Results\n to add evaluation results to spans\nHuman Annotations\nSometimes you need to rely on human annotators to attach feedback to specific traces of your application. Human annotations through the UI can be thought of as manual quality assurance. While it can be a bit more labor intensive, it can help in sharing insights within a team, curating datasets of good/bad examples, and even in training an LLM judge.\nHow to Use Feedback\nFeedback / Annotations can help you share valuble insight about how your application is performing. However making these insights actionable can be difficult. With Phoenix, the annotations you add to your trace data is propagated to datasets so that you can use the annotations during experimentation.\n\nTrack Improvements during Experimentation\nSince Phoenix datasets preserve the annotations, you can track whether or not changes to your application (e.g. \nexperimentation\n) produce better results (e.g. better scores / labels). Phoenix \nevaluators\n have access to the example metadata at evaluation time, making it possible to track improvements / regressions over previous generations (e.g. the previous annotations).\nTrain an LLM Judge\nAI development currently faces challenges when evaluating LLM application outputs at scale:\nHuman annotation is precise but time-consuming and impossible to scale efficiently.\nExisting automated methods using LLM judges require careful prompt engineering and often fall short of capturing human evaluation nuances.\nSolutions requiring extensive human resources are difficult to scale and manage.\nThese challenges create a bottleneck in the rapid development and improvement of high-quality LLM applications.\nSince Phoenix datasets preserve the annotations in the example metadata, you can use datasets to build human-preference calibrated judges using libraries and tools such as DSPy and Zenbase.\nTraining human-aligned LLM judges is still under active development. If this is an area you are interested in, please reach out!\nPrevious\nHow Tracing Works\nNext\nHow-to: Tracing\nLast updated \n10 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How does tracing help in monitoring LLM applications?\nAnswer: ""}, {'role': 'assistant', 'content': ""Tracing helps in monitoring LLM applications by providing a detailed timeline of the request's execution, capturing the sequence of operations as requests propagate through different components. It allows for the identification and addressing of slow invocations, token usage breakdown, runtime exceptions, retrieved documents, embeddings, LLM parameters, prompt templates, tool descriptions, and LLM function calls. By analyzing the collected trace data, one can optimize performance, address operational issues, and improve the overall reliability and efficiency of the system.""}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/llm-traces\ntitle: Overview: Tracing\n\nOverview: Tracing\nTracing the execution of LLM applications using Telemetry\nLLM tracing records the paths taken by requests as they propagate through multiple steps or components of an LLM application. For example, when a user interacts with an LLM application, tracing can capture the sequence of operations, such as document retrieval, embedding generation, language model invocation, and response generation to provide a detailed timeline of the request's execution.\nTracing is a helpful tool for understanding how your LLM application works. Phoenix offers comprehensive tracing capabilities that are not tied to any specific LLM vendor or framework. Phoenix accepts traces over the OpenTelemetry protocol (OTLP) and supports first-class instrumentation for a variety of frameworks ( \nLlamaIndex\n, \nLangChain\n,\n DSPy\n),  SDKs (\nOpenAI\n, \nBedrock\n, \nMistral\n, \nVertex\n), and Languages. (Python, Javascript, etc.)\nUsing Phoenix's tracing capabilities can provide important insights into the inner workings of your LLM application. By analyzing the collected trace data, you can identify and address various performance and operational issues and improve the overall reliability and efficiency of your system.\nApplication Latency\n: Identify and address slow invocations of LLMs, Retrievers, and other components within your application, enabling you to optimize performance and responsiveness.\nToken Usage\n: Gain a detailed breakdown of token usage for your LLM calls, allowing you to identify and optimize the most expensive LLM invocations.\nRuntime Exceptions\n: Capture and inspect critical runtime exceptions, such as rate-limiting events, that can help you proactively address and mitigate potential issues.\nRetrieved Documents\n: Inspect the documents retrieved during a Retriever call, including the score and order in which they were returned to provide insight into the retrieval process.\nEmbeddings\n: Examine the embedding text used for retrieval and the underlying embedding model to allow you to validate and refine your embedding strategies.\nLLM Parameters\n: Inspect the parameters used when calling an LLM, such as temperature and system prompts, to ensure optimal configuration and debugging.\nPrompt Templates\n: Understand the prompt templates used during the prompting step and the variables that were applied, allowing you to fine-tune and improve your prompting strategies.\nTool Descriptions\n: View the descriptions and function signatures of the tools your LLM has been given access to in order to better understand and control your LLM’s capabilities.\nLLM Function Calls\n: For LLMs with function call capabilities (e.g., OpenAI), you can inspect the function selection and function messages in the input to the LLM, further improving your ability to debug and optimize your application.\nBy using tracing in Phoenix, you can gain increased visibility into your LLM application, empowering you to identify and address performance bottlenecks, optimize resource utilization, and ensure the overall reliability and effectiveness of your system.\nView the inner workings for your LLM Application\nTo get started, check out the \nQuickstart guide\nAfter that, read through the \nConcepts Section\n to get and understanding of the different components.\nIf you want to learn how to accomplish a particular task, check out the \nHow-To Guides.\n\n\nPrevious\nFAQs: Deployment\nNext\nQuickstart: Tracing\nLast updated \n6 hours ago\n\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/how-to-annotate-traces\ntitle: Annotating Traces\n\nAnnotating Traces\nIn order to improve your LLM application iteratively, it's vital to collect feedback as well as to establish an evaluation pipeline so that you can monitor your application. In Phoenix we capture this type of \nfeedback\n in the form of \nannotations\n.\nPhoenix gives you the ability to annotate traces with feedback from two sources: \nLLM\n in the form of \nevaluations\n and \nHUMAN\n in the form of human \nannotations\n. Phoenix's annotation model is simple yet powerful - given an entity such as a span that is collected, you can assign a \nlabel\n and/or a \nscore\n to that entity. Let's see a few examples of the types of feedback you might want to collect:\nTypes of Feedback\nFeedback from End-users\nHuman feedback allows you to understand how your users are experiencing your application and helps draw attention to problematic traces. Phoenix makes it easy to collect feedback for traces and view it in the context of the trace, as well as filter all your traces based on the feedback annotations you send. Before anything else, you want to know if your users or customers are happy with your product. This can be as straightforward as adding \n👍\n \n👎\n buttons to your application, and logging the result as annotations.\nFor more information on how to wire up your application to collect feedback from your users, see \nCapture Feedback\nEvaluations from LLMs\nWhen you have large amounts of data it can be immensely efficient and valuable to leverage LLM judges via \nevals\n to produce labels and scores to annotate your traces with. Phoenix's \nevals library\n as well as other third-party eval libraries can be leveraged to annotate your spans with evaluations. For details see:\nQuickstart: Evals\nto generate evaluation results\nLog Evaluation Results\n to add evaluation results to spans\nHuman Annotations\nSometimes you need to rely on human annotators to attach feedback to specific traces of your application. Human annotations through the UI can be thought of as manual quality assurance. While it can be a bit more labor intensive, it can help in sharing insights within a team, curating datasets of good/bad examples, and even in training an LLM judge.\nHow to Use Feedback\nFeedback / Annotations can help you share valuble insight about how your application is performing. However making these insights actionable can be difficult. With Phoenix, the annotations you add to your trace data is propagated to datasets so that you can use the annotations during experimentation.\n\nTrack Improvements during Experimentation\nSince Phoenix datasets preserve the annotations, you can track whether or not changes to your application (e.g. \nexperimentation\n) produce better results (e.g. better scores / labels). Phoenix \nevaluators\n have access to the example metadata at evaluation time, making it possible to track improvements / regressions over previous generations (e.g. the previous annotations).\nTrain an LLM Judge\nAI development currently faces challenges when evaluating LLM application outputs at scale:\nHuman annotation is precise but time-consuming and impossible to scale efficiently.\nExisting automated methods using LLM judges require careful prompt engineering and often fall short of capturing human evaluation nuances.\nSolutions requiring extensive human resources are difficult to scale and manage.\nThese challenges create a bottleneck in the rapid development and improvement of high-quality LLM applications.\nSince Phoenix datasets preserve the annotations in the example metadata, you can use datasets to build human-preference calibrated judges using libraries and tools such as DSPy and Zenbase.\nTraining human-aligned LLM judges is still under active development. If this is an area you are interested in, please reach out!\nPrevious\nHow Tracing Works\nNext\nHow-to: Tracing\nLast updated \n10 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How does tracing help in monitoring LLM applications?\nAnswer: ""}, {'role': 'assistant', 'content': ""Tracing helps in monitoring LLM applications by providing a detailed timeline of the request's execution, capturing the sequence of operations as requests propagate through different components. It allows for the identification and addressing of slow invocations, token usage breakdown, runtime exceptions, retrieved documents, embeddings, LLM parameters, prompt templates, tool descriptions, and LLM function calls. By analyzing the collected trace data, one can optimize performance, address operational issues, and improve the overall reliability and efficiency of the system.""}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6ODM=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/configure-tracing\ntitle: Configure Tracing\n\nConfigure Tracing\nHow to customize OpenTelemetry and OpenInference for your setup\nSpan Processing\nThe tutorials and code snippets in these docs default  to the \nSimpleSpanProcessor.\n A \nSimpleSpanProcessor\n processes and exports spans as they are created. This means that if you create 5 spans, each will be processed and exported before the next span is created in code. This can be helpful in scenarios where you do not want to risk losing a batch, or if you’re experimenting with OpenTelemetry in development. However, it also comes with potentially significant overhead, especially if spans are being exported over a network - each time a call to create a span is made, it would be processed and sent over a network before your app’s execution could continue.\n The \nBatchSpanProcessor\n processes spans in batches before they are exported. This is usually the right processor to use for an application in production but it does mean spans may take some time to show up in Phoenix.\nIn production we recommend the \nBatchSpanProcessor\n over \nSimpleSpanProcessor\n\nwhen deployed and the \nSimpleSpanProcessor\n when developing.\nPrevious\nTrace a Deployed App\nNext\nUse Cases: Tracing\nLast updated \n9 hours ago\n\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nTraces\nA deep dive into the details of a trace\nSpans\nA span represents a unit of work or operation (think a \nspan\n of time). It tracks specific operations that a request makes, painting a picture of what happened during the time in which that operation was executed.\nA span contains name, time-related data, structured log messages, and other metadata (that is, Attributes) to provide information about the operation it tracks. A span for an LLM execution in JSON format is displayed below\nCopy\n{\n\n\n   \n""name""\n:\n \n""llm""\n,\n\n\n   \n""context""\n:\n {\n\n\n       \n""trace_id""\n:\n \n""0x6c80880dbeb609e2ed41e06a6397a0dd""\n,\n\n\n       \n""span_id""\n:\n \n""0xd9bdedf0df0b7208""\n,\n\n\n       \n""trace_state""\n:\n \n""[]""\n\n\n   }\n,\n\n\n   \n""kind""\n:\n \n""SpanKind.INTERNAL""\n,\n\n\n   \n""parent_id""\n:\n \n""0x7eb5df0046c77cd2""\n,\n\n\n   \n""start_time""\n:\n \n""2024-05-08T21:46:11.480777Z""\n,\n\n\n   \n""end_time""\n:\n \n""2024-05-08T21:46:35.368042Z""\n,\n\n\n   \n""status""\n:\n {\n\n\n       \n""status_code""\n:\n \n""OK""\n\n\n   }\n,\n\n\n   \n""attributes""\n:\n {\n\n\n       \n""openinference.span.kind""\n:\n \n""LLM""\n,\n\n\n       \n""llm.input_messages.0.message.role""\n:\n \n""system""\n,\n\n\n       ""llm.input_messages.0.message.content"": ""\\n  The following is a friendly conversation between a user and an AI assistant.\\n  The assistant is talkative and provides lots of specific details from its context.\\n  If the assistant does not know the answer to a question, it truthfully says it\\n  does not know.\\n\\n  Here are the relevant documents for the context:\\n\\n  page_label: 7\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.4Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\na. No piece may weigh more than 70 pounds.\\nb. The combined length and girth of a piece (the length of its longest side plus \\nthe distance around its thickest part) may not exceed 108 inches.\\nc. Lower size or weight standards apply to mail addressed to certain APOs and \\nFPOs, subject to 703.2.0  and 703.4.0  and for Department of State mail, \\nsubject to 703.3.0 .\\n\\npage_label: 6\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.2.10Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\na. The reply half of a double card must be used for reply only and may not be \\nused to convey a message to the original addressee or to send statements \\nof account. The reply half may be formatted for response purposes (e.g., contain blocks for completion by the addressee).\\nb. A double card must be folded before mailing and prepared so that the \\naddress on the reply half is on the inside when the double card is originally \\nmailed. The address side of the reply half may be prepared as Business \\nReply Mail, Courtesy Reply Mail, meter reply mail, or as a USPS Returns service label.\\nc. Plain stickers, seals, or a single wire stitch (staple) may be used to fasten the \\nopen edge at the top or bottom once the card is folded if affixed so that the \\ninner surfaces of the cards can be readily examined. Fasteners must be \\naffixed according to the applicable preparation requirements for the price claimed. Any sealing on the left and right sides of the cards, no matter the \\nsealing process used, is not permitted.\\nd. The first half of a double card must be detached when the reply half is \\nmailed for return. \\n6.2.10   Enclosures\\nEnclosures in double postcards are prohibited at card prices.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do spans and traces differ in Arize Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Spans represent specific operations within a request, tracking what happened during the execution of that operation. They contain time-related data, log messages, and metadata to provide information about the operation. On the other hand, traces in Arize Phoenix provide a detailed view of the journey of a request as it moves through various spans, capturing the sequence of operations and their relationships. Traces help visualize the flow of requests and understand the performance and behavior of the system as a whole.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/configure-tracing\ntitle: Configure Tracing\n\nConfigure Tracing\nHow to customize OpenTelemetry and OpenInference for your setup\nSpan Processing\nThe tutorials and code snippets in these docs default  to the \nSimpleSpanProcessor.\n A \nSimpleSpanProcessor\n processes and exports spans as they are created. This means that if you create 5 spans, each will be processed and exported before the next span is created in code. This can be helpful in scenarios where you do not want to risk losing a batch, or if you’re experimenting with OpenTelemetry in development. However, it also comes with potentially significant overhead, especially if spans are being exported over a network - each time a call to create a span is made, it would be processed and sent over a network before your app’s execution could continue.\n The \nBatchSpanProcessor\n processes spans in batches before they are exported. This is usually the right processor to use for an application in production but it does mean spans may take some time to show up in Phoenix.\nIn production we recommend the \nBatchSpanProcessor\n over \nSimpleSpanProcessor\n\nwhen deployed and the \nSimpleSpanProcessor\n when developing.\nPrevious\nTrace a Deployed App\nNext\nUse Cases: Tracing\nLast updated \n9 hours ago\n\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nTraces\nA deep dive into the details of a trace\nSpans\nA span represents a unit of work or operation (think a \nspan\n of time). It tracks specific operations that a request makes, painting a picture of what happened during the time in which that operation was executed.\nA span contains name, time-related data, structured log messages, and other metadata (that is, Attributes) to provide information about the operation it tracks. A span for an LLM execution in JSON format is displayed below\nCopy\n{\n\n\n   \n""name""\n:\n \n""llm""\n,\n\n\n   \n""context""\n:\n {\n\n\n       \n""trace_id""\n:\n \n""0x6c80880dbeb609e2ed41e06a6397a0dd""\n,\n\n\n       \n""span_id""\n:\n \n""0xd9bdedf0df0b7208""\n,\n\n\n       \n""trace_state""\n:\n \n""[]""\n\n\n   }\n,\n\n\n   \n""kind""\n:\n \n""SpanKind.INTERNAL""\n,\n\n\n   \n""parent_id""\n:\n \n""0x7eb5df0046c77cd2""\n,\n\n\n   \n""start_time""\n:\n \n""2024-05-08T21:46:11.480777Z""\n,\n\n\n   \n""end_time""\n:\n \n""2024-05-08T21:46:35.368042Z""\n,\n\n\n   \n""status""\n:\n {\n\n\n       \n""status_code""\n:\n \n""OK""\n\n\n   }\n,\n\n\n   \n""attributes""\n:\n {\n\n\n       \n""openinference.span.kind""\n:\n \n""LLM""\n,\n\n\n       \n""llm.input_messages.0.message.role""\n:\n \n""system""\n,\n\n\n       ""llm.input_messages.0.message.content"": ""\\n  The following is a friendly conversation between a user and an AI assistant.\\n  The assistant is talkative and provides lots of specific details from its context.\\n  If the assistant does not know the answer to a question, it truthfully says it\\n  does not know.\\n\\n  Here are the relevant documents for the context:\\n\\n  page_label: 7\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.4Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\na. No piece may weigh more than 70 pounds.\\nb. The combined length and girth of a piece (the length of its longest side plus \\nthe distance around its thickest part) may not exceed 108 inches.\\nc. Lower size or weight standards apply to mail addressed to certain APOs and \\nFPOs, subject to 703.2.0  and 703.4.0  and for Department of State mail, \\nsubject to 703.3.0 .\\n\\npage_label: 6\\nfile_path: /Users/mikeldking/work/openinference/python/examples/llama-index-new/backend/data/101.pdf\\n\\nDomestic Mail Manual \\u2022 Updated 7-9-23101\\n101.6.2.10Retail Mail: Physical Standards for Letters, Cards, Flats, and Parcels\\na. The reply half of a double card must be used for reply only and may not be \\nused to convey a message to the original addressee or to send statements \\nof account. The reply half may be formatted for response purposes (e.g., contain blocks for completion by the addressee).\\nb. A double card must be folded before mailing and prepared so that the \\naddress on the reply half is on the inside when the double card is originally \\nmailed. The address side of the reply half may be prepared as Business \\nReply Mail, Courtesy Reply Mail, meter reply mail, or as a USPS Returns service label.\\nc. Plain stickers, seals, or a single wire stitch (staple) may be used to fasten the \\nopen edge at the top or bottom once the card is folded if affixed so that the \\ninner surfaces of the cards can be readily examined. Fasteners must be \\naffixed according to the applicable preparation requirements for the price claimed. Any sealing on the left and right sides of the cards, no matter the \\nsealing process used, is not permitted.\\nd. The first half of a double card must be detached when the reply half is \\nmailed for return. \\n6.2.10   Enclosures\\nEnclosures in double postcards are prohibited at card prices.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do spans and traces differ in Arize Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Spans represent specific operations within a request, tracking what happened during the execution of that operation. They contain time-related data, log messages, and metadata to provide information about the operation. On the other hand, traces in Arize Phoenix provide a detailed view of the journey of a request as it moves through various spans, capturing the sequence of operations and their relationships. Traces help visualize the flow of requests and understand the performance and behavior of the system as a whole.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6ODQ=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing\ntitle: Integrations: Tracing\n\nIntegrations: Tracing\nPhoenix natively works with a variety of frameworks and SDKs across Python and JavaScript via OpenTelemetry\n auto-instrumentation\n.\nPython\nLibrary\nInstrumentation Package\nopeninference-instrumentation-llama-index\nopeninference-instrumentation-langchain\nopeninference-instrumentation-openai\nMistralAI\n\n\nopeninference-instrumentation-mistralai\nVertexAI\n\n\nopeninference-instrumentation-vertexai\nopeninference-instrumentation-dspy\nopeninference-instrumentation-bedrock\nopeninference-instrumentation-guardrails\nopeninference-instrumentation-haystack\nopeninference-instrumentation-crewai\nJavascript\nLibrary\nInstrumentation\n@arizeai/openinference-instrumentation-openai\n@arizeai/openinference-instrumentation-langchain\nPrevious\nQuickstart: Tracing\nNext\nOpenAI\nLast updated \n16 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the benefits of using OpenTelemetry with Arize Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Using OpenTelemetry with Arize Phoenix provides the benefit of enabling seamless integration with a variety of frameworks and SDKs across Python and JavaScript. This integration allows for auto-instrumentation, making it easier for AI Engineers and Data Scientists to monitor, evaluate, and troubleshoot their AI models effectively.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing\ntitle: Integrations: Tracing\n\nIntegrations: Tracing\nPhoenix natively works with a variety of frameworks and SDKs across Python and JavaScript via OpenTelemetry\n auto-instrumentation\n.\nPython\nLibrary\nInstrumentation Package\nopeninference-instrumentation-llama-index\nopeninference-instrumentation-langchain\nopeninference-instrumentation-openai\nMistralAI\n\n\nopeninference-instrumentation-mistralai\nVertexAI\n\n\nopeninference-instrumentation-vertexai\nopeninference-instrumentation-dspy\nopeninference-instrumentation-bedrock\nopeninference-instrumentation-guardrails\nopeninference-instrumentation-haystack\nopeninference-instrumentation-crewai\nJavascript\nLibrary\nInstrumentation\n@arizeai/openinference-instrumentation-openai\n@arizeai/openinference-instrumentation-langchain\nPrevious\nQuickstart: Tracing\nNext\nOpenAI\nLast updated \n16 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the benefits of using OpenTelemetry with Arize Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Using OpenTelemetry with Arize Phoenix provides the benefit of enabling seamless integration with a variety of frameworks and SDKs across Python and JavaScript. This integration allows for auto-instrumentation, making it easier for AI Engineers and Data Scientists to monitor, evaluate, and troubleshoot their AI models effectively.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6ODU=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/customize-traces\ntitle: Customize Traces\n\nCustomize Traces\nOftentimes you want to customize various aspects of traces you log to Phoenix\nLog to a specific project\nPhoenix uses \nprojects\n to group traces. If left unspecified, all traces are sent to a default project.\nNotebook\nServer (Python)\nIn the notebook, you can set the \nPHOENIX_PROJECT_NAME\n environment variable \nbefore \nadding instrumentation or running any of your code.\nIn python this would look like:\nCopy\nimport\n os\n\n\n\n\nos\n.\nenviron\n[\n\'PHOENIX_PROJECT_NAME\'\n]\n \n=\n \n""<your-project-name>""\nNote that setting a project via an environment variable only works in a notebook and must be done \nBEFORE\n instrumentation is initialized. If you are using OpenInference Instrumentation, see the Server tab for how to set the project name in the Resource attributes.\nIf you are using Phoenix as a collector and running your application separately, you can set the project name in the \nResource\n attributes for the trace provider. \nCopy\nfrom\n openinference\n.\nsemconv\n.\nresource \nimport\n ResourceAttributes\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\nresources \nimport\n Resource\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\n\n\nresource \n=\n \nResource\n(attributes\n=\n{\n\n\n    ResourceAttributes.PROJECT_NAME: \n\'<your-project-name>\'\n\n\n})\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n(resource\n=\nresource)\n\n\nspan_exporter \n=\n \nOTLPSpanExporter\n(endpoint\n=\n""http://phoenix:6006/v1/traces""\n)\n\n\nspan_processor \n=\n \nSimpleSpanProcessor\n(span_exporter\n=\nspan_exporter)\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor\n=\nspan_processor)\n\n\ntrace_api\n.\nset_tracer_provider\n(tracer_provider\n=\ntracer_provider)\n\n\n# Add any auto-instrumentation you want \n\n\nLlamaIndexInstrumentor\n().\ninstrument\n()\nProjects work by setting something called the \nResource\n attributes (as seen in the Server example above). The phoenix server uses the project name attribute to group traces into the appropriate project.\nSwitching projects in a notebook\nTypically you want traces for an LLM app to all be grouped in one project. However, while working with Phoenix inside a notebook, we provide a utility to temporarily associate spans with different projects.  You can use this to trace things like evaluations.\nNotebook\nCopy\nfrom\n phoenix\n.\ntrace \nimport\n using_project\n\n\n\n\n# Switch project to run evals\n\n\nwith\n \nusing_project\n(\n""my-eval-project""\n):\n\n\n    \n# all spans created within this context will be associated with\n\n\n    \n# the ""my-eval-project"" project.\n\n\n    \n# Run evaluations here...\nAdding custom metadata to spans\nSpans produced by \nauto-instrumentation\n can get you very far. However at some point you may want to track \nmetadata\n - things like account or user info. \n\nLangChain\nDSPy\nWith LangChain, you can provide metadata directly via the chain or to to an invocation of a chain.\nCopy\n# Pass metadata into the chain\n\n\nllm \n=\n \nLLMChain\n(llm\n=\nOpenAI\n(), prompt\n=\nprompt, metadata\n=\n{\n""category""\n: \n""jokes""\n})\n\n\n\n\n# Pass metadata into the invocation\n\n\ncompletion \n=\n llm\n.\npredict\n(adjective\n=\n""funny""\n, metadata\n=\n{\n""variant""\n: \n""funny""\n})\n\n\nprint\n(completion)\nTo add metadata to a span, you will have to use OpenTelemetry\'s trace_api. \nCopy\nimport\n dspy\n\n\nfrom\n openinference\n.\nsemconv\n.\ntrace \nimport\n SpanAttributes\n\n\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\n\n\nclass\n \nQuestionClassifier\n(\ndspy\n.\nModule\n):\n\n\n    \ndef\n \n__init__\n(\nself\n):\n\n\n        \nsuper\n().\n__init__\n()\n\n\n        ...\n\n\n    \ndef\n \nforward\n(\nself\n,\n \nquestion\n:\n \nstr\n) \n->\n tuple\n[\nstr\n,\nstr\n]\n:\n\n\n        current_span \n=\n trace_api\n.\nget_current_span\n()\n\n\n        current_span\n.\nset_attribute\n(SpanAttributes.METADATA, \n""{ \'foo\': \'bar\' }""\n)\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing\ntitle: How-to: Tracing\n\nHow-to: Tracing\nGuides on how to use traces\nSetup Tracing\nSetup Tracing in \nPython\n or \nTypeScript\nAdd Integrations via \nAuto Intstrumentation\nTracing Core Concepts\nHow to log traces\nHow to turn off tracing\nCustomize Traces\nHow to log to a specific project\nHow to switch projects in a notebook\nHow to add auto-instrumentation\nHow to create custom spans\nHow to add custom metadata\nCustomize Spans\nHow to set custom attributes and semantic attributes to child spans and spans created by auto-instrumentors.\nSetting metadata\nSetting tags\nSetting a user\nSetting prompt template attributes\nHow to read attributes from context\nAuto Instrumentation\nUse auto-instrumentation to export traces for common frameworks and libraries\nAuto Instrument: Python\nInstrument LlamaIndex\nInstrument LangChain\nInstrument OpenAI\nInstrument DSPy\nInstrument AWS Bedrock\nInstrument AutoGen\nAuto Instrument: TypeScript\nInstrument OpenAI Node SDK\nInstrument LangChain.js\nManual Instrumentation\nCreate and customize spans for your use-case\nInstrument: Python\nHow to acquire a Tracer\nHow to create spans\nHow to create nested spans\nHow to create spans with decorators\nHow to get the current span\nHow to add attributes to a span\nHow to add semantic attributes\nHow to add events\nHow to set a span\'s status\nHow to record exceptions\nInstrument: TypeScript\nQuerying Spans\nHow to query spans for to construct DataFrames to use for evaluation\nHow to run a query\nHow to specify a project\nHow to query for documents\nHow to apply filters\nHow to extract attributes\nHow to use data for evaluation\nHow to use pre-defined queries\nLog Evaluation Results\nHow to log evaluation results to annotate traces with evals\nHow to log span evaluations\nHow to log document evaluations\nHow to specify a project for logging evaluations\nSave and Load Traces\nSaving Traces\nLoading Traces\nTrace a Deployed Application\nHow to instrument an application\nHow to deploy a Phoenix server (collector)\nHow to use Arize as a collector \nPrevious\nAnnotating Traces\nNext\nSetup Tracing\nLast updated \n8 hours ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What customization options are available for tracing?\nAnswer: '}, {'role': 'assistant', 'content': 'Customization options available for tracing include logging to a specific project, switching projects in a notebook, adding custom metadata to spans, setting custom attributes and semantic attributes to child spans, setting metadata, tags, user information, prompt template attributes, and reading attributes from context.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/customize-traces\ntitle: Customize Traces\n\nCustomize Traces\nOftentimes you want to customize various aspects of traces you log to Phoenix\nLog to a specific project\nPhoenix uses \nprojects\n to group traces. If left unspecified, all traces are sent to a default project.\nNotebook\nServer (Python)\nIn the notebook, you can set the \nPHOENIX_PROJECT_NAME\n environment variable \nbefore \nadding instrumentation or running any of your code.\nIn python this would look like:\nCopy\nimport\n os\n\n\n\n\nos\n.\nenviron\n[\n\'PHOENIX_PROJECT_NAME\'\n]\n \n=\n \n""<your-project-name>""\nNote that setting a project via an environment variable only works in a notebook and must be done \nBEFORE\n instrumentation is initialized. If you are using OpenInference Instrumentation, see the Server tab for how to set the project name in the Resource attributes.\nIf you are using Phoenix as a collector and running your application separately, you can set the project name in the \nResource\n attributes for the trace provider. \nCopy\nfrom\n openinference\n.\nsemconv\n.\nresource \nimport\n ResourceAttributes\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\nresources \nimport\n Resource\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\n\n\nresource \n=\n \nResource\n(attributes\n=\n{\n\n\n    ResourceAttributes.PROJECT_NAME: \n\'<your-project-name>\'\n\n\n})\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n(resource\n=\nresource)\n\n\nspan_exporter \n=\n \nOTLPSpanExporter\n(endpoint\n=\n""http://phoenix:6006/v1/traces""\n)\n\n\nspan_processor \n=\n \nSimpleSpanProcessor\n(span_exporter\n=\nspan_exporter)\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor\n=\nspan_processor)\n\n\ntrace_api\n.\nset_tracer_provider\n(tracer_provider\n=\ntracer_provider)\n\n\n# Add any auto-instrumentation you want \n\n\nLlamaIndexInstrumentor\n().\ninstrument\n()\nProjects work by setting something called the \nResource\n attributes (as seen in the Server example above). The phoenix server uses the project name attribute to group traces into the appropriate project.\nSwitching projects in a notebook\nTypically you want traces for an LLM app to all be grouped in one project. However, while working with Phoenix inside a notebook, we provide a utility to temporarily associate spans with different projects.  You can use this to trace things like evaluations.\nNotebook\nCopy\nfrom\n phoenix\n.\ntrace \nimport\n using_project\n\n\n\n\n# Switch project to run evals\n\n\nwith\n \nusing_project\n(\n""my-eval-project""\n):\n\n\n    \n# all spans created within this context will be associated with\n\n\n    \n# the ""my-eval-project"" project.\n\n\n    \n# Run evaluations here...\nAdding custom metadata to spans\nSpans produced by \nauto-instrumentation\n can get you very far. However at some point you may want to track \nmetadata\n - things like account or user info. \n\nLangChain\nDSPy\nWith LangChain, you can provide metadata directly via the chain or to to an invocation of a chain.\nCopy\n# Pass metadata into the chain\n\n\nllm \n=\n \nLLMChain\n(llm\n=\nOpenAI\n(), prompt\n=\nprompt, metadata\n=\n{\n""category""\n: \n""jokes""\n})\n\n\n\n\n# Pass metadata into the invocation\n\n\ncompletion \n=\n llm\n.\npredict\n(adjective\n=\n""funny""\n, metadata\n=\n{\n""variant""\n: \n""funny""\n})\n\n\nprint\n(completion)\nTo add metadata to a span, you will have to use OpenTelemetry\'s trace_api. \nCopy\nimport\n dspy\n\n\nfrom\n openinference\n.\nsemconv\n.\ntrace \nimport\n SpanAttributes\n\n\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\n\n\nclass\n \nQuestionClassifier\n(\ndspy\n.\nModule\n):\n\n\n    \ndef\n \n__init__\n(\nself\n):\n\n\n        \nsuper\n().\n__init__\n()\n\n\n        ...\n\n\n    \ndef\n \nforward\n(\nself\n,\n \nquestion\n:\n \nstr\n) \n->\n tuple\n[\nstr\n,\nstr\n]\n:\n\n\n        current_span \n=\n trace_api\n.\nget_current_span\n()\n\n\n        current_span\n.\nset_attribute\n(SpanAttributes.METADATA, \n""{ \'foo\': \'bar\' }""\n)\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing\ntitle: How-to: Tracing\n\nHow-to: Tracing\nGuides on how to use traces\nSetup Tracing\nSetup Tracing in \nPython\n or \nTypeScript\nAdd Integrations via \nAuto Intstrumentation\nTracing Core Concepts\nHow to log traces\nHow to turn off tracing\nCustomize Traces\nHow to log to a specific project\nHow to switch projects in a notebook\nHow to add auto-instrumentation\nHow to create custom spans\nHow to add custom metadata\nCustomize Spans\nHow to set custom attributes and semantic attributes to child spans and spans created by auto-instrumentors.\nSetting metadata\nSetting tags\nSetting a user\nSetting prompt template attributes\nHow to read attributes from context\nAuto Instrumentation\nUse auto-instrumentation to export traces for common frameworks and libraries\nAuto Instrument: Python\nInstrument LlamaIndex\nInstrument LangChain\nInstrument OpenAI\nInstrument DSPy\nInstrument AWS Bedrock\nInstrument AutoGen\nAuto Instrument: TypeScript\nInstrument OpenAI Node SDK\nInstrument LangChain.js\nManual Instrumentation\nCreate and customize spans for your use-case\nInstrument: Python\nHow to acquire a Tracer\nHow to create spans\nHow to create nested spans\nHow to create spans with decorators\nHow to get the current span\nHow to add attributes to a span\nHow to add semantic attributes\nHow to add events\nHow to set a span\'s status\nHow to record exceptions\nInstrument: TypeScript\nQuerying Spans\nHow to query spans for to construct DataFrames to use for evaluation\nHow to run a query\nHow to specify a project\nHow to query for documents\nHow to apply filters\nHow to extract attributes\nHow to use data for evaluation\nHow to use pre-defined queries\nLog Evaluation Results\nHow to log evaluation results to annotate traces with evals\nHow to log span evaluations\nHow to log document evaluations\nHow to specify a project for logging evaluations\nSave and Load Traces\nSaving Traces\nLoading Traces\nTrace a Deployed Application\nHow to instrument an application\nHow to deploy a Phoenix server (collector)\nHow to use Arize as a collector \nPrevious\nAnnotating Traces\nNext\nSetup Tracing\nLast updated \n8 hours ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What customization options are available for tracing?\nAnswer: '}, {'role': 'assistant', 'content': 'Customization options available for tracing include logging to a specific project, switching projects in a notebook, adding custom metadata to spans, setting custom attributes and semantic attributes to child spans, setting metadata, tags, user information, prompt template attributes, and reading attributes from context.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6ODY=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/save-and-load-traces\ntitle: Save and Load Traces\n\nSave and Load Traces\nHow to manually save and load traces\nIn addition to persisting to a database, Phoenix allows you to save and load your trace data to and from external files.\nSaving Traces\nThe initial step involves saving the traces present in a Phoenix instance to a designated location.\nCopy\nmy_traces = px.Client().get_trace_dataset().save()\nLoading Traces\nTo facilitate the retrieval of these saved traces, one can execute the prescribed commands upon starting Phoenix.\nCopy\npx.launch_app(trace=px.TraceDataset.load(my_traces))\nNote the above will save to a default phoenix trace directory, to save in another directory, use the following example below.\nSaving Traces to a Specific Directory\nYou can specify the directory to save your traces by passing a\ndirectory\n argument to the \nsave\n method.\nCopy\nimport os\n\n\n\n\n# Specify and Create the Directory for Trace Dataset\n\n\ndirectory = \'/my_saved_traces\'\n\n\nos.makedirs(directory, exist_ok=True)\n\n\n\n\n# Save the Trace Dataset\n\n\ntrace_id = px.Client().get_trace_dataset().save(directory=directory)\nThis output the trace ID and prints the path of the saved file:\n💾 Trace dataset saved to under ID: f7733fda-6ad6-4427-a803-55ad2182b662 \n📂 Trace dataset path: /my_saved_traces/trace_dataset-f7733fda-6ad6-4427-a803-55ad2182b662.parquet\nLoading Traces from a Specific Directory\nTo load the previously saved trace dataset, use the trace ID and the specific directory path where the trace was stored.\nCopy\npx.launch_app(trace=px.TraceDataset.load(\'f7733fda-6ad6-4427-a803-55ad2182b662\', directory=""/my_saved_traces/""))\nPrevious\nImporting Existing Traces\nNext\nCapture Feedback\nLast updated \n2 months ago\n\nsource: https://docs.arize.com/phoenix/tracing/llm-traces-1\ntitle: Quickstart: Tracing\n\nQuickstart: Tracing\nInspect the inner-workings of your LLM Application using OpenInference Traces\nOverview\nTracing is a powerful tool for understanding the behavior of your LLM application. Phoenix has best-in-class tracing, irregardless of what framework you use and has first-class instrumentation for a variety of frameworks ( \nLlamaIndex\n, \nLangChain\n,\n DSPy\n),  SDKs (\nOpenAI\n, \nBedrock\n, \nMistral\n, \nVertex\n), and Languages (Python, Javascript). You can also \nmanually instrument\n your application using the OpenTelemetry SDK.\nTo get started with traces, you will first want to start a local Phoenix app. Below we will explore how to use Phoenix in a notebook but you can \ndeploy phoenix \n once you are ready for a persistent observability platform.\nIn your Jupyter or Colab environment, run the following command to install.\nUsing pip\nUsing conda\nCopy\npip\n \ninstall\n \narize-phoenix\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix\nTo get started, launch the phoenix app.\nCopy\nimport\n phoenix \nas\n px\n\n\nsession \n=\n px\n.\nlaunch_app\n()\nThe above launches a Phoenix server that acts as a trace collector for any LLM application running locally in your jupyter notebook!\nCopy\n🌍 To view the Phoenix app in your browser, visit https://z8rwookkcle1-496ff2e9c6d22116-6060-colab.googleusercontent.com/\n\n\n📺 To view the Phoenix app in a notebook, run `px.active_session().view()`\n\n\n📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\nThe \nlaunch_app\n command will spit out a URL for you to view the Phoenix UI. You can access this url again at any time via the \nsession\n.\n\nNow that phoenix is up and running, you can setup tracing for your AI application so that you can debug your application as the traces stream in.\nLlamaIndex\nLangChain\nOpenAI\nTo use llama-index\'s one click, you must install the small integration first:\nCopy\npip\n \ninstall\n \n\'llama-index>=0.10.44\'\nCopy\nimport\n phoenix \nas\n px\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\nimport\n os\n\n\nfrom\n gcsfs \nimport\n GCSFileSystem\n\n\nfrom\n llama_index\n.\ncore \nimport\n (\n\n\n    Settings\n,\n\n\n    VectorStoreIndex\n,\n\n\n    StorageContext\n,\n\n\n    set_global_handler\n,\n\n\n    load_index_from_storage\n\n\n)\n\n\nfrom\n llama_index\n.\nembeddings\n.\nopenai \nimport\n OpenAIEmbedding\n\n\nfrom\n llama_index\n.\nllms\n.\nopenai \nimport\n OpenAI\n\n\nimport\n llama_index\n\n\n\n\n# To view traces in Phoenix, you will first have to start a Phoenix server. You can do this by running the following:\n\n\nsession \n=\n px\n.\nlaunch_app\n()\n\n\n\n\n# Initialize LlamaIndex auto-instrumentation\n\n\nLlamaIndexInstrumentor\n().\ninstrument\n()\n\n\n\n\nos\n.\nenviron\n[\n""OPENAI_API_KEY""\n]\n \n=\n \n""<ENTER_YOUR_OPENAI_API_KEY_HERE>""\n\n\n\n\n# LlamaIndex application initialization may vary\n\n\n# depending on your application\n\n\nSettings\n.\nllm \n=\n \nOpenAI\n(model\n=\n""gpt-4-turbo-preview""\n)\n\n\nSettings\n.\nembed_model \n=\n \nOpenAIEmbedding\n(model\n=\n""text-embedding-ada-002""\n)\n\n\n\n\n\n\n# Load your data and create an index. Here we\'ve provided an example of our documentation\n\n\nfile_system \n=\n \nGCSFileSystem\n(project\n=\n""public-assets-275721""\n)\n\n\nindex_path \n=\n \n""arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/index/""\n\n\nstorage_context \n=\n StorageContext\n.\nfrom_defaults\n(\n\n\n    fs\n=\nfile_system,\n\n\n    persist_dir\n=\nindex_path,\n\n\n)\n\n\n\n\nindex \n=\n \nload_index_from_storage\n(storage_context)\n\n\n\n\nquery_engine \n=\n index\n.\nas_query_engine\n()\n\n\n\n\n# Query your LlamaIndex application\n\n\nquery_engine\n.\nquery\n(\n""What is the meaning of life?""\n)\n\n\nquery_engine\n.\nquery\n(\n""How can I deploy Arize?""\n)\n\n\n\n\n# View the traces in the Phoenix UI\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How is trace data stored in Arize Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Trace data in Arize Phoenix can be stored by saving the traces to a designated location, either persisting them to a database or saving and loading them to and from external files. The trace data can be saved to a default Phoenix trace directory or to a specific directory by specifying the directory path when using the save method. When loading previously saved trace data, the trace ID and the specific directory path where the trace was stored need to be provided.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/save-and-load-traces\ntitle: Save and Load Traces\n\nSave and Load Traces\nHow to manually save and load traces\nIn addition to persisting to a database, Phoenix allows you to save and load your trace data to and from external files.\nSaving Traces\nThe initial step involves saving the traces present in a Phoenix instance to a designated location.\nCopy\nmy_traces = px.Client().get_trace_dataset().save()\nLoading Traces\nTo facilitate the retrieval of these saved traces, one can execute the prescribed commands upon starting Phoenix.\nCopy\npx.launch_app(trace=px.TraceDataset.load(my_traces))\nNote the above will save to a default phoenix trace directory, to save in another directory, use the following example below.\nSaving Traces to a Specific Directory\nYou can specify the directory to save your traces by passing a\ndirectory\n argument to the \nsave\n method.\nCopy\nimport os\n\n\n\n\n# Specify and Create the Directory for Trace Dataset\n\n\ndirectory = \'/my_saved_traces\'\n\n\nos.makedirs(directory, exist_ok=True)\n\n\n\n\n# Save the Trace Dataset\n\n\ntrace_id = px.Client().get_trace_dataset().save(directory=directory)\nThis output the trace ID and prints the path of the saved file:\n💾 Trace dataset saved to under ID: f7733fda-6ad6-4427-a803-55ad2182b662 \n📂 Trace dataset path: /my_saved_traces/trace_dataset-f7733fda-6ad6-4427-a803-55ad2182b662.parquet\nLoading Traces from a Specific Directory\nTo load the previously saved trace dataset, use the trace ID and the specific directory path where the trace was stored.\nCopy\npx.launch_app(trace=px.TraceDataset.load(\'f7733fda-6ad6-4427-a803-55ad2182b662\', directory=""/my_saved_traces/""))\nPrevious\nImporting Existing Traces\nNext\nCapture Feedback\nLast updated \n2 months ago\n\nsource: https://docs.arize.com/phoenix/tracing/llm-traces-1\ntitle: Quickstart: Tracing\n\nQuickstart: Tracing\nInspect the inner-workings of your LLM Application using OpenInference Traces\nOverview\nTracing is a powerful tool for understanding the behavior of your LLM application. Phoenix has best-in-class tracing, irregardless of what framework you use and has first-class instrumentation for a variety of frameworks ( \nLlamaIndex\n, \nLangChain\n,\n DSPy\n),  SDKs (\nOpenAI\n, \nBedrock\n, \nMistral\n, \nVertex\n), and Languages (Python, Javascript). You can also \nmanually instrument\n your application using the OpenTelemetry SDK.\nTo get started with traces, you will first want to start a local Phoenix app. Below we will explore how to use Phoenix in a notebook but you can \ndeploy phoenix \n once you are ready for a persistent observability platform.\nIn your Jupyter or Colab environment, run the following command to install.\nUsing pip\nUsing conda\nCopy\npip\n \ninstall\n \narize-phoenix\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix\nTo get started, launch the phoenix app.\nCopy\nimport\n phoenix \nas\n px\n\n\nsession \n=\n px\n.\nlaunch_app\n()\nThe above launches a Phoenix server that acts as a trace collector for any LLM application running locally in your jupyter notebook!\nCopy\n🌍 To view the Phoenix app in your browser, visit https://z8rwookkcle1-496ff2e9c6d22116-6060-colab.googleusercontent.com/\n\n\n📺 To view the Phoenix app in a notebook, run `px.active_session().view()`\n\n\n📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\nThe \nlaunch_app\n command will spit out a URL for you to view the Phoenix UI. You can access this url again at any time via the \nsession\n.\n\nNow that phoenix is up and running, you can setup tracing for your AI application so that you can debug your application as the traces stream in.\nLlamaIndex\nLangChain\nOpenAI\nTo use llama-index\'s one click, you must install the small integration first:\nCopy\npip\n \ninstall\n \n\'llama-index>=0.10.44\'\nCopy\nimport\n phoenix \nas\n px\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\nimport\n os\n\n\nfrom\n gcsfs \nimport\n GCSFileSystem\n\n\nfrom\n llama_index\n.\ncore \nimport\n (\n\n\n    Settings\n,\n\n\n    VectorStoreIndex\n,\n\n\n    StorageContext\n,\n\n\n    set_global_handler\n,\n\n\n    load_index_from_storage\n\n\n)\n\n\nfrom\n llama_index\n.\nembeddings\n.\nopenai \nimport\n OpenAIEmbedding\n\n\nfrom\n llama_index\n.\nllms\n.\nopenai \nimport\n OpenAI\n\n\nimport\n llama_index\n\n\n\n\n# To view traces in Phoenix, you will first have to start a Phoenix server. You can do this by running the following:\n\n\nsession \n=\n px\n.\nlaunch_app\n()\n\n\n\n\n# Initialize LlamaIndex auto-instrumentation\n\n\nLlamaIndexInstrumentor\n().\ninstrument\n()\n\n\n\n\nos\n.\nenviron\n[\n""OPENAI_API_KEY""\n]\n \n=\n \n""<ENTER_YOUR_OPENAI_API_KEY_HERE>""\n\n\n\n\n# LlamaIndex application initialization may vary\n\n\n# depending on your application\n\n\nSettings\n.\nllm \n=\n \nOpenAI\n(model\n=\n""gpt-4-turbo-preview""\n)\n\n\nSettings\n.\nembed_model \n=\n \nOpenAIEmbedding\n(model\n=\n""text-embedding-ada-002""\n)\n\n\n\n\n\n\n# Load your data and create an index. Here we\'ve provided an example of our documentation\n\n\nfile_system \n=\n \nGCSFileSystem\n(project\n=\n""public-assets-275721""\n)\n\n\nindex_path \n=\n \n""arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/index/""\n\n\nstorage_context \n=\n StorageContext\n.\nfrom_defaults\n(\n\n\n    fs\n=\nfile_system,\n\n\n    persist_dir\n=\nindex_path,\n\n\n)\n\n\n\n\nindex \n=\n \nload_index_from_storage\n(storage_context)\n\n\n\n\nquery_engine \n=\n index\n.\nas_query_engine\n()\n\n\n\n\n# Query your LlamaIndex application\n\n\nquery_engine\n.\nquery\n(\n""What is the meaning of life?""\n)\n\n\nquery_engine\n.\nquery\n(\n""How can I deploy Arize?""\n)\n\n\n\n\n# View the traces in the Phoenix UI\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How is trace data stored in Arize Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Trace data in Arize Phoenix can be stored by saving the traces to a designated location, either persisting them to a database or saving and loading them to and from external files. The trace data can be saved to a default Phoenix trace directory or to a specific directory by specifying the directory path when using the save method. When loading previously saved trace data, the trace ID and the specific directory path where the trace was stored need to be provided.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6ODc=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/importing-existing-traces\ntitle: Importing Existing Traces\n\nImporting Existing Traces\nPhoenix also supports loading data that contains \nOpenInference traces\n. This allows you to load an existing dataframe of traces into your Phoenix instance.\nFrom the App\nCopy\n# Re-launch the app using trace data\n\n\npx\n.\nlaunch_app\n(trace\n=\npx.\nTraceDataset\n(df))\n\n\n\n\n# Load traces into an existing Phoenix instance\n\n\npx\n.\nClient\n().\nlog_traces\n(trace_dataset\n=\npx.\nTraceDataset\n(df))\nPrevious\nLog Evaluation Results\nNext\nSave and Load Traces\nLast updated \n1 month ago\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/save-and-load-traces\ntitle: Save and Load Traces\n\nSave and Load Traces\nHow to manually save and load traces\nIn addition to persisting to a database, Phoenix allows you to save and load your trace data to and from external files.\nSaving Traces\nThe initial step involves saving the traces present in a Phoenix instance to a designated location.\nCopy\nmy_traces = px.Client().get_trace_dataset().save()\nLoading Traces\nTo facilitate the retrieval of these saved traces, one can execute the prescribed commands upon starting Phoenix.\nCopy\npx.launch_app(trace=px.TraceDataset.load(my_traces))\nNote the above will save to a default phoenix trace directory, to save in another directory, use the following example below.\nSaving Traces to a Specific Directory\nYou can specify the directory to save your traces by passing a\ndirectory\n argument to the \nsave\n method.\nCopy\nimport os\n\n\n\n\n# Specify and Create the Directory for Trace Dataset\n\n\ndirectory = \'/my_saved_traces\'\n\n\nos.makedirs(directory, exist_ok=True)\n\n\n\n\n# Save the Trace Dataset\n\n\ntrace_id = px.Client().get_trace_dataset().save(directory=directory)\nThis output the trace ID and prints the path of the saved file:\n💾 Trace dataset saved to under ID: f7733fda-6ad6-4427-a803-55ad2182b662 \n📂 Trace dataset path: /my_saved_traces/trace_dataset-f7733fda-6ad6-4427-a803-55ad2182b662.parquet\nLoading Traces from a Specific Directory\nTo load the previously saved trace dataset, use the trace ID and the specific directory path where the trace was stored.\nCopy\npx.launch_app(trace=px.TraceDataset.load(\'f7733fda-6ad6-4427-a803-55ad2182b662\', directory=""/my_saved_traces/""))\nPrevious\nImporting Existing Traces\nNext\nCapture Feedback\nLast updated \n2 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I import existing traces?\nAnswer: '}, {'role': 'assistant', 'content': 'To import existing traces, you can either re-launch the app using trace data or load traces into an existing Phoenix instance. The first method involves using the `px.launch_app` function with the trace data, while the second method requires utilizing the `px.Client().log_traces` function with the trace dataset.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/importing-existing-traces\ntitle: Importing Existing Traces\n\nImporting Existing Traces\nPhoenix also supports loading data that contains \nOpenInference traces\n. This allows you to load an existing dataframe of traces into your Phoenix instance.\nFrom the App\nCopy\n# Re-launch the app using trace data\n\n\npx\n.\nlaunch_app\n(trace\n=\npx.\nTraceDataset\n(df))\n\n\n\n\n# Load traces into an existing Phoenix instance\n\n\npx\n.\nClient\n().\nlog_traces\n(trace_dataset\n=\npx.\nTraceDataset\n(df))\nPrevious\nLog Evaluation Results\nNext\nSave and Load Traces\nLast updated \n1 month ago\n\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/save-and-load-traces\ntitle: Save and Load Traces\n\nSave and Load Traces\nHow to manually save and load traces\nIn addition to persisting to a database, Phoenix allows you to save and load your trace data to and from external files.\nSaving Traces\nThe initial step involves saving the traces present in a Phoenix instance to a designated location.\nCopy\nmy_traces = px.Client().get_trace_dataset().save()\nLoading Traces\nTo facilitate the retrieval of these saved traces, one can execute the prescribed commands upon starting Phoenix.\nCopy\npx.launch_app(trace=px.TraceDataset.load(my_traces))\nNote the above will save to a default phoenix trace directory, to save in another directory, use the following example below.\nSaving Traces to a Specific Directory\nYou can specify the directory to save your traces by passing a\ndirectory\n argument to the \nsave\n method.\nCopy\nimport os\n\n\n\n\n# Specify and Create the Directory for Trace Dataset\n\n\ndirectory = \'/my_saved_traces\'\n\n\nos.makedirs(directory, exist_ok=True)\n\n\n\n\n# Save the Trace Dataset\n\n\ntrace_id = px.Client().get_trace_dataset().save(directory=directory)\nThis output the trace ID and prints the path of the saved file:\n💾 Trace dataset saved to under ID: f7733fda-6ad6-4427-a803-55ad2182b662 \n📂 Trace dataset path: /my_saved_traces/trace_dataset-f7733fda-6ad6-4427-a803-55ad2182b662.parquet\nLoading Traces from a Specific Directory\nTo load the previously saved trace dataset, use the trace ID and the specific directory path where the trace was stored.\nCopy\npx.launch_app(trace=px.TraceDataset.load(\'f7733fda-6ad6-4427-a803-55ad2182b662\', directory=""/my_saved_traces/""))\nPrevious\nImporting Existing Traces\nNext\nCapture Feedback\nLast updated \n2 months ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I import existing traces?\nAnswer: '}, {'role': 'assistant', 'content': 'To import existing traces, you can either re-launch the app using trace data or load traces into an existing Phoenix instance. The first method involves using the `px.launch_app` function with the trace data, while the second method requires utilizing the `px.Client().log_traces` function with the trace dataset.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6ODg=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/concepts-datasets\ntitle: Concepts: Datasets\n\nConcepts: Datasets\nThere are many ways to build datasets for experimentation and evaluation.\nDatasets\nDatasets are integral to evaluation and experimentation. They are collections of examples that provide the \ninputs\n and, optionally, expected \nreference\n outputs for assessing your application. Each example within a dataset represents a single data point, consisting of an \ninputs\n dictionary, an optional \noutput\n dictionary, and an optional \nmetadata\n dictionary. The \noptional\n output dictionary often contains the the expected LLM application output for the given input.\nDatasets allow you to collect data from production, staging, evaluations, and even manually. The examples collected are then used to run experiments and evaluations to track improvements.\nUse datasets to:\nStore evaluation test cases for your eval script instead of managing large JSONL or CSV files\nCapture generations to assess quality manually or using LLM-graded evals\nStore user reviewed generations to find new test cases\nWith Phoenix, datasets are:\nIntegrated\n. Datasets are integrated with the platform, so you can add production spans to datasets, use datasets to run experiments, and use metadata to track different segments and use-cases.\nVersioned\n. Every insert, update, and delete is versioned, so you can pin experiments and evaluations to a specific version of a dataset and track changes over time.\nCreating Datasets\nThere are various ways to get started with datasets:\nManually Curated Examples\nThis is how we recommend you start. From building your application, you probably have an idea of what types of inputs you expect your application to be able to handle, and what ""good"" responses look like. You probably want to cover a few different common edge cases or situations you can imagine. Even 20 high quality, manually curated examples can go a long way.\nHistorical Logs\nOnce you ship an application, you start gleaning valuable information: how users are actually using it. This information can be valuable to capture and store in datasets. This allows you to test against specific use cases as you iterate on your application.\nIf your application is going well, you will likely get a lot of usage. How can you determine which datapoints are valuable to add? There are a few heuristics you can follow. If possible, try to collect end user feedback. You can then see which datapoints got negative feedback. That is super valuable! These are spots where your application did not perform well. You should add these to your dataset to test against in the future. You can also use other heuristics to identify interesting datapoints - for example, runs that took a long time to complete could be interesting to analyze and add to a dataset.\nSynthetic Data\nOnce you have a few examples, you can try to artificially generate examples to get a lot of datapoints quickly. It\'s generally advised to have a few good handcrafted examples before this step, as the synthetic data will often resemble the source examples in some way. \nDataset Contents\nWhile Phoenix doesn\'t have dataset types, conceptually you can contain:\nKey-Value Pairs:\nInputs and outputs are arbitrary key-value pairs.\nThis dataset type is ideal for evaluating prompts, functions, and agents that require multiple inputs or generate multiple outputs.\nPrompt Template\nIf you have a RAG prompt template such as:\nCopy\nGiven the context information and not prior knowledge, answer the query.\n\n\n---------------------\n\n\n{context}\n\n\n---------------------\n\n\n\n\nQuery: {query}\n\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/how-to-datasets\ntitle: How-to: Datasets\n\nHow-to: Datasets\nHow to create datasets\nCreate datasets from Pandas\nCreate datasets from spans \nCreate datasets using synthetic data\nExporting datasets\nExporting to CSV\nExporting to OpenAI Ft\nExporting to OpenAI Evals\nPrevious\nConcepts: Datasets\nNext\nCreating Datasets\nLast updated \n1 month ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I create a custom dataset?\nAnswer: '}, {'role': 'assistant', 'content': ""You can create a custom dataset by manually curating examples, using historical logs, or generating synthetic data. Manually curated examples involve selecting high-quality examples based on your application's expected inputs and desired responses. Historical logs provide valuable insights into how users interact with your application, which can be used to create specific use cases for testing. Synthetic data generation can be employed to quickly generate a large number of data points, but it's recommended to have some handcrafted examples before using synthetic data.""}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/concepts-datasets\ntitle: Concepts: Datasets\n\nConcepts: Datasets\nThere are many ways to build datasets for experimentation and evaluation.\nDatasets\nDatasets are integral to evaluation and experimentation. They are collections of examples that provide the \ninputs\n and, optionally, expected \nreference\n outputs for assessing your application. Each example within a dataset represents a single data point, consisting of an \ninputs\n dictionary, an optional \noutput\n dictionary, and an optional \nmetadata\n dictionary. The \noptional\n output dictionary often contains the the expected LLM application output for the given input.\nDatasets allow you to collect data from production, staging, evaluations, and even manually. The examples collected are then used to run experiments and evaluations to track improvements.\nUse datasets to:\nStore evaluation test cases for your eval script instead of managing large JSONL or CSV files\nCapture generations to assess quality manually or using LLM-graded evals\nStore user reviewed generations to find new test cases\nWith Phoenix, datasets are:\nIntegrated\n. Datasets are integrated with the platform, so you can add production spans to datasets, use datasets to run experiments, and use metadata to track different segments and use-cases.\nVersioned\n. Every insert, update, and delete is versioned, so you can pin experiments and evaluations to a specific version of a dataset and track changes over time.\nCreating Datasets\nThere are various ways to get started with datasets:\nManually Curated Examples\nThis is how we recommend you start. From building your application, you probably have an idea of what types of inputs you expect your application to be able to handle, and what ""good"" responses look like. You probably want to cover a few different common edge cases or situations you can imagine. Even 20 high quality, manually curated examples can go a long way.\nHistorical Logs\nOnce you ship an application, you start gleaning valuable information: how users are actually using it. This information can be valuable to capture and store in datasets. This allows you to test against specific use cases as you iterate on your application.\nIf your application is going well, you will likely get a lot of usage. How can you determine which datapoints are valuable to add? There are a few heuristics you can follow. If possible, try to collect end user feedback. You can then see which datapoints got negative feedback. That is super valuable! These are spots where your application did not perform well. You should add these to your dataset to test against in the future. You can also use other heuristics to identify interesting datapoints - for example, runs that took a long time to complete could be interesting to analyze and add to a dataset.\nSynthetic Data\nOnce you have a few examples, you can try to artificially generate examples to get a lot of datapoints quickly. It\'s generally advised to have a few good handcrafted examples before this step, as the synthetic data will often resemble the source examples in some way. \nDataset Contents\nWhile Phoenix doesn\'t have dataset types, conceptually you can contain:\nKey-Value Pairs:\nInputs and outputs are arbitrary key-value pairs.\nThis dataset type is ideal for evaluating prompts, functions, and agents that require multiple inputs or generate multiple outputs.\nPrompt Template\nIf you have a RAG prompt template such as:\nCopy\nGiven the context information and not prior knowledge, answer the query.\n\n\n---------------------\n\n\n{context}\n\n\n---------------------\n\n\n\n\nQuery: {query}\n\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/how-to-datasets\ntitle: How-to: Datasets\n\nHow-to: Datasets\nHow to create datasets\nCreate datasets from Pandas\nCreate datasets from spans \nCreate datasets using synthetic data\nExporting datasets\nExporting to CSV\nExporting to OpenAI Ft\nExporting to OpenAI Evals\nPrevious\nConcepts: Datasets\nNext\nCreating Datasets\nLast updated \n1 month ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I create a custom dataset?\nAnswer: '}, {'role': 'assistant', 'content': ""You can create a custom dataset by manually curating examples, using historical logs, or generating synthetic data. Manually curated examples involve selecting high-quality examples based on your application's expected inputs and desired responses. Historical logs provide valuable insights into how users interact with your application, which can be used to create specific use cases for testing. Synthetic data generation can be employed to quickly generate a large number of data points, but it's recommended to have some handcrafted examples before using synthetic data.""}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6ODk=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/how-to-datasets/exporting-datasets\ntitle: Exporting Datasets\n\nExporting Datasets\nExporting for Fine-Tuning\nFine-tuning lets you get more out of the models available by providing:\nHigher quality results than prompting\nAbility to train on more examples than can fit in a prompt\nToken savings due to shorter prompts\nLower latency requests\nFine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt, letting you achieve better results on a wide number of tasks. \nOnce a model has been fine-tuned, you won't need to provide as many examples in the prompt.\n This saves costs and enables lower-latency requests.\n\nPhoenix natively exports OpenAI Fine-Tuning JSONL as long as the dataset contains compatible inputs  and outputs.\nExporting OpenAI Evals\nEvals provide a framework for evaluating large language models (LLMs) or systems built using LLMs. OpenAI Evals offer an existing registry of evals to test different dimensions of OpenAI models and the ability to write your own custom evals for use cases you care about. You can also use your data to build private evals. Phoenix can natively export the OpenAI Evals format as JSONL so you can use it with OpenAI Evals. See \nhttps://github.com/openai/evals\n for details.\nPrevious\nCreating Datasets\nNext\nHow-to: Experiments\nLast updated \n1 month ago\n\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/how-to-datasets\ntitle: How-to: Datasets\n\nHow-to: Datasets\nHow to create datasets\nCreate datasets from Pandas\nCreate datasets from spans \nCreate datasets using synthetic data\nExporting datasets\nExporting to CSV\nExporting to OpenAI Ft\nExporting to OpenAI Evals\nPrevious\nConcepts: Datasets\nNext\nCreating Datasets\nLast updated \n1 month ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I export datasets?\nAnswer: ""}, {'role': 'assistant', 'content': 'You can export datasets by using Phoenix to export to CSV, OpenAI Ft, or OpenAI Evals formats.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/how-to-datasets/exporting-datasets\ntitle: Exporting Datasets\n\nExporting Datasets\nExporting for Fine-Tuning\nFine-tuning lets you get more out of the models available by providing:\nHigher quality results than prompting\nAbility to train on more examples than can fit in a prompt\nToken savings due to shorter prompts\nLower latency requests\nFine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt, letting you achieve better results on a wide number of tasks. \nOnce a model has been fine-tuned, you won't need to provide as many examples in the prompt.\n This saves costs and enables lower-latency requests.\n\nPhoenix natively exports OpenAI Fine-Tuning JSONL as long as the dataset contains compatible inputs  and outputs.\nExporting OpenAI Evals\nEvals provide a framework for evaluating large language models (LLMs) or systems built using LLMs. OpenAI Evals offer an existing registry of evals to test different dimensions of OpenAI models and the ability to write your own custom evals for use cases you care about. You can also use your data to build private evals. Phoenix can natively export the OpenAI Evals format as JSONL so you can use it with OpenAI Evals. See \nhttps://github.com/openai/evals\n for details.\nPrevious\nCreating Datasets\nNext\nHow-to: Experiments\nLast updated \n1 month ago\n\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/how-to-datasets\ntitle: How-to: Datasets\n\nHow-to: Datasets\nHow to create datasets\nCreate datasets from Pandas\nCreate datasets from spans \nCreate datasets using synthetic data\nExporting datasets\nExporting to CSV\nExporting to OpenAI Ft\nExporting to OpenAI Evals\nPrevious\nConcepts: Datasets\nNext\nCreating Datasets\nLast updated \n1 month ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I export datasets?\nAnswer: ""}, {'role': 'assistant', 'content': 'You can export datasets by using Phoenix to export to CSV, OpenAI Ft, or OpenAI Evals formats.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6OTA=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/reference/contribute-to-phoenix\ntitle: Contribute to Phoenix\n\nContribute to Phoenix\nIf you want to contribute to the cutting edge of LLM and ML Observability, you've come to the right place!\nTo get started, please check out the following:\nOur development guide\nCode of conduct\nContribution License Agreement\nPicking a GitHub Issue\nWe encourage you to start with an issue labeled with the tag \ngood first issue\n on theGitHub issue board, to get familiar with our codebase as a first-time contributor.\nSubmit Your Code\nTo submit your code, \nfork the Phoenix repository\n, create a \nnew branch\n on your fork, and open \na Pull Request (PR)\n once your work is ready for review.\nIn the PR template, please describe the change, including the motivation/context, test coverage, and any other relevant information. Please note if the PR is a breaking change or if it is related to an open GitHub issue.\nA Core reviewer will review your PR in around one business day and provide feedback on any changes it requires to be approved. Once approved and all the tests pass, the reviewer will click the Squash and merge button in Github 🥳.\nYour PR is now merged into Phoenix! We’ll shout out your contribution in the release notes.\nPrevious\nOpenInference\nLast updated \n6 months ago\n\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can I contribute to Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'To contribute to Phoenix, you can start by checking out the development guide, code of conduct, and Contribution License Agreement. It is encouraged to pick a GitHub issue labeled with the tag ""good first issue"" to familiarize yourself with the codebase. To submit your code, fork the Phoenix repository, create a new branch, and open a Pull Request (PR) once your work is ready for review. In the PR template, describe the change, including the motivation/context, test coverage, and any other relevant information. A Core reviewer will review your PR, provide feedback, and once approved, your contribution will be merged into Phoenix.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/reference/contribute-to-phoenix\ntitle: Contribute to Phoenix\n\nContribute to Phoenix\nIf you want to contribute to the cutting edge of LLM and ML Observability, you've come to the right place!\nTo get started, please check out the following:\nOur development guide\nCode of conduct\nContribution License Agreement\nPicking a GitHub Issue\nWe encourage you to start with an issue labeled with the tag \ngood first issue\n on theGitHub issue board, to get familiar with our codebase as a first-time contributor.\nSubmit Your Code\nTo submit your code, \nfork the Phoenix repository\n, create a \nnew branch\n on your fork, and open \na Pull Request (PR)\n once your work is ready for review.\nIn the PR template, please describe the change, including the motivation/context, test coverage, and any other relevant information. Please note if the PR is a breaking change or if it is related to an open GitHub issue.\nA Core reviewer will review your PR in around one business day and provide feedback on any changes it requires to be approved. Once approved and all the tests pass, the reviewer will click the Squash and merge button in Github 🥳.\nYour PR is now merged into Phoenix! We’ll shout out your contribution in the release notes.\nPrevious\nOpenInference\nLast updated \n6 months ago\n\nsource: https://docs.arize.com/phoenix/\ntitle: Arize Phoenix\n\nArize Phoenix\nAI Observability and Evaluation\nPhoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n\nPhoenix is built by \nArize AI\n, the company behind the the industry-leading AI observability platform,  and a set of core contributors.\nInstall Phoenix\nUsing pip\nUsing conda\nContainer\nIn your Jupyter or Colab environment, run the following command to install.\nCopy\npip\n \ninstall\n \narize-phoenix\nFor full details on how to run phoenix in various environments such as Databricks, consult our \nenvironments guide.\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix[evals]\nPhoenix can also run via a container. The image can be found at:\nCheckout the \nenvironments section\n and \ndeployment guide\n for details.\nPhoenix works with OpenTelemetry and \nOpenInference\n instrumentation. If you are looking to deploy phoenix as a service rather than a library, see \nDeployment\nQuickstarts\nRunning Phoenix for the first time? Select a quickstart below. \nDemo\nNext Steps\nTry our Tutorials\nCheck out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more.  \nCommunity\nJoin the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers. \nNext\nQuickstart\nLast updated \n18 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How can I contribute to Phoenix?\nAnswer: ""}, {'role': 'assistant', 'content': 'To contribute to Phoenix, you can start by checking out the development guide, code of conduct, and Contribution License Agreement. It is encouraged to pick a GitHub issue labeled with the tag ""good first issue"" to familiarize yourself with the codebase. To submit your code, fork the Phoenix repository, create a new branch, and open a Pull Request (PR) once your work is ready for review. In the PR template, describe the change, including the motivation/context, test coverage, and any other relevant information. A Core reviewer will review your PR, provide feedback, and once approved, your contribution will be merged into Phoenix.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6OTE=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/setup/environments\ntitle: Environments\n\nEnvironments\nThe Phoenix app can be run in various notebook environments such as colab and SageMaker as well as be served via the terminal or a docker container\nIn the notebook\nRun phoenix in the notebook as you run experiments\nAs a Container\nStart a long-running collector\nFrom the Terminal\nRun phoenix via the CLI\nNotebooks\nTo start phoenix in the notebook environment, run:\nCopy\nimport\n phoenix \nas\n px\n\n\n\n\nsession \n=\n px\n.\nlaunch_app\n()\nThis will start a local Phoenix server. You can initialize the phoenix server with various kinds of data (traces, inferences). Check out the \nAPI for details\nWhen running Phoenix in the notebook, by default it does not persist your data.\nContainer\nContainer images are still actively being worked on. If you are interested in hosted phoenix, please get in touch!\nPhoenix server images are available via \nDocker Hub\n. The hosted phoenix server runs as a trace collector and can be used if you want observability for LLM traces via docker compose or simply want a long-running phoenix instance. Below are examples of how to run phoenix va Docker for a specific version.\nFirst pull the image you want to run (note you can use the tag \nlatest\n if you would just like the latest version)\nCopy\ndocker\n \npull\n \narizephoenix/phoenix:latest\nNow you can run the image you pulled (note you must expose the port \n6006\n so you can view the UI).\nCopy\ndocker\n \nrun\n \n-p\n \n6006:6006\n \narizephoenix/phoenix:latest\nThe Phoenix UI will be available at \nlocalhost:6006\n.\nIf you deploy the Phoenix server (collector) to a remote machine, you will have to make sure to configure the remote endpoint as the collector endpoint.\nSet Endpoint Environment Variable\nCopy\nimport\n os\n\n\n\n\nos\n.\nenviron\n[\n""PHOENIX_COLLECTOR_ENDPOINT""\n]\n \n=\n \n""http://123.456.789:6006""\nNote that the above is only necessary if your application is running in a Jupyter notebook. If you are trying to deploy your application and have phoenix collect traces via a container, please consult the \ndeployment guide.\nTerminal\nIf you want to start a phoenix server to collect traces, you can also run phoenix directly from the command line\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \nserve\nThis will start the phoenix server on port 6006. If you are running your instrumented notebook or application on the same machine, traces should automatically be exported to \nhttp://127.0.0.1:6006\n so no additional configuration is needed. However if the server is running remotely, you will have to modify the environment variable \nPHOENIX_COLLECTOR_ENDPOINT\n to point to that machine (e.g. \nhttp://<my-remote-machine>:<port>\n)\nNote that this command has various configuration options such as \n--host\n and \n--port\n. For example:\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \n--port\n \n1234\n \n--host\n \n0.0.0.0\n \nserve\nPrevious\nSetup\nNext\nConfiguration\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/quickstart\ntitle: Quickstart\n\nQuickstart\nThe easiest way to run Phoenix is locally in your own computer. To launch Phoenix, use the following steps.\nInstall\nUsing pip\nUsing conda\nDocker\napp.phoenix.arize.com\nCopy\npip\n \ninstall\n \narize-phoenix\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix\nPhoenix server images are available via \nDocker Hub\n and can be used via \ndocker compose \nor if you simply want a long-running phoenix instance to share with your team.\nCopy\ndocker\n \npull\n \narizephoenix/phoenix:latest\nIf you don\'t want to host an instance of Phoenix yourself or use a notebook instance, you can use a persistent instance provided on our site. Sign up for an Arize Phoenix account at \nhttps://app.phoenix.arize.com/login\nFor more details, see \nHosted Phoenix\nLaunch Phoenix\nLaunching phoenix can be done in many ways depending on your use-case.\nCommand Line\nDocker\nNotebook\napp.phoenix.arize.com\nLaunch your local Phoenix instance using:\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \nserve\nFor details on customizing a local terminal deployment, see \nTerminal Setup\n.\nLaunch the phoenix docker image using:\nCopy\ndocker\n \nrun\n \n-p\n \n6006:6006\n \n-p\n \n4317:4317\n \narizephoenix/phoenix:latest\nThis will expose the Phoenix UI and REST API on \nlocalhost:6006\n and exposes the gRPC endpoint for spans on \nlocalhost:4317\nFor more details on customizing a docker deployment, see \nDocker\nWithin your notebook, launch Phoenix using:\nCopy\nimport\n phoenix \nas\n px\n\n\npx\n.\nlaunch_app\n()\nBy default, notebook instances do not have persistent storage, so your traces will disappear after the notebook is closed. See \nPersistence\n or use one of the other deployment options to retain traces.\nHosted Phoenix instances are always online. Nothing more to do here!\nConnect your App\nTo collect traces from your application, you must configure an OpenTelemetry TracerProvider to send traces to Phoenix. The \nregister\n utility from the \nphoenix.otel\n module streamlines this process.\nPython\nTypeScript\nNotebook\napp.phoenix.arize.com\nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your instance using:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n  \nYou do not have to use phoenix.otel to connect to your phoenix instance, you can use OpenTelemetry itself to initialize  your OTEL connection. See\nUsing OTEL Python Directly\nSee \nSetup Tracing: Python\n for more details on configuration and setup\nFor setting up tracing and OpenTelemetry with TypeScript, see \nSetup Tracing: TS\nConnect your notebook to Phoenix:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n \nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your cloud instance using:\nCopy\nimport\n os\n\n\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# Add Phoenix API Key for tracing\n\n\nos\n.\nenviron\n[\n""OTEL_EXPORTER_OTLP_HEADERS""\n]\n \n=\n f\n""api_key=\n{PHOENIX_API_KEY}\n""\n\n\n\n\n# configure the Phoenix tracer\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""https://app.phoenix.arize.com/v1/traces""\n,\n\n\n)\n \nYour Phoenix API key can be found on the Keys section of your dashboard.\nNext Steps\nTrace\n a running application\nRun \nevaluations\n on traces\nTest changes to you prompts, models, and application via \nexperiments\nPrevious\nArize Phoenix\nNext\nHosted Phoenix\nLast updated \n9 hours ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I setup Phoenix in the Terminal?\nAnswer: '}, {'role': 'assistant', 'content': 'To set up Phoenix in the Terminal, you can run the following command:\n```\npython3 -m phoenix.server.main serve\n```\nThis command will start the Phoenix server on port 6006. If your instrumented notebook or application is running on the same machine, traces will automatically be exported to http://127.0.0.1:6006 without needing additional configuration. If the server is running remotely, you will need to modify the environment variable PHOENIX_COLLECTOR_ENDPOINT to point to that machine.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/setup/environments\ntitle: Environments\n\nEnvironments\nThe Phoenix app can be run in various notebook environments such as colab and SageMaker as well as be served via the terminal or a docker container\nIn the notebook\nRun phoenix in the notebook as you run experiments\nAs a Container\nStart a long-running collector\nFrom the Terminal\nRun phoenix via the CLI\nNotebooks\nTo start phoenix in the notebook environment, run:\nCopy\nimport\n phoenix \nas\n px\n\n\n\n\nsession \n=\n px\n.\nlaunch_app\n()\nThis will start a local Phoenix server. You can initialize the phoenix server with various kinds of data (traces, inferences). Check out the \nAPI for details\nWhen running Phoenix in the notebook, by default it does not persist your data.\nContainer\nContainer images are still actively being worked on. If you are interested in hosted phoenix, please get in touch!\nPhoenix server images are available via \nDocker Hub\n. The hosted phoenix server runs as a trace collector and can be used if you want observability for LLM traces via docker compose or simply want a long-running phoenix instance. Below are examples of how to run phoenix va Docker for a specific version.\nFirst pull the image you want to run (note you can use the tag \nlatest\n if you would just like the latest version)\nCopy\ndocker\n \npull\n \narizephoenix/phoenix:latest\nNow you can run the image you pulled (note you must expose the port \n6006\n so you can view the UI).\nCopy\ndocker\n \nrun\n \n-p\n \n6006:6006\n \narizephoenix/phoenix:latest\nThe Phoenix UI will be available at \nlocalhost:6006\n.\nIf you deploy the Phoenix server (collector) to a remote machine, you will have to make sure to configure the remote endpoint as the collector endpoint.\nSet Endpoint Environment Variable\nCopy\nimport\n os\n\n\n\n\nos\n.\nenviron\n[\n""PHOENIX_COLLECTOR_ENDPOINT""\n]\n \n=\n \n""http://123.456.789:6006""\nNote that the above is only necessary if your application is running in a Jupyter notebook. If you are trying to deploy your application and have phoenix collect traces via a container, please consult the \ndeployment guide.\nTerminal\nIf you want to start a phoenix server to collect traces, you can also run phoenix directly from the command line\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \nserve\nThis will start the phoenix server on port 6006. If you are running your instrumented notebook or application on the same machine, traces should automatically be exported to \nhttp://127.0.0.1:6006\n so no additional configuration is needed. However if the server is running remotely, you will have to modify the environment variable \nPHOENIX_COLLECTOR_ENDPOINT\n to point to that machine (e.g. \nhttp://<my-remote-machine>:<port>\n)\nNote that this command has various configuration options such as \n--host\n and \n--port\n. For example:\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \n--port\n \n1234\n \n--host\n \n0.0.0.0\n \nserve\nPrevious\nSetup\nNext\nConfiguration\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/quickstart\ntitle: Quickstart\n\nQuickstart\nThe easiest way to run Phoenix is locally in your own computer. To launch Phoenix, use the following steps.\nInstall\nUsing pip\nUsing conda\nDocker\napp.phoenix.arize.com\nCopy\npip\n \ninstall\n \narize-phoenix\nCopy\nconda\n \ninstall\n \n-c\n \nconda-forge\n \narize-phoenix\nPhoenix server images are available via \nDocker Hub\n and can be used via \ndocker compose \nor if you simply want a long-running phoenix instance to share with your team.\nCopy\ndocker\n \npull\n \narizephoenix/phoenix:latest\nIf you don\'t want to host an instance of Phoenix yourself or use a notebook instance, you can use a persistent instance provided on our site. Sign up for an Arize Phoenix account at \nhttps://app.phoenix.arize.com/login\nFor more details, see \nHosted Phoenix\nLaunch Phoenix\nLaunching phoenix can be done in many ways depending on your use-case.\nCommand Line\nDocker\nNotebook\napp.phoenix.arize.com\nLaunch your local Phoenix instance using:\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \nserve\nFor details on customizing a local terminal deployment, see \nTerminal Setup\n.\nLaunch the phoenix docker image using:\nCopy\ndocker\n \nrun\n \n-p\n \n6006:6006\n \n-p\n \n4317:4317\n \narizephoenix/phoenix:latest\nThis will expose the Phoenix UI and REST API on \nlocalhost:6006\n and exposes the gRPC endpoint for spans on \nlocalhost:4317\nFor more details on customizing a docker deployment, see \nDocker\nWithin your notebook, launch Phoenix using:\nCopy\nimport\n phoenix \nas\n px\n\n\npx\n.\nlaunch_app\n()\nBy default, notebook instances do not have persistent storage, so your traces will disappear after the notebook is closed. See \nPersistence\n or use one of the other deployment options to retain traces.\nHosted Phoenix instances are always online. Nothing more to do here!\nConnect your App\nTo collect traces from your application, you must configure an OpenTelemetry TracerProvider to send traces to Phoenix. The \nregister\n utility from the \nphoenix.otel\n module streamlines this process.\nPython\nTypeScript\nNotebook\napp.phoenix.arize.com\nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your instance using:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n  \nYou do not have to use phoenix.otel to connect to your phoenix instance, you can use OpenTelemetry itself to initialize  your OTEL connection. See\nUsing OTEL Python Directly\nSee \nSetup Tracing: Python\n for more details on configuration and setup\nFor setting up tracing and OpenTelemetry with TypeScript, see \nSetup Tracing: TS\nConnect your notebook to Phoenix:\nCopy\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# defaults to endpoint=""http://localhost:4317""\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""http://localhost:4317""\n,  \n# Sends traces using gRPC\n\n\n)\n \nIf \narize-phoenix\n is not installed in your python environment, you can use \narize-phoenix-otel\n to quickly connect to your phoenix instance.\nCopy\npip\n \ninstall\n \narize-phoenix-otel\nConnect your application to your cloud instance using:\nCopy\nimport\n os\n\n\nfrom\n phoenix\n.\notel \nimport\n register\n\n\n\n\n# Add Phoenix API Key for tracing\n\n\nos\n.\nenviron\n[\n""OTEL_EXPORTER_OTLP_HEADERS""\n]\n \n=\n f\n""api_key=\n{PHOENIX_API_KEY}\n""\n\n\n\n\n# configure the Phoenix tracer\n\n\nregister\n(\n\n\n  project_name\n=\n""my-llm-app""\n, \n# Default is \'default\'\n\n\n  endpoint\n=\n""https://app.phoenix.arize.com/v1/traces""\n,\n\n\n)\n \nYour Phoenix API key can be found on the Keys section of your dashboard.\nNext Steps\nTrace\n a running application\nRun \nevaluations\n on traces\nTest changes to you prompts, models, and application via \nexperiments\nPrevious\nArize Phoenix\nNext\nHosted Phoenix\nLast updated \n9 hours ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I setup Phoenix in the Terminal?\nAnswer: '}, {'role': 'assistant', 'content': 'To set up Phoenix in the Terminal, you can run the following command:\n```\npython3 -m phoenix.server.main serve\n```\nThis command will start the Phoenix server on port 6006. If your instrumented notebook or application is running on the same machine, traces will automatically be exported to http://127.0.0.1:6006 without needing additional configuration. If the server is running remotely, you will need to modify the environment variable PHOENIX_COLLECTOR_ENDPOINT to point to that machine.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6OTI=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/manual-instrumentation/javascript\ntitle: Instrument: TS\n\nInstrument: TS\nWhile Phoenix is heavily a Python-based Observability and Evaluation framework, it supports other languages like TypeScript / JavaScript\nYou can trace your NodeJS application over OpenTelemetry\nPhoenix is written and maintained in Python to make it natively runnable in Python notebooks. However, it can be stood up as a trace collector so that your LLM traces from your NodeJS application (e.g., LlamaIndex.TS, Langchain.js) can be collected. The traces collected by Phoenix can then be downloaded to a Jupyter notebook and used to run evaluations (e.g., \nLLM Evals\n, Ragas).\nGetting Started\nInstrumentation\n is the act of adding observability code to an app yourself.\nIf you’re instrumenting an app, you need to use the OpenTelemetry SDK for your language. You’ll then use the SDK to initialize OpenTelemetry and the API to instrument your code. This will emit telemetry from your app, and any library you installed that also comes with instrumentation.\nPhoenix natively supports automatic instrumentation provided by OpenInference. For more details on OpenInfernce, checkout the \nproject\ninstrumentation setup\nDependencies\nInstall OpenTelemetry API packages:\nCopy\nnpm\n \ninstall\n \n@opentelemetry/api\n \n@opentelemetry/resources\n \n@opentelemetry/semantic-conventions\nInstall OpenInference instrumentation packages. Below is an example of adding instrumentation for OpenAI as well as the semantic conventions for OpenInference.\nCopy\nnpm\n \ninstall\n \n@arizeai/openinference-instrumentation-openai\n \n@arizeai/openinference-semantic-conventions\nInitialize the SDK\nIf you instrument a Node.js application install the \nOpenTelemetry SDK for Node.js\n:\nCopy\nnpm\n \ninstall\n \n@opentelemetry/sdk-node\nBefore any other module in your application is loaded, you must initialize the SDK. If you fail to initialize the SDK or initialize it too late, no-op implementations will be provided to any library that acquires a tracer or meter from the API.\nCopy\n/*instrumentation.ts*/\n\n\nimport\n { NodeSDK } \nfrom\n \n'@opentelemetry/sdk-node'\n;\n\n\nimport\n { ConsoleSpanExporter } \nfrom\n \n'@opentelemetry/sdk-trace-node'\n;\n\n\nimport\n { Resource } \nfrom\n \n'@opentelemetry/resources'\n;\n\n\nimport\n { SemanticResourceAttributes } \nfrom\n \n'@opentelemetry/semantic-conventions'\n;\n\n\n\n\nconst\n \nsdk\n \n=\n \nnew\n \nNodeSDK\n({\n\n\n  resource\n:\n \nnew\n \nResource\n({\n\n\n    [\nSemanticResourceAttributes\n.\nSERVICE_NAME\n]\n:\n \n'yourServiceName'\n,\n\n\n    [\nSemanticResourceAttributes\n.\nSERVICE_VERSION\n]\n:\n \n'1.0'\n,\n\n\n  })\n,\n\n\n  traceExporter\n:\n \nnew\n \nConsoleSpanExporter\n()\n,\n\n\n});\n\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing\ntitle: Integrations: Tracing\n\nIntegrations: Tracing\nPhoenix natively works with a variety of frameworks and SDKs across Python and JavaScript via OpenTelemetry\n auto-instrumentation\n.\nPython\nLibrary\nInstrumentation Package\nopeninference-instrumentation-llama-index\nopeninference-instrumentation-langchain\nopeninference-instrumentation-openai\nMistralAI\n\n\nopeninference-instrumentation-mistralai\nVertexAI\n\n\nopeninference-instrumentation-vertexai\nopeninference-instrumentation-dspy\nopeninference-instrumentation-bedrock\nopeninference-instrumentation-guardrails\nopeninference-instrumentation-haystack\nopeninference-instrumentation-crewai\nJavascript\nLibrary\nInstrumentation\n@arizeai/openinference-instrumentation-openai\n@arizeai/openinference-instrumentation-langchain\nPrevious\nQuickstart: Tracing\nNext\nOpenAI\nLast updated \n16 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Does Phoenix support JavaScript?\nAnswer: ""}, {'role': 'assistant', 'content': 'Phoenix supports JavaScript, specifically TypeScript / JavaScript. It allows tracing of NodeJS applications over OpenTelemetry and can collect traces from NodeJS applications like LlamaIndex.TS and Langchain.js.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': ""Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/manual-instrumentation/javascript\ntitle: Instrument: TS\n\nInstrument: TS\nWhile Phoenix is heavily a Python-based Observability and Evaluation framework, it supports other languages like TypeScript / JavaScript\nYou can trace your NodeJS application over OpenTelemetry\nPhoenix is written and maintained in Python to make it natively runnable in Python notebooks. However, it can be stood up as a trace collector so that your LLM traces from your NodeJS application (e.g., LlamaIndex.TS, Langchain.js) can be collected. The traces collected by Phoenix can then be downloaded to a Jupyter notebook and used to run evaluations (e.g., \nLLM Evals\n, Ragas).\nGetting Started\nInstrumentation\n is the act of adding observability code to an app yourself.\nIf you’re instrumenting an app, you need to use the OpenTelemetry SDK for your language. You’ll then use the SDK to initialize OpenTelemetry and the API to instrument your code. This will emit telemetry from your app, and any library you installed that also comes with instrumentation.\nPhoenix natively supports automatic instrumentation provided by OpenInference. For more details on OpenInfernce, checkout the \nproject\ninstrumentation setup\nDependencies\nInstall OpenTelemetry API packages:\nCopy\nnpm\n \ninstall\n \n@opentelemetry/api\n \n@opentelemetry/resources\n \n@opentelemetry/semantic-conventions\nInstall OpenInference instrumentation packages. Below is an example of adding instrumentation for OpenAI as well as the semantic conventions for OpenInference.\nCopy\nnpm\n \ninstall\n \n@arizeai/openinference-instrumentation-openai\n \n@arizeai/openinference-semantic-conventions\nInitialize the SDK\nIf you instrument a Node.js application install the \nOpenTelemetry SDK for Node.js\n:\nCopy\nnpm\n \ninstall\n \n@opentelemetry/sdk-node\nBefore any other module in your application is loaded, you must initialize the SDK. If you fail to initialize the SDK or initialize it too late, no-op implementations will be provided to any library that acquires a tracer or meter from the API.\nCopy\n/*instrumentation.ts*/\n\n\nimport\n { NodeSDK } \nfrom\n \n'@opentelemetry/sdk-node'\n;\n\n\nimport\n { ConsoleSpanExporter } \nfrom\n \n'@opentelemetry/sdk-trace-node'\n;\n\n\nimport\n { Resource } \nfrom\n \n'@opentelemetry/resources'\n;\n\n\nimport\n { SemanticResourceAttributes } \nfrom\n \n'@opentelemetry/semantic-conventions'\n;\n\n\n\n\nconst\n \nsdk\n \n=\n \nnew\n \nNodeSDK\n({\n\n\n  resource\n:\n \nnew\n \nResource\n({\n\n\n    [\nSemanticResourceAttributes\n.\nSERVICE_NAME\n]\n:\n \n'yourServiceName'\n,\n\n\n    [\nSemanticResourceAttributes\n.\nSERVICE_VERSION\n]\n:\n \n'1.0'\n,\n\n\n  })\n,\n\n\n  traceExporter\n:\n \nnew\n \nConsoleSpanExporter\n()\n,\n\n\n});\n\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing\ntitle: Integrations: Tracing\n\nIntegrations: Tracing\nPhoenix natively works with a variety of frameworks and SDKs across Python and JavaScript via OpenTelemetry\n auto-instrumentation\n.\nPython\nLibrary\nInstrumentation Package\nopeninference-instrumentation-llama-index\nopeninference-instrumentation-langchain\nopeninference-instrumentation-openai\nMistralAI\n\n\nopeninference-instrumentation-mistralai\nVertexAI\n\n\nopeninference-instrumentation-vertexai\nopeninference-instrumentation-dspy\nopeninference-instrumentation-bedrock\nopeninference-instrumentation-guardrails\nopeninference-instrumentation-haystack\nopeninference-instrumentation-crewai\nJavascript\nLibrary\nInstrumentation\n@arizeai/openinference-instrumentation-openai\n@arizeai/openinference-instrumentation-langchain\nPrevious\nQuickstart: Tracing\nNext\nOpenAI\nLast updated \n16 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Does Phoenix support JavaScript?\nAnswer: ""}, {'role': 'assistant', 'content': 'Phoenix supports JavaScript, specifically TypeScript / JavaScript. It allows tracing of NodeJS applications over OpenTelemetry and can collect traces from NodeJS applications like LlamaIndex.TS and Langchain.js.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6OTM=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/instrumentation\ntitle: Auto Instrumentation\n\nAuto Instrumentation\nHow OpenInference facilitates automatic instrumentation of applications.\nIn order to make a system observable, it must be \ninstrumented\n: That is, code from the system’s components must emit traces.\nPhoenix natively supports collecting traces generated via OpenInference automatic instrumentation. Phoenix has pre-built auto-instrumentors for \nPython\n and \nTypeScript / JavaScript\n.\n\n\nPython\nLlamaIndex, LangChain, DSPy, OpenAI, Bedrock, Mistral and more\nTypeScript / JavaScript\nOpenAI, LangChainjs\nPrevious\nInstrumenting Prompt Templates and Prompt Variables\nNext\nAuto Instrument: Python\nLast updated \n4 months ago\n\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing\ntitle: Integrations: Tracing\n\nIntegrations: Tracing\nPhoenix natively works with a variety of frameworks and SDKs across Python and JavaScript via OpenTelemetry\n auto-instrumentation\n.\nPython\nLibrary\nInstrumentation Package\nopeninference-instrumentation-llama-index\nopeninference-instrumentation-langchain\nopeninference-instrumentation-openai\nMistralAI\n\n\nopeninference-instrumentation-mistralai\nVertexAI\n\n\nopeninference-instrumentation-vertexai\nopeninference-instrumentation-dspy\nopeninference-instrumentation-bedrock\nopeninference-instrumentation-guardrails\nopeninference-instrumentation-haystack\nopeninference-instrumentation-crewai\nJavascript\nLibrary\nInstrumentation\n@arizeai/openinference-instrumentation-openai\n@arizeai/openinference-instrumentation-langchain\nPrevious\nQuickstart: Tracing\nNext\nOpenAI\nLast updated \n16 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What coding languages does Phoenix support?\nAnswer: '}, {'role': 'assistant', 'content': 'Phoenix supports Python and TypeScript/JavaScript for auto-instrumentation.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/how-to-tracing/instrumentation\ntitle: Auto Instrumentation\n\nAuto Instrumentation\nHow OpenInference facilitates automatic instrumentation of applications.\nIn order to make a system observable, it must be \ninstrumented\n: That is, code from the system’s components must emit traces.\nPhoenix natively supports collecting traces generated via OpenInference automatic instrumentation. Phoenix has pre-built auto-instrumentors for \nPython\n and \nTypeScript / JavaScript\n.\n\n\nPython\nLlamaIndex, LangChain, DSPy, OpenAI, Bedrock, Mistral and more\nTypeScript / JavaScript\nOpenAI, LangChainjs\nPrevious\nInstrumenting Prompt Templates and Prompt Variables\nNext\nAuto Instrument: Python\nLast updated \n4 months ago\n\nsource: https://docs.arize.com/phoenix/tracing/integrations-tracing\ntitle: Integrations: Tracing\n\nIntegrations: Tracing\nPhoenix natively works with a variety of frameworks and SDKs across Python and JavaScript via OpenTelemetry\n auto-instrumentation\n.\nPython\nLibrary\nInstrumentation Package\nopeninference-instrumentation-llama-index\nopeninference-instrumentation-langchain\nopeninference-instrumentation-openai\nMistralAI\n\n\nopeninference-instrumentation-mistralai\nVertexAI\n\n\nopeninference-instrumentation-vertexai\nopeninference-instrumentation-dspy\nopeninference-instrumentation-bedrock\nopeninference-instrumentation-guardrails\nopeninference-instrumentation-haystack\nopeninference-instrumentation-crewai\nJavascript\nLibrary\nInstrumentation\n@arizeai/openinference-instrumentation-openai\n@arizeai/openinference-instrumentation-langchain\nPrevious\nQuickstart: Tracing\nNext\nOpenAI\nLast updated \n16 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What coding languages does Phoenix support?\nAnswer: '}, {'role': 'assistant', 'content': 'Phoenix supports Python and TypeScript/JavaScript for auto-instrumentation.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6OTQ=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/setup/environments\ntitle: Environments\n\nEnvironments\nThe Phoenix app can be run in various notebook environments such as colab and SageMaker as well as be served via the terminal or a docker container\nIn the notebook\nRun phoenix in the notebook as you run experiments\nAs a Container\nStart a long-running collector\nFrom the Terminal\nRun phoenix via the CLI\nNotebooks\nTo start phoenix in the notebook environment, run:\nCopy\nimport\n phoenix \nas\n px\n\n\n\n\nsession \n=\n px\n.\nlaunch_app\n()\nThis will start a local Phoenix server. You can initialize the phoenix server with various kinds of data (traces, inferences). Check out the \nAPI for details\nWhen running Phoenix in the notebook, by default it does not persist your data.\nContainer\nContainer images are still actively being worked on. If you are interested in hosted phoenix, please get in touch!\nPhoenix server images are available via \nDocker Hub\n. The hosted phoenix server runs as a trace collector and can be used if you want observability for LLM traces via docker compose or simply want a long-running phoenix instance. Below are examples of how to run phoenix va Docker for a specific version.\nFirst pull the image you want to run (note you can use the tag \nlatest\n if you would just like the latest version)\nCopy\ndocker\n \npull\n \narizephoenix/phoenix:latest\nNow you can run the image you pulled (note you must expose the port \n6006\n so you can view the UI).\nCopy\ndocker\n \nrun\n \n-p\n \n6006:6006\n \narizephoenix/phoenix:latest\nThe Phoenix UI will be available at \nlocalhost:6006\n.\nIf you deploy the Phoenix server (collector) to a remote machine, you will have to make sure to configure the remote endpoint as the collector endpoint.\nSet Endpoint Environment Variable\nCopy\nimport\n os\n\n\n\n\nos\n.\nenviron\n[\n""PHOENIX_COLLECTOR_ENDPOINT""\n]\n \n=\n \n""http://123.456.789:6006""\nNote that the above is only necessary if your application is running in a Jupyter notebook. If you are trying to deploy your application and have phoenix collect traces via a container, please consult the \ndeployment guide.\nTerminal\nIf you want to start a phoenix server to collect traces, you can also run phoenix directly from the command line\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \nserve\nThis will start the phoenix server on port 6006. If you are running your instrumented notebook or application on the same machine, traces should automatically be exported to \nhttp://127.0.0.1:6006\n so no additional configuration is needed. However if the server is running remotely, you will have to modify the environment variable \nPHOENIX_COLLECTOR_ENDPOINT\n to point to that machine (e.g. \nhttp://<my-remote-machine>:<port>\n)\nNote that this command has various configuration options such as \n--host\n and \n--port\n. For example:\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \n--port\n \n1234\n \n--host\n \n0.0.0.0\n \nserve\nPrevious\nSetup\nNext\nConfiguration\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/setup\ntitle: Setup\n\nSetup\nHow to get started with Phoenix\nHow to run in different environments\nHosted Phoenix by Arize\nIn a python notebook\nFrom the command line\nIn a Container\nHow to configure phoenix\nAvailable ports to run phoenix\nCustomize phoenix using environment variables\nPrevious\nExamples\nNext\nEnvironments\nLast updated \n8 hours ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I setup Phoenix as a Container?\nAnswer: '}, {'role': 'assistant', 'content': 'To set up Phoenix as a Container, you can start a long-running collector by pulling the Phoenix server image from Docker Hub and then running the image you pulled. Make sure to expose port 6006 so you can view the Phoenix UI at localhost:6006. If you are interested in hosted Phoenix, you can reach out for more information.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/setup/environments\ntitle: Environments\n\nEnvironments\nThe Phoenix app can be run in various notebook environments such as colab and SageMaker as well as be served via the terminal or a docker container\nIn the notebook\nRun phoenix in the notebook as you run experiments\nAs a Container\nStart a long-running collector\nFrom the Terminal\nRun phoenix via the CLI\nNotebooks\nTo start phoenix in the notebook environment, run:\nCopy\nimport\n phoenix \nas\n px\n\n\n\n\nsession \n=\n px\n.\nlaunch_app\n()\nThis will start a local Phoenix server. You can initialize the phoenix server with various kinds of data (traces, inferences). Check out the \nAPI for details\nWhen running Phoenix in the notebook, by default it does not persist your data.\nContainer\nContainer images are still actively being worked on. If you are interested in hosted phoenix, please get in touch!\nPhoenix server images are available via \nDocker Hub\n. The hosted phoenix server runs as a trace collector and can be used if you want observability for LLM traces via docker compose or simply want a long-running phoenix instance. Below are examples of how to run phoenix va Docker for a specific version.\nFirst pull the image you want to run (note you can use the tag \nlatest\n if you would just like the latest version)\nCopy\ndocker\n \npull\n \narizephoenix/phoenix:latest\nNow you can run the image you pulled (note you must expose the port \n6006\n so you can view the UI).\nCopy\ndocker\n \nrun\n \n-p\n \n6006:6006\n \narizephoenix/phoenix:latest\nThe Phoenix UI will be available at \nlocalhost:6006\n.\nIf you deploy the Phoenix server (collector) to a remote machine, you will have to make sure to configure the remote endpoint as the collector endpoint.\nSet Endpoint Environment Variable\nCopy\nimport\n os\n\n\n\n\nos\n.\nenviron\n[\n""PHOENIX_COLLECTOR_ENDPOINT""\n]\n \n=\n \n""http://123.456.789:6006""\nNote that the above is only necessary if your application is running in a Jupyter notebook. If you are trying to deploy your application and have phoenix collect traces via a container, please consult the \ndeployment guide.\nTerminal\nIf you want to start a phoenix server to collect traces, you can also run phoenix directly from the command line\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \nserve\nThis will start the phoenix server on port 6006. If you are running your instrumented notebook or application on the same machine, traces should automatically be exported to \nhttp://127.0.0.1:6006\n so no additional configuration is needed. However if the server is running remotely, you will have to modify the environment variable \nPHOENIX_COLLECTOR_ENDPOINT\n to point to that machine (e.g. \nhttp://<my-remote-machine>:<port>\n)\nNote that this command has various configuration options such as \n--host\n and \n--port\n. For example:\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \n--port\n \n1234\n \n--host\n \n0.0.0.0\n \nserve\nPrevious\nSetup\nNext\nConfiguration\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/setup\ntitle: Setup\n\nSetup\nHow to get started with Phoenix\nHow to run in different environments\nHosted Phoenix by Arize\nIn a python notebook\nFrom the command line\nIn a Container\nHow to configure phoenix\nAvailable ports to run phoenix\nCustomize phoenix using environment variables\nPrevious\nExamples\nNext\nEnvironments\nLast updated \n8 hours ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I setup Phoenix as a Container?\nAnswer: '}, {'role': 'assistant', 'content': 'To set up Phoenix as a Container, you can start a long-running collector by pulling the Phoenix server image from Docker Hub and then running the image you pulled. Make sure to expose port 6006 so you can view the Phoenix UI at localhost:6006. If you are interested in hosted Phoenix, you can reach out for more information.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6OTU=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/setup/environments\ntitle: Environments\n\nEnvironments\nThe Phoenix app can be run in various notebook environments such as colab and SageMaker as well as be served via the terminal or a docker container\nIn the notebook\nRun phoenix in the notebook as you run experiments\nAs a Container\nStart a long-running collector\nFrom the Terminal\nRun phoenix via the CLI\nNotebooks\nTo start phoenix in the notebook environment, run:\nCopy\nimport\n phoenix \nas\n px\n\n\n\n\nsession \n=\n px\n.\nlaunch_app\n()\nThis will start a local Phoenix server. You can initialize the phoenix server with various kinds of data (traces, inferences). Check out the \nAPI for details\nWhen running Phoenix in the notebook, by default it does not persist your data.\nContainer\nContainer images are still actively being worked on. If you are interested in hosted phoenix, please get in touch!\nPhoenix server images are available via \nDocker Hub\n. The hosted phoenix server runs as a trace collector and can be used if you want observability for LLM traces via docker compose or simply want a long-running phoenix instance. Below are examples of how to run phoenix va Docker for a specific version.\nFirst pull the image you want to run (note you can use the tag \nlatest\n if you would just like the latest version)\nCopy\ndocker\n \npull\n \narizephoenix/phoenix:latest\nNow you can run the image you pulled (note you must expose the port \n6006\n so you can view the UI).\nCopy\ndocker\n \nrun\n \n-p\n \n6006:6006\n \narizephoenix/phoenix:latest\nThe Phoenix UI will be available at \nlocalhost:6006\n.\nIf you deploy the Phoenix server (collector) to a remote machine, you will have to make sure to configure the remote endpoint as the collector endpoint.\nSet Endpoint Environment Variable\nCopy\nimport\n os\n\n\n\n\nos\n.\nenviron\n[\n""PHOENIX_COLLECTOR_ENDPOINT""\n]\n \n=\n \n""http://123.456.789:6006""\nNote that the above is only necessary if your application is running in a Jupyter notebook. If you are trying to deploy your application and have phoenix collect traces via a container, please consult the \ndeployment guide.\nTerminal\nIf you want to start a phoenix server to collect traces, you can also run phoenix directly from the command line\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \nserve\nThis will start the phoenix server on port 6006. If you are running your instrumented notebook or application on the same machine, traces should automatically be exported to \nhttp://127.0.0.1:6006\n so no additional configuration is needed. However if the server is running remotely, you will have to modify the environment variable \nPHOENIX_COLLECTOR_ENDPOINT\n to point to that machine (e.g. \nhttp://<my-remote-machine>:<port>\n)\nNote that this command has various configuration options such as \n--host\n and \n--port\n. For example:\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \n--port\n \n1234\n \n--host\n \n0.0.0.0\n \nserve\nPrevious\nSetup\nNext\nConfiguration\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/setup\ntitle: Setup\n\nSetup\nHow to get started with Phoenix\nHow to run in different environments\nHosted Phoenix by Arize\nIn a python notebook\nFrom the command line\nIn a Container\nHow to configure phoenix\nAvailable ports to run phoenix\nCustomize phoenix using environment variables\nPrevious\nExamples\nNext\nEnvironments\nLast updated \n8 hours ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I setup Phoenix in a Notebook?\nAnswer: '}, {'role': 'assistant', 'content': 'To set up Phoenix in a Notebook, you can run the following code snippet:\n\n```python\nimport phoenix as px\n\nsession = px.launch_app()\n```\n\nThis code will initiate a local Phoenix server in the Notebook environment.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/setup/environments\ntitle: Environments\n\nEnvironments\nThe Phoenix app can be run in various notebook environments such as colab and SageMaker as well as be served via the terminal or a docker container\nIn the notebook\nRun phoenix in the notebook as you run experiments\nAs a Container\nStart a long-running collector\nFrom the Terminal\nRun phoenix via the CLI\nNotebooks\nTo start phoenix in the notebook environment, run:\nCopy\nimport\n phoenix \nas\n px\n\n\n\n\nsession \n=\n px\n.\nlaunch_app\n()\nThis will start a local Phoenix server. You can initialize the phoenix server with various kinds of data (traces, inferences). Check out the \nAPI for details\nWhen running Phoenix in the notebook, by default it does not persist your data.\nContainer\nContainer images are still actively being worked on. If you are interested in hosted phoenix, please get in touch!\nPhoenix server images are available via \nDocker Hub\n. The hosted phoenix server runs as a trace collector and can be used if you want observability for LLM traces via docker compose or simply want a long-running phoenix instance. Below are examples of how to run phoenix va Docker for a specific version.\nFirst pull the image you want to run (note you can use the tag \nlatest\n if you would just like the latest version)\nCopy\ndocker\n \npull\n \narizephoenix/phoenix:latest\nNow you can run the image you pulled (note you must expose the port \n6006\n so you can view the UI).\nCopy\ndocker\n \nrun\n \n-p\n \n6006:6006\n \narizephoenix/phoenix:latest\nThe Phoenix UI will be available at \nlocalhost:6006\n.\nIf you deploy the Phoenix server (collector) to a remote machine, you will have to make sure to configure the remote endpoint as the collector endpoint.\nSet Endpoint Environment Variable\nCopy\nimport\n os\n\n\n\n\nos\n.\nenviron\n[\n""PHOENIX_COLLECTOR_ENDPOINT""\n]\n \n=\n \n""http://123.456.789:6006""\nNote that the above is only necessary if your application is running in a Jupyter notebook. If you are trying to deploy your application and have phoenix collect traces via a container, please consult the \ndeployment guide.\nTerminal\nIf you want to start a phoenix server to collect traces, you can also run phoenix directly from the command line\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \nserve\nThis will start the phoenix server on port 6006. If you are running your instrumented notebook or application on the same machine, traces should automatically be exported to \nhttp://127.0.0.1:6006\n so no additional configuration is needed. However if the server is running remotely, you will have to modify the environment variable \nPHOENIX_COLLECTOR_ENDPOINT\n to point to that machine (e.g. \nhttp://<my-remote-machine>:<port>\n)\nNote that this command has various configuration options such as \n--host\n and \n--port\n. For example:\nCopy\npython3\n \n-m\n \nphoenix.server.main\n \n--port\n \n1234\n \n--host\n \n0.0.0.0\n \nserve\nPrevious\nSetup\nNext\nConfiguration\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/setup\ntitle: Setup\n\nSetup\nHow to get started with Phoenix\nHow to run in different environments\nHosted Phoenix by Arize\nIn a python notebook\nFrom the command line\nIn a Container\nHow to configure phoenix\nAvailable ports to run phoenix\nCustomize phoenix using environment variables\nPrevious\nExamples\nNext\nEnvironments\nLast updated \n8 hours ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How do I setup Phoenix in a Notebook?\nAnswer: '}, {'role': 'assistant', 'content': 'To set up Phoenix in a Notebook, you can run the following code snippet:\n\n```python\nimport phoenix as px\n\nsession = px.launch_app()\n```\n\nThis code will initiate a local Phoenix server in the Notebook environment.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6OTY=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/reference/frequently-asked-questions\ntitle: Frequently Asked Questions\n\nos\n.\nenviron\n[\n""PHOENIX_NOTEBOOK_ENV""\n]\n \n=\n \n""sagemaker""\nCan I persistdata in the notbook?\nYou can persist data in the notebook by either setting the \nuse_temp_dir\n flag to false in \npx.launch_app\n which will persit your data in SQLite on your disk at the \nPHOENIX_WORKING_DIR\n. Alternatively you can deploy a phoenix instance and point to it via \nPHOENIX_COLLECTOR_ENDPOINT\n.\nCan I use gRPC for trace collection?\nPhoenix does natively support gRPC for trace collection post 4.0 release. See \nHow to configure phoenix\n for details.\nPrevious\nRagas\nNext\nOpenInference\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/use-cases-datasets/email-extraction\ntitle: Email Extraction\n\nEmail Extraction\nComing soon\nPrevious\nSummarization\nNext\nOverview: Retrieval\nLast updated \n1 month ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the Data retention policy?\nAnswer: '}, {'role': 'assistant', 'content': 'The data retention policy allows users to persist data in the notebook by setting the use_temp_dir flag to false in px.launch_app, which will store the data in SQLite on the disk at the PHOENIX_WORKING_DIR. Alternatively, users can deploy a Phoenix instance and point to it via PHOENIX_COLLECTOR_ENDPOINT.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/reference/frequently-asked-questions\ntitle: Frequently Asked Questions\n\nos\n.\nenviron\n[\n""PHOENIX_NOTEBOOK_ENV""\n]\n \n=\n \n""sagemaker""\nCan I persistdata in the notbook?\nYou can persist data in the notebook by either setting the \nuse_temp_dir\n flag to false in \npx.launch_app\n which will persit your data in SQLite on your disk at the \nPHOENIX_WORKING_DIR\n. Alternatively you can deploy a phoenix instance and point to it via \nPHOENIX_COLLECTOR_ENDPOINT\n.\nCan I use gRPC for trace collection?\nPhoenix does natively support gRPC for trace collection post 4.0 release. See \nHow to configure phoenix\n for details.\nPrevious\nRagas\nNext\nOpenInference\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/datasets-and-experiments/use-cases-datasets/email-extraction\ntitle: Email Extraction\n\nEmail Extraction\nComing soon\nPrevious\nSummarization\nNext\nOverview: Retrieval\nLast updated \n1 month ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the Data retention policy?\nAnswer: '}, {'role': 'assistant', 'content': 'The data retention policy allows users to persist data in the notebook by setting the use_temp_dir flag to false in px.launch_app, which will store the data in SQLite on the disk at the PHOENIX_WORKING_DIR. Alternatively, users can deploy a Phoenix instance and point to it via PHOENIX_COLLECTOR_ENDPOINT.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6OTc=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/hosted-phoenix\ntitle: Hosted Phoenix\n\nPHOENIX_API_KEY \n=\n os\n.\nenviron\n[\n""PHOENIX_API_KEY""\n]\n\n\nregister_otel\n(\n\n\n    endpoints\n=\n[Endpoints.HOSTED_PHOENIX],\n\n\n    api_key\n=\nPHOENIX_API_KEY\n\n\n)\n\n\n\n\n# Turn on instrumentation for OpenAI\n\n\nOpenAIInstrumentor\n().\ninstrument\n()\nCheckout our colab tutorial here:\nGoogle Colab\nUsing the Client SDK (downloading data & uploading datasets)\nOnce you collect trace data from the above configuration, you can access the data using the client SDK. You can also upload datasets for experiments using the client SDK.\nYou\'ll need to add the following environment variable to authenticate to hosted Phoenix.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n \n""api_key=...""\nHere\'s more sample code.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n f\n""api_key=...""\n\n\nos\n.\nenviron\n[\n""PHOENIX_COLLECTOR_ENDPOINT""\n]\n \n=\n \n""https://app.phoenix.arize.com""\n\n\n\n\nimport\n phoenix \nas\n px\n\n\n\n\npx_client \n=\n px\n.\nClient\n()\n\n\nphoenix_df \n=\n px_client\n.\nget_spans_dataframe\n()\nFAQ\nWill hosted Phoenix be on the latest version of Phoenix?\nOn account creation, we will always use the latest version of Phoenix. We try to keep all instances of hosted Phoenix up to date and run upgrades for them when new versions are available. There will be a few minutes of downtime during these periods.\nData retention\nWe have a 30 day data retention policy. We are working on plans to offer a longer data retention period.\nSharing\nCurrently accounts are setup to be used specifically for one developer. We will be adding ways to share your traces with other developers on your team shortly!\nPricing\nHosted Phoenix is free for all developers. We will add a paid tier in the future which increases your data retention and also give you access to more storage.\nAre there other demos available?\nYes. This demo and \naccompanying blog\n show how to deploy Phoenix via Docker/Kubernetes:\nPrevious\nQuickstart\nNext\nUser Guide\nLast updated \n7 days ago\n\nsource: https://docs.arize.com/phoenix/hosted-phoenix\ntitle: Hosted Phoenix\n\nHosted Phoenix\nWe now offer a hosted version of Phoenix to make it easier for developers to use Phoenix to trace their LLM applications and avoid setting up infrastructure. You can use our Colab links to follow along.\nFramework\nLlamaindex\nColab\nLlamaindex with Llamacloud\nColab\nOpenAI\nColab\nThe main differences for Hosted Phoenix:\nHosted Phoenix runs the latest version of our open source package and gates access to your data behind API keys and user authentication.\nYou must create an account\nYou need to add an API key as an environment variable during tracing\nYou need to add an API key as an environment variable when using the Client SDK\nWe also use 3rd party analytics tools to measure usage of our application to improve our services.\nHow to create an account\nClick signup on \nphoenix.arize.com\n. We offer logins via Google, Github, and email. This account will use the same account credentials as your Arize account if you have one.\nTracing: How to send in your first trace\nGet your API keys from your Phoenix application on the left hand side. \nHere\'s the full sample code for LlamaIndex and OpenAI instrumentation. You can see all of our automatic tracing options \nhere\n.\nLlamaIndex\nOpenAI\nInstall the following libraries\nCopy\n!pip install opentelemetry-sdk opentelemetry-exporter-otlp\n\n\n!pip install ""arize-phoenix[evals,llama-index]"" ""openai>=1"" gcsfs nest-asyncio ""openinference-instrumentation-llama-index>=2.0.0""\nUse the following python code to start instrumentation.\nCopy\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\n\n\n# Setup authentication and endpoint\n\n\nos\n.\nenviron\n[\n""OTEL_EXPORTER_OTLP_HEADERS""\n]\n \n=\n f\n""api_key=\n{PHOENIX_API_KEY}\n""\n\n\nendpoint \n=\n \n""https://app.phoenix.arize.com/v1/traces""\n\n\n\n\n# Setup tracing with OpenTelemetry\n\n\nspan_phoenix_processor \n=\n \nSimpleSpanProcessor\n(\nOTLPSpanExporter\n(endpoint\n=\nendpoint))\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n()\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor\n=\nspan_phoenix_processor)\n\n\n\n\n# Start instrumentation\n\n\nLlamaIndexInstrumentor\n().\ninstrument\n(tracer_provider\n=\ntracer_provider, skip_dep_check\n=\nTrue\n)\nCheckout our colab tutorial here:\nInstall the following libraries\nCopy\npip\n \ninstall\n \narize-otel\n \nopeninference-instrumentation-openai\n \nopenai\nThen, use our library \narize-otel\n, which sets up OpenTelemetry tracing with Hosted Phoenix. Run the following code to start instrumentation.\nCopy\nimport\n os\n\n\nfrom\n arize_otel \nimport\n register_otel\n,\n Endpoints\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nopenai \nimport\n OpenAIInstrumentor\n\n\n\n\n# Setup OTEL tracing for hosted Phoenix\n\n\n# Endpoints.HOSTED_PHOENIX = ""https://app.phoenix.arize.com""\n\n\nPHOENIX_API_KEY \n=\n os\n.\nenviron\n[\n""PHOENIX_API_KEY""\n]\n\n\nregister_otel\n(\n\n\n    endpoints\n=\n[Endpoints.HOSTED_PHOENIX],\n\n\n    api_key\n=\nPHOENIX_API_KEY\n\n\n)\n\n\n\n\n# Turn on instrumentation for OpenAI\n\n\nOpenAIInstrumentor\n().\ninstrument\n()\nCheckout our colab tutorial here:\nGoogle Colab\nUsing the Client SDK (downloading data & uploading datasets)\nOnce you collect trace data from the above configuration, you can access the data using the client SDK. You can also upload datasets for experiments using the client SDK.\nYou\'ll need to add the following environment variable to authenticate to hosted Phoenix.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n \n""api_key=...""\nHere\'s more sample code.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n f\n""api_key=...""\n\n\nos\n.\nenviron\n[\n""PHOENIX_COLLECTOR_ENDPOINT""\n]\n \n=\n \n""https://app.phoenix.arize.com""\n\n\n\n\nimport\n phoenix \nas\n px\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Will hosted Phoenix be on the latest version of Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Hosted Phoenix will always be on the latest version of Phoenix.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/hosted-phoenix\ntitle: Hosted Phoenix\n\nPHOENIX_API_KEY \n=\n os\n.\nenviron\n[\n""PHOENIX_API_KEY""\n]\n\n\nregister_otel\n(\n\n\n    endpoints\n=\n[Endpoints.HOSTED_PHOENIX],\n\n\n    api_key\n=\nPHOENIX_API_KEY\n\n\n)\n\n\n\n\n# Turn on instrumentation for OpenAI\n\n\nOpenAIInstrumentor\n().\ninstrument\n()\nCheckout our colab tutorial here:\nGoogle Colab\nUsing the Client SDK (downloading data & uploading datasets)\nOnce you collect trace data from the above configuration, you can access the data using the client SDK. You can also upload datasets for experiments using the client SDK.\nYou\'ll need to add the following environment variable to authenticate to hosted Phoenix.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n \n""api_key=...""\nHere\'s more sample code.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n f\n""api_key=...""\n\n\nos\n.\nenviron\n[\n""PHOENIX_COLLECTOR_ENDPOINT""\n]\n \n=\n \n""https://app.phoenix.arize.com""\n\n\n\n\nimport\n phoenix \nas\n px\n\n\n\n\npx_client \n=\n px\n.\nClient\n()\n\n\nphoenix_df \n=\n px_client\n.\nget_spans_dataframe\n()\nFAQ\nWill hosted Phoenix be on the latest version of Phoenix?\nOn account creation, we will always use the latest version of Phoenix. We try to keep all instances of hosted Phoenix up to date and run upgrades for them when new versions are available. There will be a few minutes of downtime during these periods.\nData retention\nWe have a 30 day data retention policy. We are working on plans to offer a longer data retention period.\nSharing\nCurrently accounts are setup to be used specifically for one developer. We will be adding ways to share your traces with other developers on your team shortly!\nPricing\nHosted Phoenix is free for all developers. We will add a paid tier in the future which increases your data retention and also give you access to more storage.\nAre there other demos available?\nYes. This demo and \naccompanying blog\n show how to deploy Phoenix via Docker/Kubernetes:\nPrevious\nQuickstart\nNext\nUser Guide\nLast updated \n7 days ago\n\nsource: https://docs.arize.com/phoenix/hosted-phoenix\ntitle: Hosted Phoenix\n\nHosted Phoenix\nWe now offer a hosted version of Phoenix to make it easier for developers to use Phoenix to trace their LLM applications and avoid setting up infrastructure. You can use our Colab links to follow along.\nFramework\nLlamaindex\nColab\nLlamaindex with Llamacloud\nColab\nOpenAI\nColab\nThe main differences for Hosted Phoenix:\nHosted Phoenix runs the latest version of our open source package and gates access to your data behind API keys and user authentication.\nYou must create an account\nYou need to add an API key as an environment variable during tracing\nYou need to add an API key as an environment variable when using the Client SDK\nWe also use 3rd party analytics tools to measure usage of our application to improve our services.\nHow to create an account\nClick signup on \nphoenix.arize.com\n. We offer logins via Google, Github, and email. This account will use the same account credentials as your Arize account if you have one.\nTracing: How to send in your first trace\nGet your API keys from your Phoenix application on the left hand side. \nHere\'s the full sample code for LlamaIndex and OpenAI instrumentation. You can see all of our automatic tracing options \nhere\n.\nLlamaIndex\nOpenAI\nInstall the following libraries\nCopy\n!pip install opentelemetry-sdk opentelemetry-exporter-otlp\n\n\n!pip install ""arize-phoenix[evals,llama-index]"" ""openai>=1"" gcsfs nest-asyncio ""openinference-instrumentation-llama-index>=2.0.0""\nUse the following python code to start instrumentation.\nCopy\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\n\n\n# Setup authentication and endpoint\n\n\nos\n.\nenviron\n[\n""OTEL_EXPORTER_OTLP_HEADERS""\n]\n \n=\n f\n""api_key=\n{PHOENIX_API_KEY}\n""\n\n\nendpoint \n=\n \n""https://app.phoenix.arize.com/v1/traces""\n\n\n\n\n# Setup tracing with OpenTelemetry\n\n\nspan_phoenix_processor \n=\n \nSimpleSpanProcessor\n(\nOTLPSpanExporter\n(endpoint\n=\nendpoint))\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n()\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor\n=\nspan_phoenix_processor)\n\n\n\n\n# Start instrumentation\n\n\nLlamaIndexInstrumentor\n().\ninstrument\n(tracer_provider\n=\ntracer_provider, skip_dep_check\n=\nTrue\n)\nCheckout our colab tutorial here:\nInstall the following libraries\nCopy\npip\n \ninstall\n \narize-otel\n \nopeninference-instrumentation-openai\n \nopenai\nThen, use our library \narize-otel\n, which sets up OpenTelemetry tracing with Hosted Phoenix. Run the following code to start instrumentation.\nCopy\nimport\n os\n\n\nfrom\n arize_otel \nimport\n register_otel\n,\n Endpoints\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nopenai \nimport\n OpenAIInstrumentor\n\n\n\n\n# Setup OTEL tracing for hosted Phoenix\n\n\n# Endpoints.HOSTED_PHOENIX = ""https://app.phoenix.arize.com""\n\n\nPHOENIX_API_KEY \n=\n os\n.\nenviron\n[\n""PHOENIX_API_KEY""\n]\n\n\nregister_otel\n(\n\n\n    endpoints\n=\n[Endpoints.HOSTED_PHOENIX],\n\n\n    api_key\n=\nPHOENIX_API_KEY\n\n\n)\n\n\n\n\n# Turn on instrumentation for OpenAI\n\n\nOpenAIInstrumentor\n().\ninstrument\n()\nCheckout our colab tutorial here:\nGoogle Colab\nUsing the Client SDK (downloading data & uploading datasets)\nOnce you collect trace data from the above configuration, you can access the data using the client SDK. You can also upload datasets for experiments using the client SDK.\nYou\'ll need to add the following environment variable to authenticate to hosted Phoenix.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n \n""api_key=...""\nHere\'s more sample code.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n f\n""api_key=...""\n\n\nos\n.\nenviron\n[\n""PHOENIX_COLLECTOR_ENDPOINT""\n]\n \n=\n \n""https://app.phoenix.arize.com""\n\n\n\n\nimport\n phoenix \nas\n px\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Will hosted Phoenix be on the latest version of Phoenix?\nAnswer: '}, {'role': 'assistant', 'content': 'Hosted Phoenix will always be on the latest version of Phoenix.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6OTg=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/hosted-phoenix\ntitle: Hosted Phoenix\n\nPHOENIX_API_KEY \n=\n os\n.\nenviron\n[\n""PHOENIX_API_KEY""\n]\n\n\nregister_otel\n(\n\n\n    endpoints\n=\n[Endpoints.HOSTED_PHOENIX],\n\n\n    api_key\n=\nPHOENIX_API_KEY\n\n\n)\n\n\n\n\n# Turn on instrumentation for OpenAI\n\n\nOpenAIInstrumentor\n().\ninstrument\n()\nCheckout our colab tutorial here:\nGoogle Colab\nUsing the Client SDK (downloading data & uploading datasets)\nOnce you collect trace data from the above configuration, you can access the data using the client SDK. You can also upload datasets for experiments using the client SDK.\nYou\'ll need to add the following environment variable to authenticate to hosted Phoenix.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n \n""api_key=...""\nHere\'s more sample code.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n f\n""api_key=...""\n\n\nos\n.\nenviron\n[\n""PHOENIX_COLLECTOR_ENDPOINT""\n]\n \n=\n \n""https://app.phoenix.arize.com""\n\n\n\n\nimport\n phoenix \nas\n px\n\n\n\n\npx_client \n=\n px\n.\nClient\n()\n\n\nphoenix_df \n=\n px_client\n.\nget_spans_dataframe\n()\nFAQ\nWill hosted Phoenix be on the latest version of Phoenix?\nOn account creation, we will always use the latest version of Phoenix. We try to keep all instances of hosted Phoenix up to date and run upgrades for them when new versions are available. There will be a few minutes of downtime during these periods.\nData retention\nWe have a 30 day data retention policy. We are working on plans to offer a longer data retention period.\nSharing\nCurrently accounts are setup to be used specifically for one developer. We will be adding ways to share your traces with other developers on your team shortly!\nPricing\nHosted Phoenix is free for all developers. We will add a paid tier in the future which increases your data retention and also give you access to more storage.\nAre there other demos available?\nYes. This demo and \naccompanying blog\n show how to deploy Phoenix via Docker/Kubernetes:\nPrevious\nQuickstart\nNext\nUser Guide\nLast updated \n7 days ago\n\nsource: https://docs.arize.com/phoenix/hosted-phoenix\ntitle: Hosted Phoenix\n\nHosted Phoenix\nWe now offer a hosted version of Phoenix to make it easier for developers to use Phoenix to trace their LLM applications and avoid setting up infrastructure. You can use our Colab links to follow along.\nFramework\nLlamaindex\nColab\nLlamaindex with Llamacloud\nColab\nOpenAI\nColab\nThe main differences for Hosted Phoenix:\nHosted Phoenix runs the latest version of our open source package and gates access to your data behind API keys and user authentication.\nYou must create an account\nYou need to add an API key as an environment variable during tracing\nYou need to add an API key as an environment variable when using the Client SDK\nWe also use 3rd party analytics tools to measure usage of our application to improve our services.\nHow to create an account\nClick signup on \nphoenix.arize.com\n. We offer logins via Google, Github, and email. This account will use the same account credentials as your Arize account if you have one.\nTracing: How to send in your first trace\nGet your API keys from your Phoenix application on the left hand side. \nHere\'s the full sample code for LlamaIndex and OpenAI instrumentation. You can see all of our automatic tracing options \nhere\n.\nLlamaIndex\nOpenAI\nInstall the following libraries\nCopy\n!pip install opentelemetry-sdk opentelemetry-exporter-otlp\n\n\n!pip install ""arize-phoenix[evals,llama-index]"" ""openai>=1"" gcsfs nest-asyncio ""openinference-instrumentation-llama-index>=2.0.0""\nUse the following python code to start instrumentation.\nCopy\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\n\n\n# Setup authentication and endpoint\n\n\nos\n.\nenviron\n[\n""OTEL_EXPORTER_OTLP_HEADERS""\n]\n \n=\n f\n""api_key=\n{PHOENIX_API_KEY}\n""\n\n\nendpoint \n=\n \n""https://app.phoenix.arize.com/v1/traces""\n\n\n\n\n# Setup tracing with OpenTelemetry\n\n\nspan_phoenix_processor \n=\n \nSimpleSpanProcessor\n(\nOTLPSpanExporter\n(endpoint\n=\nendpoint))\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n()\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor\n=\nspan_phoenix_processor)\n\n\n\n\n# Start instrumentation\n\n\nLlamaIndexInstrumentor\n().\ninstrument\n(tracer_provider\n=\ntracer_provider, skip_dep_check\n=\nTrue\n)\nCheckout our colab tutorial here:\nInstall the following libraries\nCopy\npip\n \ninstall\n \narize-otel\n \nopeninference-instrumentation-openai\n \nopenai\nThen, use our library \narize-otel\n, which sets up OpenTelemetry tracing with Hosted Phoenix. Run the following code to start instrumentation.\nCopy\nimport\n os\n\n\nfrom\n arize_otel \nimport\n register_otel\n,\n Endpoints\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nopenai \nimport\n OpenAIInstrumentor\n\n\n\n\n# Setup OTEL tracing for hosted Phoenix\n\n\n# Endpoints.HOSTED_PHOENIX = ""https://app.phoenix.arize.com""\n\n\nPHOENIX_API_KEY \n=\n os\n.\nenviron\n[\n""PHOENIX_API_KEY""\n]\n\n\nregister_otel\n(\n\n\n    endpoints\n=\n[Endpoints.HOSTED_PHOENIX],\n\n\n    api_key\n=\nPHOENIX_API_KEY\n\n\n)\n\n\n\n\n# Turn on instrumentation for OpenAI\n\n\nOpenAIInstrumentor\n().\ninstrument\n()\nCheckout our colab tutorial here:\nGoogle Colab\nUsing the Client SDK (downloading data & uploading datasets)\nOnce you collect trace data from the above configuration, you can access the data using the client SDK. You can also upload datasets for experiments using the client SDK.\nYou\'ll need to add the following environment variable to authenticate to hosted Phoenix.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n \n""api_key=...""\nHere\'s more sample code.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n f\n""api_key=...""\n\n\nos\n.\nenviron\n[\n""PHOENIX_COLLECTOR_ENDPOINT""\n]\n \n=\n \n""https://app.phoenix.arize.com""\n\n\n\n\nimport\n phoenix \nas\n px\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Is Hosted Phoenix free?\nAnswer: '}, {'role': 'assistant', 'content': 'Hosted Phoenix is free for all developers.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/hosted-phoenix\ntitle: Hosted Phoenix\n\nPHOENIX_API_KEY \n=\n os\n.\nenviron\n[\n""PHOENIX_API_KEY""\n]\n\n\nregister_otel\n(\n\n\n    endpoints\n=\n[Endpoints.HOSTED_PHOENIX],\n\n\n    api_key\n=\nPHOENIX_API_KEY\n\n\n)\n\n\n\n\n# Turn on instrumentation for OpenAI\n\n\nOpenAIInstrumentor\n().\ninstrument\n()\nCheckout our colab tutorial here:\nGoogle Colab\nUsing the Client SDK (downloading data & uploading datasets)\nOnce you collect trace data from the above configuration, you can access the data using the client SDK. You can also upload datasets for experiments using the client SDK.\nYou\'ll need to add the following environment variable to authenticate to hosted Phoenix.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n \n""api_key=...""\nHere\'s more sample code.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n f\n""api_key=...""\n\n\nos\n.\nenviron\n[\n""PHOENIX_COLLECTOR_ENDPOINT""\n]\n \n=\n \n""https://app.phoenix.arize.com""\n\n\n\n\nimport\n phoenix \nas\n px\n\n\n\n\npx_client \n=\n px\n.\nClient\n()\n\n\nphoenix_df \n=\n px_client\n.\nget_spans_dataframe\n()\nFAQ\nWill hosted Phoenix be on the latest version of Phoenix?\nOn account creation, we will always use the latest version of Phoenix. We try to keep all instances of hosted Phoenix up to date and run upgrades for them when new versions are available. There will be a few minutes of downtime during these periods.\nData retention\nWe have a 30 day data retention policy. We are working on plans to offer a longer data retention period.\nSharing\nCurrently accounts are setup to be used specifically for one developer. We will be adding ways to share your traces with other developers on your team shortly!\nPricing\nHosted Phoenix is free for all developers. We will add a paid tier in the future which increases your data retention and also give you access to more storage.\nAre there other demos available?\nYes. This demo and \naccompanying blog\n show how to deploy Phoenix via Docker/Kubernetes:\nPrevious\nQuickstart\nNext\nUser Guide\nLast updated \n7 days ago\n\nsource: https://docs.arize.com/phoenix/hosted-phoenix\ntitle: Hosted Phoenix\n\nHosted Phoenix\nWe now offer a hosted version of Phoenix to make it easier for developers to use Phoenix to trace their LLM applications and avoid setting up infrastructure. You can use our Colab links to follow along.\nFramework\nLlamaindex\nColab\nLlamaindex with Llamacloud\nColab\nOpenAI\nColab\nThe main differences for Hosted Phoenix:\nHosted Phoenix runs the latest version of our open source package and gates access to your data behind API keys and user authentication.\nYou must create an account\nYou need to add an API key as an environment variable during tracing\nYou need to add an API key as an environment variable when using the Client SDK\nWe also use 3rd party analytics tools to measure usage of our application to improve our services.\nHow to create an account\nClick signup on \nphoenix.arize.com\n. We offer logins via Google, Github, and email. This account will use the same account credentials as your Arize account if you have one.\nTracing: How to send in your first trace\nGet your API keys from your Phoenix application on the left hand side. \nHere\'s the full sample code for LlamaIndex and OpenAI instrumentation. You can see all of our automatic tracing options \nhere\n.\nLlamaIndex\nOpenAI\nInstall the following libraries\nCopy\n!pip install opentelemetry-sdk opentelemetry-exporter-otlp\n\n\n!pip install ""arize-phoenix[evals,llama-index]"" ""openai>=1"" gcsfs nest-asyncio ""openinference-instrumentation-llama-index>=2.0.0""\nUse the following python code to start instrumentation.\nCopy\nfrom\n opentelemetry \nimport\n trace \nas\n trace_api\n\n\nfrom\n opentelemetry\n.\nexporter\n.\notlp\n.\nproto\n.\nhttp\n.\ntrace_exporter \nimport\n OTLPSpanExporter\n\n\nfrom\n opentelemetry\n.\nsdk \nimport\n trace \nas\n trace_sdk\n\n\nfrom\n opentelemetry\n.\nsdk\n.\ntrace\n.\nexport \nimport\n SimpleSpanProcessor\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nllama_index \nimport\n LlamaIndexInstrumentor\n\n\n\n\n# Setup authentication and endpoint\n\n\nos\n.\nenviron\n[\n""OTEL_EXPORTER_OTLP_HEADERS""\n]\n \n=\n f\n""api_key=\n{PHOENIX_API_KEY}\n""\n\n\nendpoint \n=\n \n""https://app.phoenix.arize.com/v1/traces""\n\n\n\n\n# Setup tracing with OpenTelemetry\n\n\nspan_phoenix_processor \n=\n \nSimpleSpanProcessor\n(\nOTLPSpanExporter\n(endpoint\n=\nendpoint))\n\n\ntracer_provider \n=\n trace_sdk\n.\nTracerProvider\n()\n\n\ntracer_provider\n.\nadd_span_processor\n(span_processor\n=\nspan_phoenix_processor)\n\n\n\n\n# Start instrumentation\n\n\nLlamaIndexInstrumentor\n().\ninstrument\n(tracer_provider\n=\ntracer_provider, skip_dep_check\n=\nTrue\n)\nCheckout our colab tutorial here:\nInstall the following libraries\nCopy\npip\n \ninstall\n \narize-otel\n \nopeninference-instrumentation-openai\n \nopenai\nThen, use our library \narize-otel\n, which sets up OpenTelemetry tracing with Hosted Phoenix. Run the following code to start instrumentation.\nCopy\nimport\n os\n\n\nfrom\n arize_otel \nimport\n register_otel\n,\n Endpoints\n\n\nfrom\n openinference\n.\ninstrumentation\n.\nopenai \nimport\n OpenAIInstrumentor\n\n\n\n\n# Setup OTEL tracing for hosted Phoenix\n\n\n# Endpoints.HOSTED_PHOENIX = ""https://app.phoenix.arize.com""\n\n\nPHOENIX_API_KEY \n=\n os\n.\nenviron\n[\n""PHOENIX_API_KEY""\n]\n\n\nregister_otel\n(\n\n\n    endpoints\n=\n[Endpoints.HOSTED_PHOENIX],\n\n\n    api_key\n=\nPHOENIX_API_KEY\n\n\n)\n\n\n\n\n# Turn on instrumentation for OpenAI\n\n\nOpenAIInstrumentor\n().\ninstrument\n()\nCheckout our colab tutorial here:\nGoogle Colab\nUsing the Client SDK (downloading data & uploading datasets)\nOnce you collect trace data from the above configuration, you can access the data using the client SDK. You can also upload datasets for experiments using the client SDK.\nYou\'ll need to add the following environment variable to authenticate to hosted Phoenix.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n \n""api_key=...""\nHere\'s more sample code.\nCopy\nos\n.\nenviron\n[\n""PHOENIX_CLIENT_HEADERS""\n]\n \n=\n f\n""api_key=...""\n\n\nos\n.\nenviron\n[\n""PHOENIX_COLLECTOR_ENDPOINT""\n]\n \n=\n \n""https://app.phoenix.arize.com""\n\n\n\n\nimport\n phoenix \nas\n px\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Is Hosted Phoenix free?\nAnswer: '}, {'role': 'assistant', 'content': 'Hosted Phoenix is free for all developers.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6OTk=,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/reference/frequently-asked-questions\ntitle: Frequently Asked Questions\n\nos\n.\nenviron\n[\n""PHOENIX_NOTEBOOK_ENV""\n]\n \n=\n \n""sagemaker""\nCan I persistdata in the notbook?\nYou can persist data in the notebook by either setting the \nuse_temp_dir\n flag to false in \npx.launch_app\n which will persit your data in SQLite on your disk at the \nPHOENIX_WORKING_DIR\n. Alternatively you can deploy a phoenix instance and point to it via \nPHOENIX_COLLECTOR_ENDPOINT\n.\nCan I use gRPC for trace collection?\nPhoenix does natively support gRPC for trace collection post 4.0 release. See \nHow to configure phoenix\n for details.\nPrevious\nRagas\nNext\nOpenInference\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/deployment/persistence\ntitle: Persistence\n\nPersistence\nPhoenix is backed by a SQL database. By default, if you run phoenix with no configuration, it uses SQLite. However you can also configure Phoenix to use PostgreSQL as the backend database as well.\nPersistence is only available for \'arize-phoenix>=4.0.0\'\nPersistence for notebooks (a.k.a. \nlaunch_app\n) is disabled by default. To enable persistence in notebooks, set the \nuse_temp_dir\n to false.\nSQLite\nBy default Phoenix uses SQLite so that it runs with no external dependancies. This SQLite instance is by default mounted in the directory specified by the \nPHOENIX_WORKING_DIR\n environment variable (default value in your home directory, e.x. \n~/.phoenix/\n). The easiest way to make Phoenix to persist data is to back this working directory to a mounted volume. Attach the mounted volume to the phoenix pod and point \nPHOENIX_WORKING_DIR\n to that volume (e.x. \n/mnt/volume\n)\\\nPostgreSQL\nPhoenix also can natively be backed by PostgreSQL. To make Phoenix talk to PostgreSQL instead of SQLite, you will have to set the \nPHOENIX_SQL_DATABASE_URL\n to your PostgreSQL instance.\nPrevious\nQuickstart: Deployment\nNext\nKubernetes\nLast updated \n20 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Can I persist data in the notebook?\nAnswer: '}, {'role': 'assistant', 'content': 'You can persist data in the notebook by either setting the use_temp_dir flag to false in px.launch_app which will persist your data in SQLite on your disk at the PHOENIX_WORKING_DIR. Alternatively, you can deploy a phoenix instance and point to it via PHOENIX_COLLECTOR_ENDPOINT.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/reference/frequently-asked-questions\ntitle: Frequently Asked Questions\n\nos\n.\nenviron\n[\n""PHOENIX_NOTEBOOK_ENV""\n]\n \n=\n \n""sagemaker""\nCan I persistdata in the notbook?\nYou can persist data in the notebook by either setting the \nuse_temp_dir\n flag to false in \npx.launch_app\n which will persit your data in SQLite on your disk at the \nPHOENIX_WORKING_DIR\n. Alternatively you can deploy a phoenix instance and point to it via \nPHOENIX_COLLECTOR_ENDPOINT\n.\nCan I use gRPC for trace collection?\nPhoenix does natively support gRPC for trace collection post 4.0 release. See \nHow to configure phoenix\n for details.\nPrevious\nRagas\nNext\nOpenInference\nLast updated \n3 months ago\n\nsource: https://docs.arize.com/phoenix/deployment/persistence\ntitle: Persistence\n\nPersistence\nPhoenix is backed by a SQL database. By default, if you run phoenix with no configuration, it uses SQLite. However you can also configure Phoenix to use PostgreSQL as the backend database as well.\nPersistence is only available for \'arize-phoenix>=4.0.0\'\nPersistence for notebooks (a.k.a. \nlaunch_app\n) is disabled by default. To enable persistence in notebooks, set the \nuse_temp_dir\n to false.\nSQLite\nBy default Phoenix uses SQLite so that it runs with no external dependancies. This SQLite instance is by default mounted in the directory specified by the \nPHOENIX_WORKING_DIR\n environment variable (default value in your home directory, e.x. \n~/.phoenix/\n). The easiest way to make Phoenix to persist data is to back this working directory to a mounted volume. Attach the mounted volume to the phoenix pod and point \nPHOENIX_WORKING_DIR\n to that volume (e.x. \n/mnt/volume\n)\\\nPostgreSQL\nPhoenix also can natively be backed by PostgreSQL. To make Phoenix talk to PostgreSQL instead of SQLite, you will have to set the \nPHOENIX_SQL_DATABASE_URL\n to your PostgreSQL instance.\nPrevious\nQuickstart: Deployment\nNext\nKubernetes\nLast updated \n20 days ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Can I persist data in the notebook?\nAnswer: '}, {'role': 'assistant', 'content': 'You can persist data in the notebook by either setting the use_temp_dir flag to false in px.launch_app which will persist your data in SQLite on your disk at the PHOENIX_WORKING_DIR. Alternatively, you can deploy a phoenix instance and point to it via PHOENIX_COLLECTOR_ENDPOINT.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MTAw,"[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nNot more than 4-1/4 inches high, or more than 6 inches long, or greater than \\n0.016 inch thick.\\nd. Not more than 3.5 ounces (Charge flat-size prices for First-Class Mail \\ncard-type pieces over 3.5 ounces.)\\n\\n  Instruction: Based on the above documents, provide a detailed answer for the user question below.\\n  Answer \\""don\'t know\\"" if not present in the document.\\n  "",\n\n\n       \n""llm.input_messages.1.message.role""\n:\n \n""user""\n,\n\n\n       \n""llm.input_messages.1.message.content""\n:\n \n""Hello""\n,\n\n\n       \n""llm.model_name""\n:\n \n""gpt-4-turbo-preview""\n,\n\n\n       \n""llm.invocation_parameters""\n:\n \n""{\\""temperature\\"": 0.1, \\""model\\"": \\""gpt-4-turbo-preview\\""}""\n,\n\n\n       \n""output.value""\n:\n \n""How are you?""\n }\n,\n\n\n   \n""events""\n:\n []\n,\n\n\n   \n""links""\n:\n []\n,\n\n\n   \n""resource""\n:\n {\n\n\n       \n""attributes""\n:\n {}\n,\n\n\n       \n""schema_url""\n:\n \n""""\n\n\n   }\n\n\n}\nSpans can be nested, as is implied by the presence of a parent span ID: child spans represent sub-operations. This allows spans to more accurately capture the work done in an application.\nTraces\nA trace records the paths taken by requests (made by an application or end-user) as they propagate through multiple steps.\nWithout tracing, it is challenging to pinpoint the cause of performance problems in a system.\nIt improves the visibility of our application or system’s health and lets us debug behavior that is difficult to reproduce locally. Tracing is essential for LLM applications, which commonly have nondeterministic problems or are too complicated to reproduce locally.\nTracing makes debugging and understanding LLM applications less daunting by breaking down what happens within a request as it flows through a system.\nA trace is made of one or more spans. The first span represents the root span. Each root span represents a request from start to finish. The spans underneath the parent provide a more in-depth context of what occurs during a request (or what steps make up a request).\nProjects\nA \nproject\n is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces. Projects can be useful for various use-cases such as separating out environments, logging traces for evaluation runs, etc. To learn more about how to setup projects,  see the \nhow-to guide.\nSpan Kind\nWhen a span is created,  it is created as one of the following: Chain, Retriever, Reranker, LLM, Embedding, Agent, or Tool. \nCHAIN\nA Chain is a starting point or a link between different LLM application steps. For example, a Chain span could be used to represent the beginning of a request to an LLM application or the glue code that passes context from a retriever to and LLM call.\nRETRIEVER\nA Retriever is a span that represents a data retrieval step. For example, a Retriever span could be used to represent a call to a vector store or a database.\nRERANKER\nA Reranker is a span that represents the reranking of a set of input documents. For example, a cross-encoder may be used to compute the input documents\' relevance scores with respect to a user query, and the top K documents with the highest scores are then returned by the Reranker.\nLLM\nAn LLM is a span that represents a call to an LLM. For example, an LLM span could be used to represent a call to OpenAI or Llama.\nEMBEDDING\nAn Embedding is a span that represents a call to an LLM for an embedding. For example, an Embedding span could be used to represent a call OpenAI to get an ada-2 embedding for retrieval.\nTOOL\nA Tool is a span that represents a call to an external tool such as a calculator or a weather API.\nAGENT\nA span that encompasses calls to LLMs and Tools. An agent describes a reasoning block that acts on tools using the guidance of an LLM.\n\nSpan Attributes\nAttributes are key-value pairs that contain metadata that you can use to annotate a span to carry information about the operation it is tracking.\nFor example, if a span invokes an LLM, you can capture the model name, the invocation parameters, the token count, and so on.\nAttributes have the following rules:\nKeys must be non-null string values\nValues must be a non-null string, boolean, floating point value, integer, or an array of these values Additionally, there are Semantic Attributes, which are known naming conventions for metadata that is typically present in common operations. It\'s helpful to use semantic attribute naming wherever possible so that common kinds of metadata are standardized across systems. See \nsemantic conventions\n for more information.\n\nsource: https://docs.arize.com/phoenix/tracing/llm-traces\ntitle: Overview: Tracing\n\nOverview: Tracing\nTracing the execution of LLM applications using Telemetry\nLLM tracing records the paths taken by requests as they propagate through multiple steps or components of an LLM application. For example, when a user interacts with an LLM application, tracing can capture the sequence of operations, such as document retrieval, embedding generation, language model invocation, and response generation to provide a detailed timeline of the request\'s execution.\nTracing is a helpful tool for understanding how your LLM application works. Phoenix offers comprehensive tracing capabilities that are not tied to any specific LLM vendor or framework. Phoenix accepts traces over the OpenTelemetry protocol (OTLP) and supports first-class instrumentation for a variety of frameworks ( \nLlamaIndex\n, \nLangChain\n,\n DSPy\n),  SDKs (\nOpenAI\n, \nBedrock\n, \nMistral\n, \nVertex\n), and Languages. (Python, Javascript, etc.)\nUsing Phoenix\'s tracing capabilities can provide important insights into the inner workings of your LLM application. By analyzing the collected trace data, you can identify and address various performance and operational issues and improve the overall reliability and efficiency of your system.\nApplication Latency\n: Identify and address slow invocations of LLMs, Retrievers, and other components within your application, enabling you to optimize performance and responsiveness.\nToken Usage\n: Gain a detailed breakdown of token usage for your LLM calls, allowing you to identify and optimize the most expensive LLM invocations.\nRuntime Exceptions\n: Capture and inspect critical runtime exceptions, such as rate-limiting events, that can help you proactively address and mitigate potential issues.\nRetrieved Documents\n: Inspect the documents retrieved during a Retriever call, including the score and order in which they were returned to provide insight into the retrieval process.\nEmbeddings\n: Examine the embedding text used for retrieval and the underlying embedding model to allow you to validate and refine your embedding strategies.\nLLM Parameters\n: Inspect the parameters used when calling an LLM, such as temperature and system prompts, to ensure optimal configuration and debugging.\nPrompt Templates\n: Understand the prompt templates used during the prompting step and the variables that were applied, allowing you to fine-tune and improve your prompting strategies.\nTool Descriptions\n: View the descriptions and function signatures of the tools your LLM has been given access to in order to better understand and control your LLM’s capabilities.\nLLM Function Calls\n: For LLMs with function call capabilities (e.g., OpenAI), you can inspect the function selection and function messages in the input to the LLM, further improving your ability to debug and optimize your application.\nBy using tracing in Phoenix, you can gain increased visibility into your LLM application, empowering you to identify and address performance bottlenecks, optimize resource utilization, and ensure the overall reliability and effectiveness of your system.\nView the inner workings for your LLM Application\nTo get started, check out the \nQuickstart guide\nAfter that, read through the \nConcepts Section\n to get and understanding of the different components.\nIf you want to learn how to accomplish a particular task, check out the \nHow-To Guides.\n\n\nPrevious\nFAQs: Deployment\nNext\nQuickstart: Tracing\nLast updated \n6 hours ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Can I use gRPC for trace collection?\nAnswer: '}, {'role': 'assistant', 'content': 'You can use gRPC for trace collection.'}]","[{'role': 'system', 'content': ""You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.""}, {'role': 'user', 'content': 'Context information is below.\n---------------------\nsource: https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces\ntitle: Traces\n\nNot more than 4-1/4 inches high, or more than 6 inches long, or greater than \\n0.016 inch thick.\\nd. Not more than 3.5 ounces (Charge flat-size prices for First-Class Mail \\ncard-type pieces over 3.5 ounces.)\\n\\n  Instruction: Based on the above documents, provide a detailed answer for the user question below.\\n  Answer \\""don\'t know\\"" if not present in the document.\\n  "",\n\n\n       \n""llm.input_messages.1.message.role""\n:\n \n""user""\n,\n\n\n       \n""llm.input_messages.1.message.content""\n:\n \n""Hello""\n,\n\n\n       \n""llm.model_name""\n:\n \n""gpt-4-turbo-preview""\n,\n\n\n       \n""llm.invocation_parameters""\n:\n \n""{\\""temperature\\"": 0.1, \\""model\\"": \\""gpt-4-turbo-preview\\""}""\n,\n\n\n       \n""output.value""\n:\n \n""How are you?""\n }\n,\n\n\n   \n""events""\n:\n []\n,\n\n\n   \n""links""\n:\n []\n,\n\n\n   \n""resource""\n:\n {\n\n\n       \n""attributes""\n:\n {}\n,\n\n\n       \n""schema_url""\n:\n \n""""\n\n\n   }\n\n\n}\nSpans can be nested, as is implied by the presence of a parent span ID: child spans represent sub-operations. This allows spans to more accurately capture the work done in an application.\nTraces\nA trace records the paths taken by requests (made by an application or end-user) as they propagate through multiple steps.\nWithout tracing, it is challenging to pinpoint the cause of performance problems in a system.\nIt improves the visibility of our application or system’s health and lets us debug behavior that is difficult to reproduce locally. Tracing is essential for LLM applications, which commonly have nondeterministic problems or are too complicated to reproduce locally.\nTracing makes debugging and understanding LLM applications less daunting by breaking down what happens within a request as it flows through a system.\nA trace is made of one or more spans. The first span represents the root span. Each root span represents a request from start to finish. The spans underneath the parent provide a more in-depth context of what occurs during a request (or what steps make up a request).\nProjects\nA \nproject\n is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces. Projects can be useful for various use-cases such as separating out environments, logging traces for evaluation runs, etc. To learn more about how to setup projects,  see the \nhow-to guide.\nSpan Kind\nWhen a span is created,  it is created as one of the following: Chain, Retriever, Reranker, LLM, Embedding, Agent, or Tool. \nCHAIN\nA Chain is a starting point or a link between different LLM application steps. For example, a Chain span could be used to represent the beginning of a request to an LLM application or the glue code that passes context from a retriever to and LLM call.\nRETRIEVER\nA Retriever is a span that represents a data retrieval step. For example, a Retriever span could be used to represent a call to a vector store or a database.\nRERANKER\nA Reranker is a span that represents the reranking of a set of input documents. For example, a cross-encoder may be used to compute the input documents\' relevance scores with respect to a user query, and the top K documents with the highest scores are then returned by the Reranker.\nLLM\nAn LLM is a span that represents a call to an LLM. For example, an LLM span could be used to represent a call to OpenAI or Llama.\nEMBEDDING\nAn Embedding is a span that represents a call to an LLM for an embedding. For example, an Embedding span could be used to represent a call OpenAI to get an ada-2 embedding for retrieval.\nTOOL\nA Tool is a span that represents a call to an external tool such as a calculator or a weather API.\nAGENT\nA span that encompasses calls to LLMs and Tools. An agent describes a reasoning block that acts on tools using the guidance of an LLM.\n\nSpan Attributes\nAttributes are key-value pairs that contain metadata that you can use to annotate a span to carry information about the operation it is tracking.\nFor example, if a span invokes an LLM, you can capture the model name, the invocation parameters, the token count, and so on.\nAttributes have the following rules:\nKeys must be non-null string values\nValues must be a non-null string, boolean, floating point value, integer, or an array of these values Additionally, there are Semantic Attributes, which are known naming conventions for metadata that is typically present in common operations. It\'s helpful to use semantic attribute naming wherever possible so that common kinds of metadata are standardized across systems. See \nsemantic conventions\n for more information.\n\nsource: https://docs.arize.com/phoenix/tracing/llm-traces\ntitle: Overview: Tracing\n\nOverview: Tracing\nTracing the execution of LLM applications using Telemetry\nLLM tracing records the paths taken by requests as they propagate through multiple steps or components of an LLM application. For example, when a user interacts with an LLM application, tracing can capture the sequence of operations, such as document retrieval, embedding generation, language model invocation, and response generation to provide a detailed timeline of the request\'s execution.\nTracing is a helpful tool for understanding how your LLM application works. Phoenix offers comprehensive tracing capabilities that are not tied to any specific LLM vendor or framework. Phoenix accepts traces over the OpenTelemetry protocol (OTLP) and supports first-class instrumentation for a variety of frameworks ( \nLlamaIndex\n, \nLangChain\n,\n DSPy\n),  SDKs (\nOpenAI\n, \nBedrock\n, \nMistral\n, \nVertex\n), and Languages. (Python, Javascript, etc.)\nUsing Phoenix\'s tracing capabilities can provide important insights into the inner workings of your LLM application. By analyzing the collected trace data, you can identify and address various performance and operational issues and improve the overall reliability and efficiency of your system.\nApplication Latency\n: Identify and address slow invocations of LLMs, Retrievers, and other components within your application, enabling you to optimize performance and responsiveness.\nToken Usage\n: Gain a detailed breakdown of token usage for your LLM calls, allowing you to identify and optimize the most expensive LLM invocations.\nRuntime Exceptions\n: Capture and inspect critical runtime exceptions, such as rate-limiting events, that can help you proactively address and mitigate potential issues.\nRetrieved Documents\n: Inspect the documents retrieved during a Retriever call, including the score and order in which they were returned to provide insight into the retrieval process.\nEmbeddings\n: Examine the embedding text used for retrieval and the underlying embedding model to allow you to validate and refine your embedding strategies.\nLLM Parameters\n: Inspect the parameters used when calling an LLM, such as temperature and system prompts, to ensure optimal configuration and debugging.\nPrompt Templates\n: Understand the prompt templates used during the prompting step and the variables that were applied, allowing you to fine-tune and improve your prompting strategies.\nTool Descriptions\n: View the descriptions and function signatures of the tools your LLM has been given access to in order to better understand and control your LLM’s capabilities.\nLLM Function Calls\n: For LLMs with function call capabilities (e.g., OpenAI), you can inspect the function selection and function messages in the input to the LLM, further improving your ability to debug and optimize your application.\nBy using tracing in Phoenix, you can gain increased visibility into your LLM application, empowering you to identify and address performance bottlenecks, optimize resource utilization, and ensure the overall reliability and effectiveness of your system.\nView the inner workings for your LLM Application\nTo get started, check out the \nQuickstart guide\nAfter that, read through the \nConcepts Section\n to get and understanding of the different components.\nIf you want to learn how to accomplish a particular task, check out the \nHow-To Guides.\n\n\nPrevious\nFAQs: Deployment\nNext\nQuickstart: Tracing\nLast updated \n6 hours ago\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Can I use gRPC for trace collection?\nAnswer: '}, {'role': 'assistant', 'content': 'You can use gRPC for trace collection.'}]",,,,,,,,,,
RGF0YXNldEV4YW1wbGU6MTA1,"[{'role': 'system', 'content': 'You are an expert in ProteinMPNN modeling. Provide parameters followed by a clear, conversational explanation.'}, {'role': 'user', 'content': ""\n        Analyze the following PDB content and suggest appropriate parameters for running ProteinMPNN:\n\n        ATOM      1  N   GLY A   1      21.389 -26.554  -5.432  1.00  0.00\nATOM      2  CA  GLY A   1      21.056 -26.409  -4.020  1.00  0.00\nATOM      3  C   GLY A   1      19.768 -25.616  -3.837  1.00  0.00\nATOM      4  O   GLY A   1      19.530 -24.631  -4.535  1.00  0.00\nATOM      5  N   GLY A   2      18.866 -26.184  -3.160  1.00  0.00\nATOM      6  CA  GLY A   2      17.618 -25.470  -2.917  1.00  0.00\nATOM      7  C   GLY A   2      17.803 -24.376  -1.874  1.00  0.00\nATOM      8  O   GLY A   2     ...\n\n        Provide a clear, conversational explanation of:\n        1. ca_only setting and why it's appropriate\n        2. use_soluble_model choice based on the structure\n        3. num_seq_per_target recommendation\n        4. sampling_temp selection\n\n        Format your response with clear parameter values first, followed by a natural explanation suitable for voice synthesis.\n        ""}]","[{'role': 'assistant', 'content': ""Sure! Here are the recommended parameters for running ProteinMPNN with your PDB content:\n\n1. **ca_only**: `True`\n2. **use_soluble_model**: `False`\n3. **num_seq_per_target**: `10`\n4. **sampling_temp**: `0.1`\n\nAnd now, let's go through each of these parameters to explain why I've chosen them:\n\n### 1. ca_only setting: `True`\nIn your PDB content, we see only backbone atoms listed for each residue (namely, N, CA, C, and O for Glycine). The `ca_only` parameter set to `True` tells ProteinMPNN to focus only on alpha carbon (`CA`) coordinates to predict the protein sequences. This is often used when you have high-resolution structures where side chain data might be missing or unnecessary for initial predictions.\n\n### 2. use_soluble_model: `False`\nThe choice between using a soluble versus a membrane model depends significantly on the type of protein you’re analyzing. Since no specific mention of membrane-binding properties is indicated in your snippet, and given that default practices favor the general model due to its wide applicability, setting `use_soluble_model` to `False` is appropriate. This utilizes the general-purpose model, likely offering better cross-model predictions for various protein contexts.\n\n### 3. num_seq_per_target: `10`\nThe `num_seq_per_target` parameter dictates how many sequence predictions you want to generate for each structure. A value of `10` is a good starting point. It balances computation time and diversity of results, providing a wide enough pool of sequence possibilities without being overwhelmingly large. This number enables you to identify high-confidence predictions while still exploring a range of possible sequences.\n\n### 4. sampling_temp: `0.1`\nThe `sampling_temp` parameter controls the stochastic aspect of sequence generation. Lower values lead to less randomization, focusing on predicted patterns. A sampling temperature of `0.1` means the model will give you predictions with a bit of variety but leaning heavily on the most probable sequences. It's suitable for early iterations when you'd like to see likely sequences first before considering more diversity.\n\nFeel free to reach out for more details or any further questions!""}]","{'mime_type': 'application/json', 'value': '{""id"":""chatcmpl-ARfwMJ6TMq7FcxQwxwCUgdFjTdPvi"",""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""message"":{""content"":""Sure! Here are the recommended parameters for running ProteinMPNN with your PDB content:\\n\\n1. **ca_only**: `True`\\n2. **use_soluble_model**: `False`\\n3. **num_seq_per_target**: `10`\\n4. **sampling_temp**: `0.1`\\n\\nAnd now, let\'s go through each of these parameters to explain why I\'ve chosen them:\\n\\n### 1. ca_only setting: `True`\\nIn your PDB content, we see only backbone atoms listed for each residue (namely, N, CA, C, and O for Glycine). The `ca_only` parameter set to `True` tells ProteinMPNN to focus only on alpha carbon (`CA`) coordinates to predict the protein sequences. This is often used when you have high-resolution structures where side chain data might be missing or unnecessary for initial predictions.\\n\\n### 2. use_soluble_model: `False`\\nThe choice between using a soluble versus a membrane model depends significantly on the type of protein you’re analyzing. Since no specific mention of membrane-binding properties is indicated in your snippet, and given that default practices favor the general model due to its wide applicability, setting `use_soluble_model` to `False` is appropriate. This utilizes the general-purpose model, likely offering better cross-model predictions for various protein contexts.\\n\\n### 3. num_seq_per_target: `10`\\nThe `num_seq_per_target` parameter dictates how many sequence predictions you want to generate for each structure. A value of `10` is a good starting point. It balances computation time and diversity of results, providing a wide enough pool of sequence possibilities without being overwhelmingly large. This number enables you to identify high-confidence predictions while still exploring a range of possible sequences.\\n\\n### 4. sampling_temp: `0.1`\\nThe `sampling_temp` parameter controls the stochastic aspect of sequence generation. Lower values lead to less randomization, focusing on predicted patterns. A sampling temperature of `0.1` means the model will give you predictions with a bit of variety but leaning heavily on the most probable sequences. It\'s suitable for early iterations when you\'d like to see likely sequences first before considering more diversity.\\n\\nFeel free to reach out for more details or any further questions!"",""refusal"":null,""role"":""assistant""}}],""created"":1731159438,""model"":""gpt-4o-2024-08-06"",""object"":""chat.completion"",""system_fingerprint"":""fp_45cf54deae"",""usage"":{""completion_tokens"":456,""prompt_tokens"":406,""total_tokens"":862,""completion_tokens_details"":{""accepted_prediction_tokens"":0,""audio_tokens"":0,""reasoning_tokens"":0,""rejected_prediction_tokens"":0},""prompt_tokens_details"":{""audio_tokens"":0,""cached_tokens"":0}}}'}","{'invocation_parameters': '{""model"": ""gpt-4o""}', 'input_messages': [{'message': {'content': 'You are an expert in ProteinMPNN modeling. Provide parameters followed by a clear, conversational explanation.', 'role': 'system'}}, {'message': {'content': ""\n        Analyze the following PDB content and suggest appropriate parameters for running ProteinMPNN:\n\n        ATOM      1  N   GLY A   1      21.389 -26.554  -5.432  1.00  0.00\nATOM      2  CA  GLY A   1      21.056 -26.409  -4.020  1.00  0.00\nATOM      3  C   GLY A   1      19.768 -25.616  -3.837  1.00  0.00\nATOM      4  O   GLY A   1      19.530 -24.631  -4.535  1.00  0.00\nATOM      5  N   GLY A   2      18.866 -26.184  -3.160  1.00  0.00\nATOM      6  CA  GLY A   2      17.618 -25.470  -2.917  1.00  0.00\nATOM      7  C   GLY A   2      17.803 -24.376  -1.874  1.00  0.00\nATOM      8  O   GLY A   2     ...\n\n        Provide a clear, conversational explanation of:\n        1. ca_only setting and why it's appropriate\n        2. use_soluble_model choice based on the structure\n        3. num_seq_per_target recommendation\n        4. sampling_temp selection\n\n        Format your response with clear parameter values first, followed by a natural explanation suitable for voice synthesis.\n        "", 'role': 'user'}}], 'token_count': {'total': 862, 'prompt': 406, 'completion': 456}, 'system': 'openai', 'provider': 'openai', 'output_messages': [{'message': {'content': ""Sure! Here are the recommended parameters for running ProteinMPNN with your PDB content:\n\n1. **ca_only**: `True`\n2. **use_soluble_model**: `False`\n3. **num_seq_per_target**: `10`\n4. **sampling_temp**: `0.1`\n\nAnd now, let's go through each of these parameters to explain why I've chosen them:\n\n### 1. ca_only setting: `True`\nIn your PDB content, we see only backbone atoms listed for each residue (namely, N, CA, C, and O for Glycine). The `ca_only` parameter set to `True` tells ProteinMPNN to focus only on alpha carbon (`CA`) coordinates to predict the protein sequences. This is often used when you have high-resolution structures where side chain data might be missing or unnecessary for initial predictions.\n\n### 2. use_soluble_model: `False`\nThe choice between using a soluble versus a membrane model depends significantly on the type of protein you’re analyzing. Since no specific mention of membrane-binding properties is indicated in your snippet, and given that default practices favor the general model due to its wide applicability, setting `use_soluble_model` to `False` is appropriate. This utilizes the general-purpose model, likely offering better cross-model predictions for various protein contexts.\n\n### 3. num_seq_per_target: `10`\nThe `num_seq_per_target` parameter dictates how many sequence predictions you want to generate for each structure. A value of `10` is a good starting point. It balances computation time and diversity of results, providing a wide enough pool of sequence possibilities without being overwhelmingly large. This number enables you to identify high-confidence predictions while still exploring a range of possible sequences.\n\n### 4. sampling_temp: `0.1`\nThe `sampling_temp` parameter controls the stochastic aspect of sequence generation. Lower values lead to less randomization, focusing on predicted patterns. A sampling temperature of `0.1` means the model will give you predictions with a bit of variety but leaning heavily on the most probable sequences. It's suitable for early iterations when you'd like to see likely sequences first before considering more diversity.\n\nFeel free to reach out for more details or any further questions!"", 'role': 'assistant'}}], 'model_name': 'gpt-4o-2024-08-06'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""messages"": [{""role"": ""system"", ""content"": ""You are an expert in ProteinMPNN modeling. Provide parameters followed by a clear, conversational explanation.""}, {""role"": ""user"", ""content"": ""\\n        Analyze the following PDB content and suggest appropriate parameters for running ProteinMPNN:\\n\\n        ATOM      1  N   GLY A   1      21.389 -26.554  -5.432  1.00  0.00\\nATOM      2  CA  GLY A   1      21.056 -26.409  -4.020  1.00  0.00\\nATOM      3  C   GLY A   1      19.768 -25.616  -3.837  1.00  0.00\\nATOM      4  O   GLY A   1      19.530 -24.631  -4.535  1.00  0.00\\nATOM      5  N   GLY A   2      18.866 -26.184  -3.160  1.00  0.00\\nATOM      6  CA  GLY A   2      17.618 -25.470  -2.917  1.00  0.00\\nATOM      7  C   GLY A   2      17.803 -24.376  -1.874  1.00  0.00\\nATOM      8  O   GLY A   2     ...\\n\\n        Provide a clear, conversational explanation of:\\n        1. ca_only setting and why it\'s appropriate\\n        2. use_soluble_model choice based on the structure\\n        3. num_seq_per_target recommendation\\n        4. sampling_temp selection\\n\\n        Format your response with clear parameter values first, followed by a natural explanation suitable for voice synthesis.\\n        ""}], ""model"": ""gpt-4o""}'}",{},,,,,
RGF0YXNldEV4YW1wbGU6MTA2,,,{'value': '<openai._legacy_response.HttpxBinaryResponseContent object at 0x7a73fc002710>'},"{'invocation_parameters': '{""input"": ""1. **ca_only**: `True`\\n2. **use_soluble_model**: `False`\\n3. **num_seq_per_target**: `10`\\n4. **sampling_temp**: `0.1`\\n\\nAnd now, let\'s go through each of these parameters to explain why I\'ve chosen them:\\n\\n### 1. ca_only setting: `True`\\nIn your PDB content, we see only backbone atoms listed for each residue (namely, N, CA, C, and O for Glycine). The `ca_only` parameter set to `True` tells ProteinMPNN to focus only on alpha carbon (`CA`) coordinates to predict the protein sequences. This is often used when you have high-resolution structures where side chain data might be missing or unnecessary for initial predictions.\\n\\n### 2. use_soluble_model: `False`\\nThe choice between using a soluble versus a membrane model depends significantly on the type of protein you’re analyzing. Since no specific mention of membrane-binding properties is indicated in your snippet, and given that default practices favor the general model due to its wide applicability, setting `use_soluble_model` to `False` is appropriate. This utilizes the general-purpose model, likely offering better cross-model predictions for various protein contexts.\\n\\n### 3. num_seq_per_target: `10`\\nThe `num_seq_per_target` parameter dictates how many sequence predictions you want to generate for each structure. A value of `10` is a good starting point. It balances computation time and diversity of results, providing a wide enough pool of sequence possibilities without being overwhelmingly large. This number enables you to identify high-confidence predictions while still exploring a range of possible sequences.\\n\\n### 4. sampling_temp: `0.1`\\nThe `sampling_temp` parameter controls the stochastic aspect of sequence generation. Lower values lead to less randomization, focusing on predicted patterns. A sampling temperature of `0.1` means the model will give you predictions with a bit of variety but leaning heavily on the most probable sequences. It\'s suitable for early iterations when you\'d like to see likely sequences first before considering more diversity.\\n\\nFeel free to reach out for more details or any further questions!"", ""model"": ""tts-1"", ""voice"": ""alloy""}', 'provider': 'openai', 'system': 'openai'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""input"": ""1. **ca_only**: `True`\\n2. **use_soluble_model**: `False`\\n3. **num_seq_per_target**: `10`\\n4. **sampling_temp**: `0.1`\\n\\nAnd now, let\'s go through each of these parameters to explain why I\'ve chosen them:\\n\\n### 1. ca_only setting: `True`\\nIn your PDB content, we see only backbone atoms listed for each residue (namely, N, CA, C, and O for Glycine). The `ca_only` parameter set to `True` tells ProteinMPNN to focus only on alpha carbon (`CA`) coordinates to predict the protein sequences. This is often used when you have high-resolution structures where side chain data might be missing or unnecessary for initial predictions.\\n\\n### 2. use_soluble_model: `False`\\nThe choice between using a soluble versus a membrane model depends significantly on the type of protein you’re analyzing. Since no specific mention of membrane-binding properties is indicated in your snippet, and given that default practices favor the general model due to its wide applicability, setting `use_soluble_model` to `False` is appropriate. This utilizes the general-purpose model, likely offering better cross-model predictions for various protein contexts.\\n\\n### 3. num_seq_per_target: `10`\\nThe `num_seq_per_target` parameter dictates how many sequence predictions you want to generate for each structure. A value of `10` is a good starting point. It balances computation time and diversity of results, providing a wide enough pool of sequence possibilities without being overwhelmingly large. This number enables you to identify high-confidence predictions while still exploring a range of possible sequences.\\n\\n### 4. sampling_temp: `0.1`\\nThe `sampling_temp` parameter controls the stochastic aspect of sequence generation. Lower values lead to less randomization, focusing on predicted patterns. A sampling temperature of `0.1` means the model will give you predictions with a bit of variety but leaning heavily on the most probable sequences. It\'s suitable for early iterations when you\'d like to see likely sequences first before considering more diversity.\\n\\nFeel free to reach out for more details or any further questions!"", ""model"": ""tts-1"", ""voice"": ""alloy""}'}",{},"1. **ca_only**: `True`
2. **use_soluble_model**: `False`
3. **num_seq_per_target**: `10`
4. **sampling_temp**: `0.1`

And now, let's go through each of these parameters to explain why I've chosen them:

### 1. ca_only setting: `True`
In your PDB content, we see only backbone atoms listed for each residue (namely, N, CA, C, and O for Glycine). The `ca_only` parameter set to `True` tells ProteinMPNN to focus only on alpha carbon (`CA`) coordinates to predict the protein sequences. This is often used when you have high-resolution structures where side chain data might be missing or unnecessary for initial predictions.

### 2. use_soluble_model: `False`
The choice between using a soluble versus a membrane model depends significantly on the type of protein you’re analyzing. Since no specific mention of membrane-binding properties is indicated in your snippet, and given that default practices favor the general model due to its wide applicability, setting `use_soluble_model` to `False` is appropriate. This utilizes the general-purpose model, likely offering better cross-model predictions for various protein contexts.

### 3. num_seq_per_target: `10`
The `num_seq_per_target` parameter dictates how many sequence predictions you want to generate for each structure. A value of `10` is a good starting point. It balances computation time and diversity of results, providing a wide enough pool of sequence possibilities without being overwhelmingly large. This number enables you to identify high-confidence predictions while still exploring a range of possible sequences.

### 4. sampling_temp: `0.1`
The `sampling_temp` parameter controls the stochastic aspect of sequence generation. Lower values lead to less randomization, focusing on predicted patterns. A sampling temperature of `0.1` means the model will give you predictions with a bit of variety but leaning heavily on the most probable sequences. It's suitable for early iterations when you'd like to see likely sequences first before considering more diversity.

Feel free to reach out for more details or any further questions!",tts-1,alloy,<openai._legacy_response.HttpxBinaryResponseContent object at 0x7a73fc002710>,
RGF0YXNldEV4YW1wbGU6MTA3,,,,,,,{},,,,,47.0
RGF0YXNldEV4YW1wbGU6MTA4,,,,,,,{},,,,,47.0
RGF0YXNldEV4YW1wbGU6MTA5,,,,,,,{},,,,,47.0
RGF0YXNldEV4YW1wbGU6MTEw,"[{'role': 'system', 'content': ""You are an expert in RFdiffusion parameter optimization.\n                Always ensure contigs parameter has both target specification AND binder length range after /0.\n                Example: 'A20-60/0 50-100' NOT just 'A20-60/0'""}, {'role': 'user', 'content': '\n        Analyze this PDB content and suggest basic parameters for RFdiffusion that will work reliably.\n        Follow this exact format in your response:\n\n        PARAMETERS:\n        1. contigs: [value]\n        2. hotspot_res: [value]\n        3. diffusion_steps: [value]\n\n        EXPLANATION:\n        Provide a clear, conversational explanation of why these parameters were chosen and how they will affect the protein design process.\n        Keep the explanation natural and suitable for voice output.\n\n        Base the analysis on this PDB structure:\n        HEADER    IMMUNE SYSTEM                           24-JAN-01   1HZH              \nTITLE     CRYSTAL STRUCTURE OF THE INTACT HUMAN IGG B12 WITH BROAD AND POTENT   \nTITLE    2 ACTIVITY AGAINST PRIMARY HIV-1 ISOLATES: A TEMPLATE FOR HIV VACCINE  \nTITLE    3 DESIGN                                                               \nCOMPND    MOL_ID: 1;                                                            \nCOMPND   2 MOLECULE: IMMUNOGLOBULIN HEAVY CHAIN;                                \nCOMPND   3 CHA...\n        '}]","[{'role': 'assistant', 'content': ""PARAMETERS:\n1. contigs: 'A20-60/0 50-100'\n2. hotspot_res: 'H:55, L:100'\n3. diffusion_steps: '200'\n\nEXPLANATION:\nLet's break down how these parameters would work in RFdiffusion for designing with the given PDB structure, which is an intact human immunoglobulin G (IgG) antibody. The 'contigs' parameter specifies the range of amino acid segments to be considered in the design. Here, 'A20-60/0 50-100' indicates we're targeting a segment that spans residues 20 to 60 in a chain, and also focusing on a possible flexible region or linker of lengths 50 to 100. This allows us to consider both the specificity needed for binding, given the nature of antibodies, and an acceptable range of binder length variations.\n\nFor 'hotspot_res', I've chosen residues based on typical locations of antibody-antigen interactions. These are general suggestions assuming you might be interested in the complementarity-determining region (CDR) of the heavy chain ('H:55') and light chain ('L:100'), although these can be refined based on specific antigen interactions or structural insights.\n\nFinally, 'diffusion_steps', set to 200, provides a balance between computational efficiency and the thorough exploration of conformational space, which is especially important in achieving a stable design with complex structures like antibodies. This straightforward explanation aims to align technical decisions with their practical implications, ensuring the protein design process is both effective and efficient.""}]","{'mime_type': 'application/json', 'value': '{""id"":""chatcmpl-AReCUhTMm4DJbfGRAQssoBVDqB5ER"",""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""message"":{""content"":""PARAMETERS:\\n1. contigs: \'A20-60/0 50-100\'\\n2. hotspot_res: \'H:55, L:100\'\\n3. diffusion_steps: \'200\'\\n\\nEXPLANATION:\\nLet\'s break down how these parameters would work in RFdiffusion for designing with the given PDB structure, which is an intact human immunoglobulin G (IgG) antibody. The \'contigs\' parameter specifies the range of amino acid segments to be considered in the design. Here, \'A20-60/0 50-100\' indicates we\'re targeting a segment that spans residues 20 to 60 in a chain, and also focusing on a possible flexible region or linker of lengths 50 to 100. This allows us to consider both the specificity needed for binding, given the nature of antibodies, and an acceptable range of binder length variations.\\n\\nFor \'hotspot_res\', I\'ve chosen residues based on typical locations of antibody-antigen interactions. These are general suggestions assuming you might be interested in the complementarity-determining region (CDR) of the heavy chain (\'H:55\') and light chain (\'L:100\'), although these can be refined based on specific antigen interactions or structural insights.\\n\\nFinally, \'diffusion_steps\', set to 200, provides a balance between computational efficiency and the thorough exploration of conformational space, which is especially important in achieving a stable design with complex structures like antibodies. This straightforward explanation aims to align technical decisions with their practical implications, ensuring the protein design process is both effective and efficient."",""refusal"":null,""role"":""assistant""}}],""created"":1731152750,""model"":""gpt-4o-2024-08-06"",""object"":""chat.completion"",""system_fingerprint"":""fp_45cf54deae"",""usage"":{""completion_tokens"":315,""prompt_tokens"":294,""total_tokens"":609,""completion_tokens_details"":{""accepted_prediction_tokens"":0,""audio_tokens"":0,""reasoning_tokens"":0,""rejected_prediction_tokens"":0},""prompt_tokens_details"":{""audio_tokens"":0,""cached_tokens"":0}}}'}","{'invocation_parameters': '{""model"": ""gpt-4o""}', 'input_messages': [{'message': {'content': ""You are an expert in RFdiffusion parameter optimization.\n                Always ensure contigs parameter has both target specification AND binder length range after /0.\n                Example: 'A20-60/0 50-100' NOT just 'A20-60/0'"", 'role': 'system'}}, {'message': {'content': '\n        Analyze this PDB content and suggest basic parameters for RFdiffusion that will work reliably.\n        Follow this exact format in your response:\n\n        PARAMETERS:\n        1. contigs: [value]\n        2. hotspot_res: [value]\n        3. diffusion_steps: [value]\n\n        EXPLANATION:\n        Provide a clear, conversational explanation of why these parameters were chosen and how they will affect the protein design process.\n        Keep the explanation natural and suitable for voice output.\n\n        Base the analysis on this PDB structure:\n        HEADER    IMMUNE SYSTEM                           24-JAN-01   1HZH              \nTITLE     CRYSTAL STRUCTURE OF THE INTACT HUMAN IGG B12 WITH BROAD AND POTENT   \nTITLE    2 ACTIVITY AGAINST PRIMARY HIV-1 ISOLATES: A TEMPLATE FOR HIV VACCINE  \nTITLE    3 DESIGN                                                               \nCOMPND    MOL_ID: 1;                                                            \nCOMPND   2 MOLECULE: IMMUNOGLOBULIN HEAVY CHAIN;                                \nCOMPND   3 CHA...\n        ', 'role': 'user'}}], 'token_count': {'total': 609, 'prompt': 294, 'completion': 315}, 'system': 'openai', 'provider': 'openai', 'output_messages': [{'message': {'content': ""PARAMETERS:\n1. contigs: 'A20-60/0 50-100'\n2. hotspot_res: 'H:55, L:100'\n3. diffusion_steps: '200'\n\nEXPLANATION:\nLet's break down how these parameters would work in RFdiffusion for designing with the given PDB structure, which is an intact human immunoglobulin G (IgG) antibody. The 'contigs' parameter specifies the range of amino acid segments to be considered in the design. Here, 'A20-60/0 50-100' indicates we're targeting a segment that spans residues 20 to 60 in a chain, and also focusing on a possible flexible region or linker of lengths 50 to 100. This allows us to consider both the specificity needed for binding, given the nature of antibodies, and an acceptable range of binder length variations.\n\nFor 'hotspot_res', I've chosen residues based on typical locations of antibody-antigen interactions. These are general suggestions assuming you might be interested in the complementarity-determining region (CDR) of the heavy chain ('H:55') and light chain ('L:100'), although these can be refined based on specific antigen interactions or structural insights.\n\nFinally, 'diffusion_steps', set to 200, provides a balance between computational efficiency and the thorough exploration of conformational space, which is especially important in achieving a stable design with complex structures like antibodies. This straightforward explanation aims to align technical decisions with their practical implications, ensuring the protein design process is both effective and efficient."", 'role': 'assistant'}}], 'model_name': 'gpt-4o-2024-08-06'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""messages"": [{""role"": ""system"", ""content"": ""You are an expert in RFdiffusion parameter optimization.\\n                Always ensure contigs parameter has both target specification AND binder length range after /0.\\n                Example: \'A20-60/0 50-100\' NOT just \'A20-60/0\'""}, {""role"": ""user"", ""content"": ""\\n        Analyze this PDB content and suggest basic parameters for RFdiffusion that will work reliably.\\n        Follow this exact format in your response:\\n\\n        PARAMETERS:\\n        1. contigs: [value]\\n        2. hotspot_res: [value]\\n        3. diffusion_steps: [value]\\n\\n        EXPLANATION:\\n        Provide a clear, conversational explanation of why these parameters were chosen and how they will affect the protein design process.\\n        Keep the explanation natural and suitable for voice output.\\n\\n        Base the analysis on this PDB structure:\\n        HEADER    IMMUNE SYSTEM                           24-JAN-01   1HZH              \\nTITLE     CRYSTAL STRUCTURE OF THE INTACT HUMAN IGG B12 WITH BROAD AND POTENT   \\nTITLE    2 ACTIVITY AGAINST PRIMARY HIV-1 ISOLATES: A TEMPLATE FOR HIV VACCINE  \\nTITLE    3 DESIGN                                                               \\nCOMPND    MOL_ID: 1;                                                            \\nCOMPND   2 MOLECULE: IMMUNOGLOBULIN HEAVY CHAIN;                                \\nCOMPND   3 CHA...\\n        ""}], ""model"": ""gpt-4o""}'}",{},,,,,
RGF0YXNldEV4YW1wbGU6MTEx,,,{'value': '<openai._legacy_response.HttpxBinaryResponseContent object at 0x7a7456e7e140>'},"{'invocation_parameters': '{""input"": ""Let\'s break down how these parameters would work in RFdiffusion for designing with the given PDB structure, which is an intact human immunoglobulin G (IgG) antibody. The \'contigs\' parameter specifies the range of amino acid segments to be considered in the design. Here, \'A20-60/0 50-100\' indicates we\'re targeting a segment that spans residues 20 to 60 in a chain, and also focusing on a possible flexible region or linker of lengths 50 to 100. This allows us to consider both the specificity needed for binding, given the nature of antibodies, and an acceptable range of binder length variations.\\n\\nFor \'hotspot_res\', I\'ve chosen residues based on typical locations of antibody-antigen interactions. These are general suggestions assuming you might be interested in the complementarity-determining region (CDR) of the heavy chain (\'H:55\') and light chain (\'L:100\'), although these can be refined based on specific antigen interactions or structural insights.\\n\\nFinally, \'diffusion_steps\', set to 200, provides a balance between computational efficiency and the thorough exploration of conformational space, which is especially important in achieving a stable design with complex structures like antibodies. This straightforward explanation aims to align technical decisions with their practical implications, ensuring the protein design process is both effective and efficient."", ""model"": ""tts-1"", ""voice"": ""alloy""}', 'provider': 'openai', 'system': 'openai'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""input"": ""Let\'s break down how these parameters would work in RFdiffusion for designing with the given PDB structure, which is an intact human immunoglobulin G (IgG) antibody. The \'contigs\' parameter specifies the range of amino acid segments to be considered in the design. Here, \'A20-60/0 50-100\' indicates we\'re targeting a segment that spans residues 20 to 60 in a chain, and also focusing on a possible flexible region or linker of lengths 50 to 100. This allows us to consider both the specificity needed for binding, given the nature of antibodies, and an acceptable range of binder length variations.\\n\\nFor \'hotspot_res\', I\'ve chosen residues based on typical locations of antibody-antigen interactions. These are general suggestions assuming you might be interested in the complementarity-determining region (CDR) of the heavy chain (\'H:55\') and light chain (\'L:100\'), although these can be refined based on specific antigen interactions or structural insights.\\n\\nFinally, \'diffusion_steps\', set to 200, provides a balance between computational efficiency and the thorough exploration of conformational space, which is especially important in achieving a stable design with complex structures like antibodies. This straightforward explanation aims to align technical decisions with their practical implications, ensuring the protein design process is both effective and efficient."", ""model"": ""tts-1"", ""voice"": ""alloy""}'}",{},"Let's break down how these parameters would work in RFdiffusion for designing with the given PDB structure, which is an intact human immunoglobulin G (IgG) antibody. The 'contigs' parameter specifies the range of amino acid segments to be considered in the design. Here, 'A20-60/0 50-100' indicates we're targeting a segment that spans residues 20 to 60 in a chain, and also focusing on a possible flexible region or linker of lengths 50 to 100. This allows us to consider both the specificity needed for binding, given the nature of antibodies, and an acceptable range of binder length variations.

For 'hotspot_res', I've chosen residues based on typical locations of antibody-antigen interactions. These are general suggestions assuming you might be interested in the complementarity-determining region (CDR) of the heavy chain ('H:55') and light chain ('L:100'), although these can be refined based on specific antigen interactions or structural insights.

Finally, 'diffusion_steps', set to 200, provides a balance between computational efficiency and the thorough exploration of conformational space, which is especially important in achieving a stable design with complex structures like antibodies. This straightforward explanation aims to align technical decisions with their practical implications, ensuring the protein design process is both effective and efficient.",tts-1,alloy,<openai._legacy_response.HttpxBinaryResponseContent object at 0x7a7456e7e140>,
RGF0YXNldEV4YW1wbGU6MTEy,"[{'role': 'system', 'content': ""You are an expert in RFdiffusion parameter optimization.\n                Always ensure contigs parameter has both target specification AND binder length range after /0.\n                Example: 'A20-60/0 50-100' NOT just 'A20-60/0'""}, {'role': 'user', 'content': '\n        Analyze this PDB content and suggest basic parameters for RFdiffusion that will work reliably.\n        Follow this exact format in your response:\n\n        PARAMETERS:\n        1. contigs: [value]\n        2. hotspot_res: [value]\n        3. diffusion_steps: [value]\n\n        EXPLANATION:\n        Provide a clear, conversational explanation of why these parameters were chosen and how they will affect the protein design process.\n        Keep the explanation natural and suitable for voice output.\n\n        Base the analysis on this PDB structure:\n        HEADER    HYDROLASE                               19-MAY-97   1AKI              \nTITLE     THE STRUCTURE OF THE ORTHORHOMBIC FORM OF HEN EGG-WHITE LYSOZYME AT   \nTITLE    2 1.5 ANGSTROMS RESOLUTION                                             \nCOMPND    MOL_ID: 1;                                                            \nCOMPND   2 MOLECULE: LYSOZYME;                                                  \nCOMPND   3 CHAIN: A;                                                            \nCOMPND   4 EC:...\n        '}]","[{'role': 'assistant', 'content': ""PARAMETERS:\n1. contigs: A1-129/0 20-40\n2. hotspot_res: 35, 52, 66\n3. diffusion_steps: 1000\n\nEXPLANATION:\nGiven the PDB structure of hen egg-white lysozyme, a well-studied enzyme, we need to choose parameters that accommodate the entire protein while providing flexibility for potential modifications. The lysozyme has 129 amino acids. For contigs, we have set 'A1-129/0 20-40'. The first part 'A1-129' represents the full sequence of the chain, ensuring the target is covered entirely from start to finish. The '/0' represents that we are not specifying any chain break, and '20-40' allows for a flexible design binder length, which facilitates incorporating significant modifications or additional functional segments ranging from 20 to 40 amino acids.\n\nFor hotspot residues, we choose key active site and structurally crucial residues such as 35, 52, and 66, which are often involved in the protein's enzymatic activity or stability. By setting these as hotspots, we aim to preserve or enhance important functional sites during diffusion-based design.\n\nDiffusion steps are set to 1000, providing sufficient iterations to explore a broad conformational space for substantial sequence and structural optimization. This step count balances computational efficiency with the need for thorough sampling, enabling the preservation of essential enzymatic characteristics while allowing room for innovative enhancements.\n\nOverall, these parameters are aimed at reliably probing new structural variants of lysozyme while maintaining its biological function and ensuring flexibility for computational design applications.""}]","{'mime_type': 'application/json', 'value': '{""id"":""chatcmpl-AReLN7XhejmmzHbM4sXsEFSDl7imQ"",""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""message"":{""content"":""PARAMETERS:\\n1. contigs: A1-129/0 20-40\\n2. hotspot_res: 35, 52, 66\\n3. diffusion_steps: 1000\\n\\nEXPLANATION:\\nGiven the PDB structure of hen egg-white lysozyme, a well-studied enzyme, we need to choose parameters that accommodate the entire protein while providing flexibility for potential modifications. The lysozyme has 129 amino acids. For contigs, we have set \'A1-129/0 20-40\'. The first part \'A1-129\' represents the full sequence of the chain, ensuring the target is covered entirely from start to finish. The \'/0\' represents that we are not specifying any chain break, and \'20-40\' allows for a flexible design binder length, which facilitates incorporating significant modifications or additional functional segments ranging from 20 to 40 amino acids.\\n\\nFor hotspot residues, we choose key active site and structurally crucial residues such as 35, 52, and 66, which are often involved in the protein\'s enzymatic activity or stability. By setting these as hotspots, we aim to preserve or enhance important functional sites during diffusion-based design.\\n\\nDiffusion steps are set to 1000, providing sufficient iterations to explore a broad conformational space for substantial sequence and structural optimization. This step count balances computational efficiency with the need for thorough sampling, enabling the preservation of essential enzymatic characteristics while allowing room for innovative enhancements.\\n\\nOverall, these parameters are aimed at reliably probing new structural variants of lysozyme while maintaining its biological function and ensuring flexibility for computational design applications."",""refusal"":null,""role"":""assistant""}}],""created"":1731153301,""model"":""gpt-4o-2024-08-06"",""object"":""chat.completion"",""system_fingerprint"":""fp_45cf54deae"",""usage"":{""completion_tokens"":331,""prompt_tokens"":294,""total_tokens"":625,""completion_tokens_details"":{""accepted_prediction_tokens"":0,""audio_tokens"":0,""reasoning_tokens"":0,""rejected_prediction_tokens"":0},""prompt_tokens_details"":{""audio_tokens"":0,""cached_tokens"":0}}}'}","{'invocation_parameters': '{""model"": ""gpt-4o""}', 'input_messages': [{'message': {'content': ""You are an expert in RFdiffusion parameter optimization.\n                Always ensure contigs parameter has both target specification AND binder length range after /0.\n                Example: 'A20-60/0 50-100' NOT just 'A20-60/0'"", 'role': 'system'}}, {'message': {'content': '\n        Analyze this PDB content and suggest basic parameters for RFdiffusion that will work reliably.\n        Follow this exact format in your response:\n\n        PARAMETERS:\n        1. contigs: [value]\n        2. hotspot_res: [value]\n        3. diffusion_steps: [value]\n\n        EXPLANATION:\n        Provide a clear, conversational explanation of why these parameters were chosen and how they will affect the protein design process.\n        Keep the explanation natural and suitable for voice output.\n\n        Base the analysis on this PDB structure:\n        HEADER    HYDROLASE                               19-MAY-97   1AKI              \nTITLE     THE STRUCTURE OF THE ORTHORHOMBIC FORM OF HEN EGG-WHITE LYSOZYME AT   \nTITLE    2 1.5 ANGSTROMS RESOLUTION                                             \nCOMPND    MOL_ID: 1;                                                            \nCOMPND   2 MOLECULE: LYSOZYME;                                                  \nCOMPND   3 CHAIN: A;                                                            \nCOMPND   4 EC:...\n        ', 'role': 'user'}}], 'token_count': {'total': 625, 'prompt': 294, 'completion': 331}, 'system': 'openai', 'provider': 'openai', 'output_messages': [{'message': {'content': ""PARAMETERS:\n1. contigs: A1-129/0 20-40\n2. hotspot_res: 35, 52, 66\n3. diffusion_steps: 1000\n\nEXPLANATION:\nGiven the PDB structure of hen egg-white lysozyme, a well-studied enzyme, we need to choose parameters that accommodate the entire protein while providing flexibility for potential modifications. The lysozyme has 129 amino acids. For contigs, we have set 'A1-129/0 20-40'. The first part 'A1-129' represents the full sequence of the chain, ensuring the target is covered entirely from start to finish. The '/0' represents that we are not specifying any chain break, and '20-40' allows for a flexible design binder length, which facilitates incorporating significant modifications or additional functional segments ranging from 20 to 40 amino acids.\n\nFor hotspot residues, we choose key active site and structurally crucial residues such as 35, 52, and 66, which are often involved in the protein's enzymatic activity or stability. By setting these as hotspots, we aim to preserve or enhance important functional sites during diffusion-based design.\n\nDiffusion steps are set to 1000, providing sufficient iterations to explore a broad conformational space for substantial sequence and structural optimization. This step count balances computational efficiency with the need for thorough sampling, enabling the preservation of essential enzymatic characteristics while allowing room for innovative enhancements.\n\nOverall, these parameters are aimed at reliably probing new structural variants of lysozyme while maintaining its biological function and ensuring flexibility for computational design applications."", 'role': 'assistant'}}], 'model_name': 'gpt-4o-2024-08-06'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""messages"": [{""role"": ""system"", ""content"": ""You are an expert in RFdiffusion parameter optimization.\\n                Always ensure contigs parameter has both target specification AND binder length range after /0.\\n                Example: \'A20-60/0 50-100\' NOT just \'A20-60/0\'""}, {""role"": ""user"", ""content"": ""\\n        Analyze this PDB content and suggest basic parameters for RFdiffusion that will work reliably.\\n        Follow this exact format in your response:\\n\\n        PARAMETERS:\\n        1. contigs: [value]\\n        2. hotspot_res: [value]\\n        3. diffusion_steps: [value]\\n\\n        EXPLANATION:\\n        Provide a clear, conversational explanation of why these parameters were chosen and how they will affect the protein design process.\\n        Keep the explanation natural and suitable for voice output.\\n\\n        Base the analysis on this PDB structure:\\n        HEADER    HYDROLASE                               19-MAY-97   1AKI              \\nTITLE     THE STRUCTURE OF THE ORTHORHOMBIC FORM OF HEN EGG-WHITE LYSOZYME AT   \\nTITLE    2 1.5 ANGSTROMS RESOLUTION                                             \\nCOMPND    MOL_ID: 1;                                                            \\nCOMPND   2 MOLECULE: LYSOZYME;                                                  \\nCOMPND   3 CHAIN: A;                                                            \\nCOMPND   4 EC:...\\n        ""}], ""model"": ""gpt-4o""}'}",{},,,,,
RGF0YXNldEV4YW1wbGU6MTEz,,,{'value': '<openai._legacy_response.HttpxBinaryResponseContent object at 0x7a745637c130>'},"{'invocation_parameters': '{""input"": ""Given the PDB structure of hen egg-white lysozyme, a well-studied enzyme, we need to choose parameters that accommodate the entire protein while providing flexibility for potential modifications. The lysozyme has 129 amino acids. For contigs, we have set \'A1-129/0 20-40\'. The first part \'A1-129\' represents the full sequence of the chain, ensuring the target is covered entirely from start to finish. The \'/0\' represents that we are not specifying any chain break, and \'20-40\' allows for a flexible design binder length, which facilitates incorporating significant modifications or additional functional segments ranging from 20 to 40 amino acids.\\n\\nFor hotspot residues, we choose key active site and structurally crucial residues such as 35, 52, and 66, which are often involved in the protein\'s enzymatic activity or stability. By setting these as hotspots, we aim to preserve or enhance important functional sites during diffusion-based design.\\n\\nDiffusion steps are set to 1000, providing sufficient iterations to explore a broad conformational space for substantial sequence and structural optimization. This step count balances computational efficiency with the need for thorough sampling, enabling the preservation of essential enzymatic characteristics while allowing room for innovative enhancements.\\n\\nOverall, these parameters are aimed at reliably probing new structural variants of lysozyme while maintaining its biological function and ensuring flexibility for computational design applications."", ""model"": ""tts-1"", ""voice"": ""alloy""}', 'provider': 'openai', 'system': 'openai'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""input"": ""Given the PDB structure of hen egg-white lysozyme, a well-studied enzyme, we need to choose parameters that accommodate the entire protein while providing flexibility for potential modifications. The lysozyme has 129 amino acids. For contigs, we have set \'A1-129/0 20-40\'. The first part \'A1-129\' represents the full sequence of the chain, ensuring the target is covered entirely from start to finish. The \'/0\' represents that we are not specifying any chain break, and \'20-40\' allows for a flexible design binder length, which facilitates incorporating significant modifications or additional functional segments ranging from 20 to 40 amino acids.\\n\\nFor hotspot residues, we choose key active site and structurally crucial residues such as 35, 52, and 66, which are often involved in the protein\'s enzymatic activity or stability. By setting these as hotspots, we aim to preserve or enhance important functional sites during diffusion-based design.\\n\\nDiffusion steps are set to 1000, providing sufficient iterations to explore a broad conformational space for substantial sequence and structural optimization. This step count balances computational efficiency with the need for thorough sampling, enabling the preservation of essential enzymatic characteristics while allowing room for innovative enhancements.\\n\\nOverall, these parameters are aimed at reliably probing new structural variants of lysozyme while maintaining its biological function and ensuring flexibility for computational design applications."", ""model"": ""tts-1"", ""voice"": ""alloy""}'}",{},"Given the PDB structure of hen egg-white lysozyme, a well-studied enzyme, we need to choose parameters that accommodate the entire protein while providing flexibility for potential modifications. The lysozyme has 129 amino acids. For contigs, we have set 'A1-129/0 20-40'. The first part 'A1-129' represents the full sequence of the chain, ensuring the target is covered entirely from start to finish. The '/0' represents that we are not specifying any chain break, and '20-40' allows for a flexible design binder length, which facilitates incorporating significant modifications or additional functional segments ranging from 20 to 40 amino acids.

For hotspot residues, we choose key active site and structurally crucial residues such as 35, 52, and 66, which are often involved in the protein's enzymatic activity or stability. By setting these as hotspots, we aim to preserve or enhance important functional sites during diffusion-based design.

Diffusion steps are set to 1000, providing sufficient iterations to explore a broad conformational space for substantial sequence and structural optimization. This step count balances computational efficiency with the need for thorough sampling, enabling the preservation of essential enzymatic characteristics while allowing room for innovative enhancements.

Overall, these parameters are aimed at reliably probing new structural variants of lysozyme while maintaining its biological function and ensuring flexibility for computational design applications.",tts-1,alloy,<openai._legacy_response.HttpxBinaryResponseContent object at 0x7a745637c130>,
RGF0YXNldEV4YW1wbGU6MTE0,"[{'role': 'system', 'content': ""You are an expert in RFdiffusion parameter optimization. \n                Always ensure contigs parameter has both target specification AND binder length range after /0.\n                Example: 'A20-60/0 50-100' NOT just 'A20-60/0'""}, {'role': 'user', 'content': ""\n        Analyze this PDB content and suggest basic parameters for RFdiffusion that will work reliably.\n        Only provide the following required parameters:\n        1. contigs: Must be in format 'chainID-range/0 binder_length_range'. \n           Example: 'A20-60/0 50-100' means:\n           - Keep amino acids 20-60 in Chain A as target\n           - /0 marks the end of target chain\n           - Generate a binder protein of length 50-100 amino acids\n        2. hotspot_res: List three sequential residues from the middle of the chain\n        3. diffusion_steps: Use minimum value of 15 for faster results\n        \n        Keep the parameters simple and conservative to ensure API success.\n        Base the analysis on this PDB structure:\n        HEADER    HYDROLASE                               19-MAY-97   1AKI              \nTITLE     THE STRUCTURE OF THE ORTHORHOMBIC FORM OF HEN EGG-WHITE LYSOZYME AT   \nTITLE    2 1.5 ANGSTROMS RESOLUTION                                             \nCOMPND    MOL_ID: 1;                                                            \nCOMPND   2 MOLECULE: LYSOZYME;                                                  \nCOMPND   3 CHAIN: A;                                                            \nCOMPND   4 EC:...\n        ""}]","[{'role': 'assistant', 'content': ""Based on the provided PDB structure information for hen egg-white lysozyme (PDB ID: 1AKI) with a single Chain A, here are the suggested parameters for RFdiffusion:\n\n1. **contigs**: 'A30-60/0 50-100'\n   - Explanation: Amino acids 30-60 in Chain A are chosen for target specification as they represent a portion of the protein. A binder length range of 50-100 amino acids is provided for flexibility and reliability.\n\n2. **hotspot_res**: 'A45, A46, A47'\n   - Explanation: Residues 45 to 47 from Chain A are selected as hotspot residues since they are approximately central, providing a balanced context for binding.\n\n3. **diffusion_steps**: 15\n   - Explanation: Using the minimum value of 15 for faster computational results while ensuring adequate processing for reliable outputs.\n\nThese parameters are set to maintain simplicity and conservativeness to maximize the chances of successful diffusion.""}]","{'mime_type': 'application/json', 'value': '{""id"":""chatcmpl-AR7b5wvmWlzgwntNnr9LZb1ZYRxVp"",""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""message"":{""content"":""Based on the provided PDB structure information for hen egg-white lysozyme (PDB ID: 1AKI) with a single Chain A, here are the suggested parameters for RFdiffusion:\\n\\n1. **contigs**: \'A30-60/0 50-100\'\\n   - Explanation: Amino acids 30-60 in Chain A are chosen for target specification as they represent a portion of the protein. A binder length range of 50-100 amino acids is provided for flexibility and reliability.\\n\\n2. **hotspot_res**: \'A45, A46, A47\'\\n   - Explanation: Residues 45 to 47 from Chain A are selected as hotspot residues since they are approximately central, providing a balanced context for binding.\\n\\n3. **diffusion_steps**: 15\\n   - Explanation: Using the minimum value of 15 for faster computational results while ensuring adequate processing for reliable outputs.\\n\\nThese parameters are set to maintain simplicity and conservativeness to maximize the chances of successful diffusion."",""refusal"":null,""role"":""assistant""}}],""created"":1731027423,""model"":""gpt-4o-2024-08-06"",""object"":""chat.completion"",""system_fingerprint"":""fp_45cf54deae"",""usage"":{""completion_tokens"":209,""prompt_tokens"":349,""total_tokens"":558,""completion_tokens_details"":{""accepted_prediction_tokens"":0,""audio_tokens"":0,""reasoning_tokens"":0,""rejected_prediction_tokens"":0},""prompt_tokens_details"":{""audio_tokens"":0,""cached_tokens"":0}}}'}","{'invocation_parameters': '{""model"": ""gpt-4o""}', 'input_messages': [{'message': {'content': ""You are an expert in RFdiffusion parameter optimization. \n                Always ensure contigs parameter has both target specification AND binder length range after /0.\n                Example: 'A20-60/0 50-100' NOT just 'A20-60/0'"", 'role': 'system'}}, {'message': {'content': ""\n        Analyze this PDB content and suggest basic parameters for RFdiffusion that will work reliably.\n        Only provide the following required parameters:\n        1. contigs: Must be in format 'chainID-range/0 binder_length_range'. \n           Example: 'A20-60/0 50-100' means:\n           - Keep amino acids 20-60 in Chain A as target\n           - /0 marks the end of target chain\n           - Generate a binder protein of length 50-100 amino acids\n        2. hotspot_res: List three sequential residues from the middle of the chain\n        3. diffusion_steps: Use minimum value of 15 for faster results\n        \n        Keep the parameters simple and conservative to ensure API success.\n        Base the analysis on this PDB structure:\n        HEADER    HYDROLASE                               19-MAY-97   1AKI              \nTITLE     THE STRUCTURE OF THE ORTHORHOMBIC FORM OF HEN EGG-WHITE LYSOZYME AT   \nTITLE    2 1.5 ANGSTROMS RESOLUTION                                             \nCOMPND    MOL_ID: 1;                                                            \nCOMPND   2 MOLECULE: LYSOZYME;                                                  \nCOMPND   3 CHAIN: A;                                                            \nCOMPND   4 EC:...\n        "", 'role': 'user'}}], 'token_count': {'total': 558, 'prompt': 349, 'completion': 209}, 'system': 'openai', 'provider': 'openai', 'output_messages': [{'message': {'content': ""Based on the provided PDB structure information for hen egg-white lysozyme (PDB ID: 1AKI) with a single Chain A, here are the suggested parameters for RFdiffusion:\n\n1. **contigs**: 'A30-60/0 50-100'\n   - Explanation: Amino acids 30-60 in Chain A are chosen for target specification as they represent a portion of the protein. A binder length range of 50-100 amino acids is provided for flexibility and reliability.\n\n2. **hotspot_res**: 'A45, A46, A47'\n   - Explanation: Residues 45 to 47 from Chain A are selected as hotspot residues since they are approximately central, providing a balanced context for binding.\n\n3. **diffusion_steps**: 15\n   - Explanation: Using the minimum value of 15 for faster computational results while ensuring adequate processing for reliable outputs.\n\nThese parameters are set to maintain simplicity and conservativeness to maximize the chances of successful diffusion."", 'role': 'assistant'}}], 'model_name': 'gpt-4o-2024-08-06'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""messages"": [{""role"": ""system"", ""content"": ""You are an expert in RFdiffusion parameter optimization. \\n                Always ensure contigs parameter has both target specification AND binder length range after /0.\\n                Example: \'A20-60/0 50-100\' NOT just \'A20-60/0\'""}, {""role"": ""user"", ""content"": ""\\n        Analyze this PDB content and suggest basic parameters for RFdiffusion that will work reliably.\\n        Only provide the following required parameters:\\n        1. contigs: Must be in format \'chainID-range/0 binder_length_range\'. \\n           Example: \'A20-60/0 50-100\' means:\\n           - Keep amino acids 20-60 in Chain A as target\\n           - /0 marks the end of target chain\\n           - Generate a binder protein of length 50-100 amino acids\\n        2. hotspot_res: List three sequential residues from the middle of the chain\\n        3. diffusion_steps: Use minimum value of 15 for faster results\\n        \\n        Keep the parameters simple and conservative to ensure API success.\\n        Base the analysis on this PDB structure:\\n        HEADER    HYDROLASE                               19-MAY-97   1AKI              \\nTITLE     THE STRUCTURE OF THE ORTHORHOMBIC FORM OF HEN EGG-WHITE LYSOZYME AT   \\nTITLE    2 1.5 ANGSTROMS RESOLUTION                                             \\nCOMPND    MOL_ID: 1;                                                            \\nCOMPND   2 MOLECULE: LYSOZYME;                                                  \\nCOMPND   3 CHAIN: A;                                                            \\nCOMPND   4 EC:...\\n        ""}], ""model"": ""gpt-4o""}'}",{},,,,,
RGF0YXNldEV4YW1wbGU6MTE1,"[{'role': 'system', 'content': 'You are an expert in ProteinMPNN modeling.'}, {'role': 'user', 'content': '\n        Analyze the following PDB content and suggest appropriate parameters for running ProteinMPNN:\n\n        ATOM      1  N   GLY A   1      28.387   8.876  22.027  1.00  0.00\nATOM      2  CA  GLY A   1      27.477   8.701  20.902  1.00  0.00\nATOM      3  C   GLY A   1      28.219   8.221  19.662  1.00  0.00\nATOM      4  O   GLY A   1      27.682   7.453  18.863  1.00  0.00\nATOM      5  N   GLY A   2      29.476   8.450  19.612  1.00  0.00\nATOM      6  CA  GLY A   2      30.310   8.036  18.490  1.00  0.00\nATOM      7  C   GLY A   2      30.502   6.525  18.474  1.00  0.00\nATOM      8  O   GLY A   2     ...\n\n        Provide the following parameters:\n        1. ca_only\n        2. use_soluble_model\n        3. num_seq_per_target\n        4. sampling_temp\n\n        Explain your reasoning for each parameter.\n        '}]","[{'role': 'assistant', 'content': ""To suggest appropriate parameters for running ProteinMPNN on the provided PDB content, let's examine each parameter individually and consider the nature of the input data:\n\n1. **ca_only:**\n   - **Explanation:** The `ca_only` parameter is used when the input contains only C-alpha (CA) atoms rather than the full atom detail of the backbone (N, CA, C, O). By examining the provided PDB content, we see that it includes all standard backbone atoms (N, CA, C, O) for at least two residues.\n   - **Recommended Value:** `False` because the PDB data includes full backbone atoms and not just C-alpha atoms.\n\n2. **use_soluble_model:**\n   - **Explanation:** The choice between using a soluble model or a different model variant typically depends on the expected environment of the protein. If the protein is expected to function in a soluble environment (e.g., cytoplasm), the soluble model is advised.\n   - **Recommended Value:** Given that no specific contextual information about the protein's environment is present in the provided PDB file, a default assumption would be to choose a soluble protein model, which is a common scenario.\n   - **Recommended Value:** `True` to use the soluble model unless additional information suggests a membrane or insoluble context.\n\n3. **num_seq_per_target:**\n   - **Explanation:** This parameter specifies how many sequences you wish to sample for your target protein. The higher the number, the more diverse sequences you will generate, which can be useful for exploring sequence space.\n   - **Recommended Value:** The choice here is often dependent on computational resources and desired assessment depth. A moderate number like `10` can provide a decent diversity without being overly computationally expensive.\n   - **Recommended Value:** `10` as a balanced choice to sample multiple sequences.\n\n4. **sampling_temp:**\n   - **Explanation:** Sampling temperature affects the diversity of sequences generated. A higher temperature results in more diverse predictions, while a lower temperature favors sequences closer to the initial input.\n   - **Recommended Value:** A typical value is between `0.1` and `1.0`, where around `0.5` is often a balanced choice for moderate diversity without excessive deviation.\n   - **Recommended Value:** `0.5` for a balanced level of diversity in sequence prediction.\n\nIn summary, based on the general context and analysis of the PDB file, the recommended parameters for ProteinMPNN are:\n- `ca_only = False`\n- `use_soluble_model = True`\n- `num_seq_per_target = 10`\n- `sampling_temp = 0.5`""}]","{'mime_type': 'application/json', 'value': '{""id"":""chatcmpl-AR7iqIp2a7cAlRDwmLKXEx2rKwn2D"",""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""message"":{""content"":""To suggest appropriate parameters for running ProteinMPNN on the provided PDB content, let\'s examine each parameter individually and consider the nature of the input data:\\n\\n1. **ca_only:**\\n   - **Explanation:** The `ca_only` parameter is used when the input contains only C-alpha (CA) atoms rather than the full atom detail of the backbone (N, CA, C, O). By examining the provided PDB content, we see that it includes all standard backbone atoms (N, CA, C, O) for at least two residues.\\n   - **Recommended Value:** `False` because the PDB data includes full backbone atoms and not just C-alpha atoms.\\n\\n2. **use_soluble_model:**\\n   - **Explanation:** The choice between using a soluble model or a different model variant typically depends on the expected environment of the protein. If the protein is expected to function in a soluble environment (e.g., cytoplasm), the soluble model is advised.\\n   - **Recommended Value:** Given that no specific contextual information about the protein\'s environment is present in the provided PDB file, a default assumption would be to choose a soluble protein model, which is a common scenario.\\n   - **Recommended Value:** `True` to use the soluble model unless additional information suggests a membrane or insoluble context.\\n\\n3. **num_seq_per_target:**\\n   - **Explanation:** This parameter specifies how many sequences you wish to sample for your target protein. The higher the number, the more diverse sequences you will generate, which can be useful for exploring sequence space.\\n   - **Recommended Value:** The choice here is often dependent on computational resources and desired assessment depth. A moderate number like `10` can provide a decent diversity without being overly computationally expensive.\\n   - **Recommended Value:** `10` as a balanced choice to sample multiple sequences.\\n\\n4. **sampling_temp:**\\n   - **Explanation:** Sampling temperature affects the diversity of sequences generated. A higher temperature results in more diverse predictions, while a lower temperature favors sequences closer to the initial input.\\n   - **Recommended Value:** A typical value is between `0.1` and `1.0`, where around `0.5` is often a balanced choice for moderate diversity without excessive deviation.\\n   - **Recommended Value:** `0.5` for a balanced level of diversity in sequence prediction.\\n\\nIn summary, based on the general context and analysis of the PDB file, the recommended parameters for ProteinMPNN are:\\n- `ca_only = False`\\n- `use_soluble_model = True`\\n- `num_seq_per_target = 10`\\n- `sampling_temp = 0.5`"",""refusal"":null,""role"":""assistant""}}],""created"":1731027904,""model"":""gpt-4o-2024-08-06"",""object"":""chat.completion"",""system_fingerprint"":""fp_45cf54deae"",""usage"":{""completion_tokens"":538,""prompt_tokens"":376,""total_tokens"":914,""completion_tokens_details"":{""accepted_prediction_tokens"":0,""audio_tokens"":0,""reasoning_tokens"":0,""rejected_prediction_tokens"":0},""prompt_tokens_details"":{""audio_tokens"":0,""cached_tokens"":0}}}'}","{'invocation_parameters': '{""model"": ""gpt-4o""}', 'input_messages': [{'message': {'content': 'You are an expert in ProteinMPNN modeling.', 'role': 'system'}}, {'message': {'content': '\n        Analyze the following PDB content and suggest appropriate parameters for running ProteinMPNN:\n\n        ATOM      1  N   GLY A   1      28.387   8.876  22.027  1.00  0.00\nATOM      2  CA  GLY A   1      27.477   8.701  20.902  1.00  0.00\nATOM      3  C   GLY A   1      28.219   8.221  19.662  1.00  0.00\nATOM      4  O   GLY A   1      27.682   7.453  18.863  1.00  0.00\nATOM      5  N   GLY A   2      29.476   8.450  19.612  1.00  0.00\nATOM      6  CA  GLY A   2      30.310   8.036  18.490  1.00  0.00\nATOM      7  C   GLY A   2      30.502   6.525  18.474  1.00  0.00\nATOM      8  O   GLY A   2     ...\n\n        Provide the following parameters:\n        1. ca_only\n        2. use_soluble_model\n        3. num_seq_per_target\n        4. sampling_temp\n\n        Explain your reasoning for each parameter.\n        ', 'role': 'user'}}], 'token_count': {'total': 914, 'prompt': 376, 'completion': 538}, 'system': 'openai', 'provider': 'openai', 'output_messages': [{'message': {'content': ""To suggest appropriate parameters for running ProteinMPNN on the provided PDB content, let's examine each parameter individually and consider the nature of the input data:\n\n1. **ca_only:**\n   - **Explanation:** The `ca_only` parameter is used when the input contains only C-alpha (CA) atoms rather than the full atom detail of the backbone (N, CA, C, O). By examining the provided PDB content, we see that it includes all standard backbone atoms (N, CA, C, O) for at least two residues.\n   - **Recommended Value:** `False` because the PDB data includes full backbone atoms and not just C-alpha atoms.\n\n2. **use_soluble_model:**\n   - **Explanation:** The choice between using a soluble model or a different model variant typically depends on the expected environment of the protein. If the protein is expected to function in a soluble environment (e.g., cytoplasm), the soluble model is advised.\n   - **Recommended Value:** Given that no specific contextual information about the protein's environment is present in the provided PDB file, a default assumption would be to choose a soluble protein model, which is a common scenario.\n   - **Recommended Value:** `True` to use the soluble model unless additional information suggests a membrane or insoluble context.\n\n3. **num_seq_per_target:**\n   - **Explanation:** This parameter specifies how many sequences you wish to sample for your target protein. The higher the number, the more diverse sequences you will generate, which can be useful for exploring sequence space.\n   - **Recommended Value:** The choice here is often dependent on computational resources and desired assessment depth. A moderate number like `10` can provide a decent diversity without being overly computationally expensive.\n   - **Recommended Value:** `10` as a balanced choice to sample multiple sequences.\n\n4. **sampling_temp:**\n   - **Explanation:** Sampling temperature affects the diversity of sequences generated. A higher temperature results in more diverse predictions, while a lower temperature favors sequences closer to the initial input.\n   - **Recommended Value:** A typical value is between `0.1` and `1.0`, where around `0.5` is often a balanced choice for moderate diversity without excessive deviation.\n   - **Recommended Value:** `0.5` for a balanced level of diversity in sequence prediction.\n\nIn summary, based on the general context and analysis of the PDB file, the recommended parameters for ProteinMPNN are:\n- `ca_only = False`\n- `use_soluble_model = True`\n- `num_seq_per_target = 10`\n- `sampling_temp = 0.5`"", 'role': 'assistant'}}], 'model_name': 'gpt-4o-2024-08-06'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""messages"": [{""role"": ""system"", ""content"": ""You are an expert in ProteinMPNN modeling.""}, {""role"": ""user"", ""content"": ""\\n        Analyze the following PDB content and suggest appropriate parameters for running ProteinMPNN:\\n\\n        ATOM      1  N   GLY A   1      28.387   8.876  22.027  1.00  0.00\\nATOM      2  CA  GLY A   1      27.477   8.701  20.902  1.00  0.00\\nATOM      3  C   GLY A   1      28.219   8.221  19.662  1.00  0.00\\nATOM      4  O   GLY A   1      27.682   7.453  18.863  1.00  0.00\\nATOM      5  N   GLY A   2      29.476   8.450  19.612  1.00  0.00\\nATOM      6  CA  GLY A   2      30.310   8.036  18.490  1.00  0.00\\nATOM      7  C   GLY A   2      30.502   6.525  18.474  1.00  0.00\\nATOM      8  O   GLY A   2     ...\\n\\n        Provide the following parameters:\\n        1. ca_only\\n        2. use_soluble_model\\n        3. num_seq_per_target\\n        4. sampling_temp\\n\\n        Explain your reasoning for each parameter.\\n        ""}], ""model"": ""gpt-4o""}'}",{},,,,,
RGF0YXNldEV4YW1wbGU6MTE2,"[{'role': 'system', 'content': 'You are an expert in ProteinMPNN modeling.'}, {'role': 'user', 'content': '\n        Analyze the following PDB content and suggest appropriate parameters for running ProteinMPNN:\n\n        ATOM      1  N   GLY A   1      28.387   8.876  22.027  1.00  0.00\nATOM      2  CA  GLY A   1      27.477   8.701  20.902  1.00  0.00\nATOM      3  C   GLY A   1      28.219   8.221  19.662  1.00  0.00\nATOM      4  O   GLY A   1      27.682   7.453  18.863  1.00  0.00\nATOM      5  N   GLY A   2      29.476   8.450  19.612  1.00  0.00\nATOM      6  CA  GLY A   2      30.310   8.036  18.490  1.00  0.00\nATOM      7  C   GLY A   2      30.502   6.525  18.474  1.00  0.00\nATOM      8  O   GLY A   2     ...\n\n        Provide the following parameters:\n        1. ca_only\n        2. use_soluble_model\n        3. num_seq_per_target\n        4. sampling_temp\n\n        Explain your reasoning for each parameter.\n        '}]","[{'role': 'assistant', 'content': ""To suggest appropriate parameters for running ProteinMPNN on the provided PDB content, it's crucial to understand the context and intent of using ProteinMPNN. Given that we have only a partial list of residues and details of two consecutive glycine residues, here are some suggested parameters and their rationale:\n\n1. **ca_only**: `False`\n   - **Rationale**: The provided PDB content includes all backbone atoms (N, CA, C, and O) for glycine residues. The `ca_only` option is typically used when the PDB file only includes alpha carbon (CA) positions and not the full backbone. Since you have the full backbone available, it's preferable to use the complete information to achieve more accurate modeling.\n\n2. **use_soluble_model**: `True` or `False`\n   - **Rationale**: The choice of using a soluble model depends on whether the protein or peptide in this PDB file is soluble. Glycine residues are flexible and are often found in loops or flexible regions. Without additional context, it's not clear if this is part of a soluble protein or part of a membrane-associated structure. If you're modeling a typical soluble protein, set this to `True`. If you know it's membrane-associated, opt for `False`.\n\n3. **num_seq_per_target**: `10` (or adjust based on computational resources and needs)\n   - **Rationale**: The number of sequences per target depends on the diversity and quantity of designed sequences you want. A setting of `10` is a reasonable starting point as it provides diversity without excessive computational demand. You may increase this number if you have specific requirements for exploring a wider variety of sequences.\n\n4. **sampling_temp**: `0.1` to `0.5`\n   - **Rationale**: Sampling temperature affects the diversity and convergence of the sequences generated. A lower temperature (closer to `0.1`) means more deterministic design with less variability among sequences. A higher temperature (`0.5` or higher) introduces more variability and allows exploration of a broader conformational space. Begin with a moderate range to balance diversity and convergence, adjusting based on design goals and observed results.\n\nThese parameters can be adjusted based on specific experimental needs or additional context about the protein's function, environment, or design objectives.""}]","{'mime_type': 'application/json', 'value': '{""id"":""chatcmpl-AR7j2tXJF1N68GTQzqSg7pbHnQEFa"",""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""message"":{""content"":""To suggest appropriate parameters for running ProteinMPNN on the provided PDB content, it\'s crucial to understand the context and intent of using ProteinMPNN. Given that we have only a partial list of residues and details of two consecutive glycine residues, here are some suggested parameters and their rationale:\\n\\n1. **ca_only**: `False`\\n   - **Rationale**: The provided PDB content includes all backbone atoms (N, CA, C, and O) for glycine residues. The `ca_only` option is typically used when the PDB file only includes alpha carbon (CA) positions and not the full backbone. Since you have the full backbone available, it\'s preferable to use the complete information to achieve more accurate modeling.\\n\\n2. **use_soluble_model**: `True` or `False`\\n   - **Rationale**: The choice of using a soluble model depends on whether the protein or peptide in this PDB file is soluble. Glycine residues are flexible and are often found in loops or flexible regions. Without additional context, it\'s not clear if this is part of a soluble protein or part of a membrane-associated structure. If you\'re modeling a typical soluble protein, set this to `True`. If you know it\'s membrane-associated, opt for `False`.\\n\\n3. **num_seq_per_target**: `10` (or adjust based on computational resources and needs)\\n   - **Rationale**: The number of sequences per target depends on the diversity and quantity of designed sequences you want. A setting of `10` is a reasonable starting point as it provides diversity without excessive computational demand. You may increase this number if you have specific requirements for exploring a wider variety of sequences.\\n\\n4. **sampling_temp**: `0.1` to `0.5`\\n   - **Rationale**: Sampling temperature affects the diversity and convergence of the sequences generated. A lower temperature (closer to `0.1`) means more deterministic design with less variability among sequences. A higher temperature (`0.5` or higher) introduces more variability and allows exploration of a broader conformational space. Begin with a moderate range to balance diversity and convergence, adjusting based on design goals and observed results.\\n\\nThese parameters can be adjusted based on specific experimental needs or additional context about the protein\'s function, environment, or design objectives."",""refusal"":null,""role"":""assistant""}}],""created"":1731027916,""model"":""gpt-4o-2024-08-06"",""object"":""chat.completion"",""system_fingerprint"":""fp_45cf54deae"",""usage"":{""completion_tokens"":470,""prompt_tokens"":376,""total_tokens"":846,""completion_tokens_details"":{""accepted_prediction_tokens"":0,""audio_tokens"":0,""reasoning_tokens"":0,""rejected_prediction_tokens"":0},""prompt_tokens_details"":{""audio_tokens"":0,""cached_tokens"":0}}}'}","{'invocation_parameters': '{""model"": ""gpt-4o""}', 'input_messages': [{'message': {'content': 'You are an expert in ProteinMPNN modeling.', 'role': 'system'}}, {'message': {'content': '\n        Analyze the following PDB content and suggest appropriate parameters for running ProteinMPNN:\n\n        ATOM      1  N   GLY A   1      28.387   8.876  22.027  1.00  0.00\nATOM      2  CA  GLY A   1      27.477   8.701  20.902  1.00  0.00\nATOM      3  C   GLY A   1      28.219   8.221  19.662  1.00  0.00\nATOM      4  O   GLY A   1      27.682   7.453  18.863  1.00  0.00\nATOM      5  N   GLY A   2      29.476   8.450  19.612  1.00  0.00\nATOM      6  CA  GLY A   2      30.310   8.036  18.490  1.00  0.00\nATOM      7  C   GLY A   2      30.502   6.525  18.474  1.00  0.00\nATOM      8  O   GLY A   2     ...\n\n        Provide the following parameters:\n        1. ca_only\n        2. use_soluble_model\n        3. num_seq_per_target\n        4. sampling_temp\n\n        Explain your reasoning for each parameter.\n        ', 'role': 'user'}}], 'token_count': {'total': 846, 'prompt': 376, 'completion': 470}, 'system': 'openai', 'provider': 'openai', 'output_messages': [{'message': {'content': ""To suggest appropriate parameters for running ProteinMPNN on the provided PDB content, it's crucial to understand the context and intent of using ProteinMPNN. Given that we have only a partial list of residues and details of two consecutive glycine residues, here are some suggested parameters and their rationale:\n\n1. **ca_only**: `False`\n   - **Rationale**: The provided PDB content includes all backbone atoms (N, CA, C, and O) for glycine residues. The `ca_only` option is typically used when the PDB file only includes alpha carbon (CA) positions and not the full backbone. Since you have the full backbone available, it's preferable to use the complete information to achieve more accurate modeling.\n\n2. **use_soluble_model**: `True` or `False`\n   - **Rationale**: The choice of using a soluble model depends on whether the protein or peptide in this PDB file is soluble. Glycine residues are flexible and are often found in loops or flexible regions. Without additional context, it's not clear if this is part of a soluble protein or part of a membrane-associated structure. If you're modeling a typical soluble protein, set this to `True`. If you know it's membrane-associated, opt for `False`.\n\n3. **num_seq_per_target**: `10` (or adjust based on computational resources and needs)\n   - **Rationale**: The number of sequences per target depends on the diversity and quantity of designed sequences you want. A setting of `10` is a reasonable starting point as it provides diversity without excessive computational demand. You may increase this number if you have specific requirements for exploring a wider variety of sequences.\n\n4. **sampling_temp**: `0.1` to `0.5`\n   - **Rationale**: Sampling temperature affects the diversity and convergence of the sequences generated. A lower temperature (closer to `0.1`) means more deterministic design with less variability among sequences. A higher temperature (`0.5` or higher) introduces more variability and allows exploration of a broader conformational space. Begin with a moderate range to balance diversity and convergence, adjusting based on design goals and observed results.\n\nThese parameters can be adjusted based on specific experimental needs or additional context about the protein's function, environment, or design objectives."", 'role': 'assistant'}}], 'model_name': 'gpt-4o-2024-08-06'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""messages"": [{""role"": ""system"", ""content"": ""You are an expert in ProteinMPNN modeling.""}, {""role"": ""user"", ""content"": ""\\n        Analyze the following PDB content and suggest appropriate parameters for running ProteinMPNN:\\n\\n        ATOM      1  N   GLY A   1      28.387   8.876  22.027  1.00  0.00\\nATOM      2  CA  GLY A   1      27.477   8.701  20.902  1.00  0.00\\nATOM      3  C   GLY A   1      28.219   8.221  19.662  1.00  0.00\\nATOM      4  O   GLY A   1      27.682   7.453  18.863  1.00  0.00\\nATOM      5  N   GLY A   2      29.476   8.450  19.612  1.00  0.00\\nATOM      6  CA  GLY A   2      30.310   8.036  18.490  1.00  0.00\\nATOM      7  C   GLY A   2      30.502   6.525  18.474  1.00  0.00\\nATOM      8  O   GLY A   2     ...\\n\\n        Provide the following parameters:\\n        1. ca_only\\n        2. use_soluble_model\\n        3. num_seq_per_target\\n        4. sampling_temp\\n\\n        Explain your reasoning for each parameter.\\n        ""}], ""model"": ""gpt-4o""}'}",{},,,,,
RGF0YXNldEV4YW1wbGU6MTE3,"[{'role': 'system', 'content': ""You are an expert in RFdiffusion parameter optimization. \n                Always ensure contigs parameter has both target specification AND binder length range after /0.\n                Example: 'A20-60/0 50-100' NOT just 'A20-60/0'""}, {'role': 'user', 'content': ""\n        Analyze this PDB content and suggest basic parameters for RFdiffusion that will work reliably.\n        Only provide the following required parameters:\n        1. contigs: Must be in format 'chainID-range/0 binder_length_range'. \n           Example: 'A20-60/0 50-100' means:\n           - Keep amino acids 20-60 in Chain A as target\n           - /0 marks the end of target chain\n           - Generate a binder protein of length 50-100 amino acids\n        2. hotspot_res: List three sequential residues from the middle of the chain\n        3. diffusion_steps: Use minimum value of 15 for faster results\n        \n        Keep the parameters simple and conservative to ensure API success.\n        Base the analysis on this PDB structure:\n        HEADER    LIGASE/RNA                              15-SEP-93   1GTS              \nTITLE     STRUCTURAL BASIS FOR TRANSFER RNA AMINOACEYLATION BY ESCHERICHIA COLI \nTITLE    2 GLUTAMINYL-TRNA SYNTHETASE                                           \nCOMPND    MOL_ID: 1;                                                            \nCOMPND   2 MOLECULE: TRNAGLN;                                                   \nCOMPND   3 CHAIN: B;                                                            \nCOMPND   4 MOL...\n        ""}]","[{'role': 'assistant', 'content': ""Based on your PDB structure and its description, here are the suggested RFdiffusion parameters:\n\n1. **contigs**: Since you have a molecule (likely tRNA or a protein interacting with tRNA) in Chain B, you can assume a typical continuous stretch. For example, use:  \n   `'B15-45/0 50-100'`  \n   This means amino acids 15 to 45 in Chain B are part of the target structure, and the binder length range to be generated is between 50 and 100 amino acids.\n\n2. **hotspot_res**: Choose three sequential residues from the middle of the Chain B. Assuming this chain has more than 45 residues, you might select residues:  \n   `hotspot_res: B30, B31, B32`\n\n3. **diffusion_steps**: Use the minimum recommended value:  \n   `diffusion_steps: 15`\n\nThese parameters are designed to optimize RFdiffusion for generating a binder to the specified target region effectively and quickly.""}]","{'mime_type': 'application/json', 'value': '{""id"":""chatcmpl-ARGFWftvjjo8SxjBI5vzQ1rdzhYOK"",""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""message"":{""content"":""Based on your PDB structure and its description, here are the suggested RFdiffusion parameters:\\n\\n1. **contigs**: Since you have a molecule (likely tRNA or a protein interacting with tRNA) in Chain B, you can assume a typical continuous stretch. For example, use:  \\n   `\'B15-45/0 50-100\'`  \\n   This means amino acids 15 to 45 in Chain B are part of the target structure, and the binder length range to be generated is between 50 and 100 amino acids.\\n\\n2. **hotspot_res**: Choose three sequential residues from the middle of the Chain B. Assuming this chain has more than 45 residues, you might select residues:  \\n   `hotspot_res: B30, B31, B32`\\n\\n3. **diffusion_steps**: Use the minimum recommended value:  \\n   `diffusion_steps: 15`\\n\\nThese parameters are designed to optimize RFdiffusion for generating a binder to the specified target region effectively and quickly."",""refusal"":null,""role"":""assistant""}}],""created"":1731060682,""model"":""gpt-4o-2024-08-06"",""object"":""chat.completion"",""system_fingerprint"":""fp_45cf54deae"",""usage"":{""completion_tokens"":211,""prompt_tokens"":344,""total_tokens"":555,""completion_tokens_details"":{""accepted_prediction_tokens"":0,""audio_tokens"":0,""reasoning_tokens"":0,""rejected_prediction_tokens"":0},""prompt_tokens_details"":{""audio_tokens"":0,""cached_tokens"":0}}}'}","{'invocation_parameters': '{""model"": ""gpt-4o""}', 'input_messages': [{'message': {'content': ""You are an expert in RFdiffusion parameter optimization. \n                Always ensure contigs parameter has both target specification AND binder length range after /0.\n                Example: 'A20-60/0 50-100' NOT just 'A20-60/0'"", 'role': 'system'}}, {'message': {'content': ""\n        Analyze this PDB content and suggest basic parameters for RFdiffusion that will work reliably.\n        Only provide the following required parameters:\n        1. contigs: Must be in format 'chainID-range/0 binder_length_range'. \n           Example: 'A20-60/0 50-100' means:\n           - Keep amino acids 20-60 in Chain A as target\n           - /0 marks the end of target chain\n           - Generate a binder protein of length 50-100 amino acids\n        2. hotspot_res: List three sequential residues from the middle of the chain\n        3. diffusion_steps: Use minimum value of 15 for faster results\n        \n        Keep the parameters simple and conservative to ensure API success.\n        Base the analysis on this PDB structure:\n        HEADER    LIGASE/RNA                              15-SEP-93   1GTS              \nTITLE     STRUCTURAL BASIS FOR TRANSFER RNA AMINOACEYLATION BY ESCHERICHIA COLI \nTITLE    2 GLUTAMINYL-TRNA SYNTHETASE                                           \nCOMPND    MOL_ID: 1;                                                            \nCOMPND   2 MOLECULE: TRNAGLN;                                                   \nCOMPND   3 CHAIN: B;                                                            \nCOMPND   4 MOL...\n        "", 'role': 'user'}}], 'token_count': {'total': 555, 'prompt': 344, 'completion': 211}, 'system': 'openai', 'provider': 'openai', 'output_messages': [{'message': {'content': ""Based on your PDB structure and its description, here are the suggested RFdiffusion parameters:\n\n1. **contigs**: Since you have a molecule (likely tRNA or a protein interacting with tRNA) in Chain B, you can assume a typical continuous stretch. For example, use:  \n   `'B15-45/0 50-100'`  \n   This means amino acids 15 to 45 in Chain B are part of the target structure, and the binder length range to be generated is between 50 and 100 amino acids.\n\n2. **hotspot_res**: Choose three sequential residues from the middle of the Chain B. Assuming this chain has more than 45 residues, you might select residues:  \n   `hotspot_res: B30, B31, B32`\n\n3. **diffusion_steps**: Use the minimum recommended value:  \n   `diffusion_steps: 15`\n\nThese parameters are designed to optimize RFdiffusion for generating a binder to the specified target region effectively and quickly."", 'role': 'assistant'}}], 'model_name': 'gpt-4o-2024-08-06'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""messages"": [{""role"": ""system"", ""content"": ""You are an expert in RFdiffusion parameter optimization. \\n                Always ensure contigs parameter has both target specification AND binder length range after /0.\\n                Example: \'A20-60/0 50-100\' NOT just \'A20-60/0\'""}, {""role"": ""user"", ""content"": ""\\n        Analyze this PDB content and suggest basic parameters for RFdiffusion that will work reliably.\\n        Only provide the following required parameters:\\n        1. contigs: Must be in format \'chainID-range/0 binder_length_range\'. \\n           Example: \'A20-60/0 50-100\' means:\\n           - Keep amino acids 20-60 in Chain A as target\\n           - /0 marks the end of target chain\\n           - Generate a binder protein of length 50-100 amino acids\\n        2. hotspot_res: List three sequential residues from the middle of the chain\\n        3. diffusion_steps: Use minimum value of 15 for faster results\\n        \\n        Keep the parameters simple and conservative to ensure API success.\\n        Base the analysis on this PDB structure:\\n        HEADER    LIGASE/RNA                              15-SEP-93   1GTS              \\nTITLE     STRUCTURAL BASIS FOR TRANSFER RNA AMINOACEYLATION BY ESCHERICHIA COLI \\nTITLE    2 GLUTAMINYL-TRNA SYNTHETASE                                           \\nCOMPND    MOL_ID: 1;                                                            \\nCOMPND   2 MOLECULE: TRNAGLN;                                                   \\nCOMPND   3 CHAIN: B;                                                            \\nCOMPND   4 MOL...\\n        ""}], ""model"": ""gpt-4o""}'}",{},,,,,
RGF0YXNldEV4YW1wbGU6MTE4,"[{'role': 'system', 'content': ""You are an expert in RFdiffusion parameter optimization. \n                Always ensure contigs parameter has both target specification AND binder length range after /0.\n                Example: 'A20-60/0 50-100' NOT just 'A20-60/0'""}, {'role': 'user', 'content': '\n        Analyze this PDB content and suggest basic parameters for RFdiffusion that will work reliably.\n        Follow this exact format in your response:\n        \n        PARAMETERS:\n        1. contigs: [value]\n        2. hotspot_res: [value]\n        3. diffusion_steps: [value]\n        \n        EXPLANATION:\n        Provide a clear, conversational explanation of why these parameters were chosen and how they will affect the protein design process.\n        Keep the explanation natural and suitable for voice output.\n        \n        Base the analysis on this PDB structure:\n        HEADER    LIGASE/RNA                              15-SEP-93   1GTS              \nTITLE     STRUCTURAL BASIS FOR TRANSFER RNA AMINOACEYLATION BY ESCHERICHIA COLI \nTITLE    2 GLUTAMINYL-TRNA SYNTHETASE                                           \nCOMPND    MOL_ID: 1;                                                            \nCOMPND   2 MOLECULE: TRNAGLN;                                                   \nCOMPND   3 CHAIN: B;                                                            \nCOMPND   4 MOL...\n        '}]","[{'role': 'assistant', 'content': ""PARAMETERS:\n1. contigs: A20-200/0 150-300\n2. hotspot_res: B:10, B:30, B:50\n3. diffusion_steps: 200\n\nEXPLANATION:\nLet's dive into the rationale behind selecting these parameters for the RFdiffusion run. First off, looking at the structure, we have a transfer RNA (tRNA) molecule in complex with glutaminyl-tRNA synthetase. The chain of interest is chain B, where the tRNA is located. This type of complex requires a focus on both the target specificity (the enzyme) and a proper binding length of the RNA.\n\nFor the contigs parameter, I chose 'A20-200/0 150-300'. This includes a wide range of potential target sequences on the enzyme side, from 20 to 200 amino acids, and for the associated RNA binding section, a slightly narrower range of 150 to 300 nucleotides. The use of 'A20-200/0 150-300' ensures that we have not only the flexibility to evolve different regions of the protein with appropriate length but also cater to the functional binding with the RNA, reflecting the biological function of transferring amino acids accurately.\n\nNext, the hotspot residues are assigned to specific positions in the tRNA (chain B) that are likely critical for enzyme interaction, namely B:10, B:30, and B:50. These positions are often integral to maintaining the structure and function of tRNA during aminoacylation processes.\n\nFinally, diffusion_steps is set to 200. This is a moderate number, aimed at providing sufficient iterations for sampling while preventing excessive computation time. This setting ensures that the diffusion process can explore the conformational space adequately, allowing for variations that improve or retain the functional characteristics of the enzyme-RNA interaction.\n\nBy combining these parameters, RFdiffusion will have a guided yet sufficiently exploratory runway to generate new, plausible designs while respecting the complex's biological purpose.""}]","{'mime_type': 'application/json', 'value': '{""id"":""chatcmpl-ARHDI6dQkEAUcDNM00ysutM50iXZE"",""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""message"":{""content"":""PARAMETERS:\\n1. contigs: A20-200/0 150-300\\n2. hotspot_res: B:10, B:30, B:50\\n3. diffusion_steps: 200\\n\\nEXPLANATION:\\nLet\'s dive into the rationale behind selecting these parameters for the RFdiffusion run. First off, looking at the structure, we have a transfer RNA (tRNA) molecule in complex with glutaminyl-tRNA synthetase. The chain of interest is chain B, where the tRNA is located. This type of complex requires a focus on both the target specificity (the enzyme) and a proper binding length of the RNA.\\n\\nFor the contigs parameter, I chose \'A20-200/0 150-300\'. This includes a wide range of potential target sequences on the enzyme side, from 20 to 200 amino acids, and for the associated RNA binding section, a slightly narrower range of 150 to 300 nucleotides. The use of \'A20-200/0 150-300\' ensures that we have not only the flexibility to evolve different regions of the protein with appropriate length but also cater to the functional binding with the RNA, reflecting the biological function of transferring amino acids accurately.\\n\\nNext, the hotspot residues are assigned to specific positions in the tRNA (chain B) that are likely critical for enzyme interaction, namely B:10, B:30, and B:50. These positions are often integral to maintaining the structure and function of tRNA during aminoacylation processes.\\n\\nFinally, diffusion_steps is set to 200. This is a moderate number, aimed at providing sufficient iterations for sampling while preventing excessive computation time. This setting ensures that the diffusion process can explore the conformational space adequately, allowing for variations that improve or retain the functional characteristics of the enzyme-RNA interaction.\\n\\nBy combining these parameters, RFdiffusion will have a guided yet sufficiently exploratory runway to generate new, plausible designs while respecting the complex\'s biological purpose."",""refusal"":null,""role"":""assistant""}}],""created"":1731064388,""model"":""gpt-4o-2024-08-06"",""object"":""chat.completion"",""system_fingerprint"":""fp_45cf54deae"",""usage"":{""completion_tokens"":405,""prompt_tokens"":293,""total_tokens"":698,""completion_tokens_details"":{""accepted_prediction_tokens"":0,""audio_tokens"":0,""reasoning_tokens"":0,""rejected_prediction_tokens"":0},""prompt_tokens_details"":{""audio_tokens"":0,""cached_tokens"":0}}}'}","{'invocation_parameters': '{""model"": ""gpt-4o""}', 'input_messages': [{'message': {'content': ""You are an expert in RFdiffusion parameter optimization. \n                Always ensure contigs parameter has both target specification AND binder length range after /0.\n                Example: 'A20-60/0 50-100' NOT just 'A20-60/0'"", 'role': 'system'}}, {'message': {'content': '\n        Analyze this PDB content and suggest basic parameters for RFdiffusion that will work reliably.\n        Follow this exact format in your response:\n        \n        PARAMETERS:\n        1. contigs: [value]\n        2. hotspot_res: [value]\n        3. diffusion_steps: [value]\n        \n        EXPLANATION:\n        Provide a clear, conversational explanation of why these parameters were chosen and how they will affect the protein design process.\n        Keep the explanation natural and suitable for voice output.\n        \n        Base the analysis on this PDB structure:\n        HEADER    LIGASE/RNA                              15-SEP-93   1GTS              \nTITLE     STRUCTURAL BASIS FOR TRANSFER RNA AMINOACEYLATION BY ESCHERICHIA COLI \nTITLE    2 GLUTAMINYL-TRNA SYNTHETASE                                           \nCOMPND    MOL_ID: 1;                                                            \nCOMPND   2 MOLECULE: TRNAGLN;                                                   \nCOMPND   3 CHAIN: B;                                                            \nCOMPND   4 MOL...\n        ', 'role': 'user'}}], 'token_count': {'total': 698, 'prompt': 293, 'completion': 405}, 'system': 'openai', 'provider': 'openai', 'output_messages': [{'message': {'content': ""PARAMETERS:\n1. contigs: A20-200/0 150-300\n2. hotspot_res: B:10, B:30, B:50\n3. diffusion_steps: 200\n\nEXPLANATION:\nLet's dive into the rationale behind selecting these parameters for the RFdiffusion run. First off, looking at the structure, we have a transfer RNA (tRNA) molecule in complex with glutaminyl-tRNA synthetase. The chain of interest is chain B, where the tRNA is located. This type of complex requires a focus on both the target specificity (the enzyme) and a proper binding length of the RNA.\n\nFor the contigs parameter, I chose 'A20-200/0 150-300'. This includes a wide range of potential target sequences on the enzyme side, from 20 to 200 amino acids, and for the associated RNA binding section, a slightly narrower range of 150 to 300 nucleotides. The use of 'A20-200/0 150-300' ensures that we have not only the flexibility to evolve different regions of the protein with appropriate length but also cater to the functional binding with the RNA, reflecting the biological function of transferring amino acids accurately.\n\nNext, the hotspot residues are assigned to specific positions in the tRNA (chain B) that are likely critical for enzyme interaction, namely B:10, B:30, and B:50. These positions are often integral to maintaining the structure and function of tRNA during aminoacylation processes.\n\nFinally, diffusion_steps is set to 200. This is a moderate number, aimed at providing sufficient iterations for sampling while preventing excessive computation time. This setting ensures that the diffusion process can explore the conformational space adequately, allowing for variations that improve or retain the functional characteristics of the enzyme-RNA interaction.\n\nBy combining these parameters, RFdiffusion will have a guided yet sufficiently exploratory runway to generate new, plausible designs while respecting the complex's biological purpose."", 'role': 'assistant'}}], 'model_name': 'gpt-4o-2024-08-06'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""messages"": [{""role"": ""system"", ""content"": ""You are an expert in RFdiffusion parameter optimization. \\n                Always ensure contigs parameter has both target specification AND binder length range after /0.\\n                Example: \'A20-60/0 50-100\' NOT just \'A20-60/0\'""}, {""role"": ""user"", ""content"": ""\\n        Analyze this PDB content and suggest basic parameters for RFdiffusion that will work reliably.\\n        Follow this exact format in your response:\\n        \\n        PARAMETERS:\\n        1. contigs: [value]\\n        2. hotspot_res: [value]\\n        3. diffusion_steps: [value]\\n        \\n        EXPLANATION:\\n        Provide a clear, conversational explanation of why these parameters were chosen and how they will affect the protein design process.\\n        Keep the explanation natural and suitable for voice output.\\n        \\n        Base the analysis on this PDB structure:\\n        HEADER    LIGASE/RNA                              15-SEP-93   1GTS              \\nTITLE     STRUCTURAL BASIS FOR TRANSFER RNA AMINOACEYLATION BY ESCHERICHIA COLI \\nTITLE    2 GLUTAMINYL-TRNA SYNTHETASE                                           \\nCOMPND    MOL_ID: 1;                                                            \\nCOMPND   2 MOLECULE: TRNAGLN;                                                   \\nCOMPND   3 CHAIN: B;                                                            \\nCOMPND   4 MOL...\\n        ""}], ""model"": ""gpt-4o""}'}",{},,,,,
RGF0YXNldEV4YW1wbGU6MTE5,,,{'value': '<openai._legacy_response.HttpxBinaryResponseContent object at 0x7a6e9ee0a440>'},"{'invocation_parameters': '{""input"": ""Let\'s dive into the rationale behind selecting these parameters for the RFdiffusion run. First off, looking at the structure, we have a transfer RNA (tRNA) molecule in complex with glutaminyl-tRNA synthetase. The chain of interest is chain B, where the tRNA is located. This type of complex requires a focus on both the target specificity (the enzyme) and a proper binding length of the RNA.\\n\\nFor the contigs parameter, I chose \'A20-200/0 150-300\'. This includes a wide range of potential target sequences on the enzyme side, from 20 to 200 amino acids, and for the associated RNA binding section, a slightly narrower range of 150 to 300 nucleotides. The use of \'A20-200/0 150-300\' ensures that we have not only the flexibility to evolve different regions of the protein with appropriate length but also cater to the functional binding with the RNA, reflecting the biological function of transferring amino acids accurately.\\n\\nNext, the hotspot residues are assigned to specific positions in the tRNA (chain B) that are likely critical for enzyme interaction, namely B:10, B:30, and B:50. These positions are often integral to maintaining the structure and function of tRNA during aminoacylation processes.\\n\\nFinally, diffusion_steps is set to 200. This is a moderate number, aimed at providing sufficient iterations for sampling while preventing excessive computation time. This setting ensures that the diffusion process can explore the conformational space adequately, allowing for variations that improve or retain the functional characteristics of the enzyme-RNA interaction.\\n\\nBy combining these parameters, RFdiffusion will have a guided yet sufficiently exploratory runway to generate new, plausible designs while respecting the complex\'s biological purpose."", ""model"": ""tts-1"", ""voice"": ""alloy""}', 'provider': 'openai', 'system': 'openai'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""input"": ""Let\'s dive into the rationale behind selecting these parameters for the RFdiffusion run. First off, looking at the structure, we have a transfer RNA (tRNA) molecule in complex with glutaminyl-tRNA synthetase. The chain of interest is chain B, where the tRNA is located. This type of complex requires a focus on both the target specificity (the enzyme) and a proper binding length of the RNA.\\n\\nFor the contigs parameter, I chose \'A20-200/0 150-300\'. This includes a wide range of potential target sequences on the enzyme side, from 20 to 200 amino acids, and for the associated RNA binding section, a slightly narrower range of 150 to 300 nucleotides. The use of \'A20-200/0 150-300\' ensures that we have not only the flexibility to evolve different regions of the protein with appropriate length but also cater to the functional binding with the RNA, reflecting the biological function of transferring amino acids accurately.\\n\\nNext, the hotspot residues are assigned to specific positions in the tRNA (chain B) that are likely critical for enzyme interaction, namely B:10, B:30, and B:50. These positions are often integral to maintaining the structure and function of tRNA during aminoacylation processes.\\n\\nFinally, diffusion_steps is set to 200. This is a moderate number, aimed at providing sufficient iterations for sampling while preventing excessive computation time. This setting ensures that the diffusion process can explore the conformational space adequately, allowing for variations that improve or retain the functional characteristics of the enzyme-RNA interaction.\\n\\nBy combining these parameters, RFdiffusion will have a guided yet sufficiently exploratory runway to generate new, plausible designs while respecting the complex\'s biological purpose."", ""model"": ""tts-1"", ""voice"": ""alloy""}'}",{},"Let's dive into the rationale behind selecting these parameters for the RFdiffusion run. First off, looking at the structure, we have a transfer RNA (tRNA) molecule in complex with glutaminyl-tRNA synthetase. The chain of interest is chain B, where the tRNA is located. This type of complex requires a focus on both the target specificity (the enzyme) and a proper binding length of the RNA.

For the contigs parameter, I chose 'A20-200/0 150-300'. This includes a wide range of potential target sequences on the enzyme side, from 20 to 200 amino acids, and for the associated RNA binding section, a slightly narrower range of 150 to 300 nucleotides. The use of 'A20-200/0 150-300' ensures that we have not only the flexibility to evolve different regions of the protein with appropriate length but also cater to the functional binding with the RNA, reflecting the biological function of transferring amino acids accurately.

Next, the hotspot residues are assigned to specific positions in the tRNA (chain B) that are likely critical for enzyme interaction, namely B:10, B:30, and B:50. These positions are often integral to maintaining the structure and function of tRNA during aminoacylation processes.

Finally, diffusion_steps is set to 200. This is a moderate number, aimed at providing sufficient iterations for sampling while preventing excessive computation time. This setting ensures that the diffusion process can explore the conformational space adequately, allowing for variations that improve or retain the functional characteristics of the enzyme-RNA interaction.

By combining these parameters, RFdiffusion will have a guided yet sufficiently exploratory runway to generate new, plausible designs while respecting the complex's biological purpose.",tts-1,alloy,<openai._legacy_response.HttpxBinaryResponseContent object at 0x7a6e9ee0a440>,
RGF0YXNldEV4YW1wbGU6MTIw,"[{'role': 'system', 'content': 'You are an expert in ProteinMPNN modeling.'}, {'role': 'user', 'content': '\n        Analyze the following PDB content and suggest appropriate parameters for running ProteinMPNN:\n\n        ATOM      1  N   GLY A   1      -7.161   1.641 -19.813  1.00  0.00\nATOM      2  CA  GLY A   1      -7.289   0.193 -19.927  1.00  0.00\nATOM      3  C   GLY A   1      -6.237  -0.520 -19.088  1.00  0.00\nATOM      4  O   GLY A   1      -5.146   0.006 -18.863  1.00  0.00\nATOM      5  N   GLY A   2      -6.620  -1.470 -18.394  1.00  0.00\nATOM      6  CA  GLY A   2      -5.691  -2.169 -17.514  1.00  0.00\nATOM      7  C   GLY A   2      -4.499  -2.712 -18.292  1.00  0.00\nATOM      8  O   GLY A   2     ...\n\n        Provide the following parameters:\n        1. ca_only\n        2. use_soluble_model\n        3. num_seq_per_target\n        4. sampling_temp\n\n        Explain your reasoning for each parameter.\n        '}]","[{'role': 'assistant', 'content': ""Certainly! Let's analyze the provided PDB content and determine the appropriate parameters for running ProteinMPNN:\n\n### PDB Content Analysis:\nThe PDB content provided consists of atoms from a protein chain, labeled with chain identifier 'A'. The residues are listed sequentially and include atomic coordinates. The specific sequence of residues highlighted appears to be composed solely of Glycine residues.\n\n### Suggested Parameters for ProteinMPNN:\n\n1. **ca_only:**\n   - **Value:** `False`\n   - **Reasoning:** The `ca_only` parameter in ProteinMPNN determines whether only C-alpha (CA) atoms are used for structure representation. Since you provided a full atomic representation (including backbone atoms like N, CA, C, and O), setting `ca_only` to `False` is suitable because it allows the use of detailed atomic data, which can potentially lead to more accurate modeling.\n\n2. **use_soluble_model:**\n   - **Value:** `True` or `False` (Context-Dependent)\n   - **Reasoning:** The `use_soluble_model` parameter should be set according to the environment in which you expect this protein to function. If this protein is mainly expressed or functions in a soluble environment (e.g., cytoplasm), you would typically set `use_soluble_model` to `True`. If it is a membrane protein or part of insoluble complexes, `False` might be more appropriate. If no contextual information is provided, defaulting to `True` for general use is a safe choice.\n\n3. **num_seq_per_target:**\n   - **Value:** `10` (for initial exploration)\n   - **Reasoning:** The `num_seq_per_target` parameter specifies the number of sequence predictions or designs generated per structure. Starting with a moderate number like 10 gives a balanced initial sample of sequences for evaluation without overwhelming computational resources. Depending on the results, you can adjust higher to explore more diverse sequences.\n\n4. **sampling_temp:**\n   - **Value:** `0.1` to `0.3`\n   - **Reasoning:** The `sampling_temp` parameter controls the diversity of the generated sequences; lower values result in sequences closer to the most likely prediction while higher values allow more exploration and diversity. A value of `0.1` to `0.3` is a typical starting point, ensuring that the diversity is reasonable while still being relatively focused on high-probability sequences.\n\n### Conclusion:\nThese parameters are chosen based on the given PDB structure and typical use cases for ProteinMPNN. Adjust the parameters further based on the specific biological context, computational resources, or after evaluating preliminary results.""}]","{'mime_type': 'application/json', 'value': '{""id"":""chatcmpl-ARHRBLirhC0VGlHYahIFBrYciamov"",""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""message"":{""content"":""Certainly! Let\'s analyze the provided PDB content and determine the appropriate parameters for running ProteinMPNN:\\n\\n### PDB Content Analysis:\\nThe PDB content provided consists of atoms from a protein chain, labeled with chain identifier \'A\'. The residues are listed sequentially and include atomic coordinates. The specific sequence of residues highlighted appears to be composed solely of Glycine residues.\\n\\n### Suggested Parameters for ProteinMPNN:\\n\\n1. **ca_only:**\\n   - **Value:** `False`\\n   - **Reasoning:** The `ca_only` parameter in ProteinMPNN determines whether only C-alpha (CA) atoms are used for structure representation. Since you provided a full atomic representation (including backbone atoms like N, CA, C, and O), setting `ca_only` to `False` is suitable because it allows the use of detailed atomic data, which can potentially lead to more accurate modeling.\\n\\n2. **use_soluble_model:**\\n   - **Value:** `True` or `False` (Context-Dependent)\\n   - **Reasoning:** The `use_soluble_model` parameter should be set according to the environment in which you expect this protein to function. If this protein is mainly expressed or functions in a soluble environment (e.g., cytoplasm), you would typically set `use_soluble_model` to `True`. If it is a membrane protein or part of insoluble complexes, `False` might be more appropriate. If no contextual information is provided, defaulting to `True` for general use is a safe choice.\\n\\n3. **num_seq_per_target:**\\n   - **Value:** `10` (for initial exploration)\\n   - **Reasoning:** The `num_seq_per_target` parameter specifies the number of sequence predictions or designs generated per structure. Starting with a moderate number like 10 gives a balanced initial sample of sequences for evaluation without overwhelming computational resources. Depending on the results, you can adjust higher to explore more diverse sequences.\\n\\n4. **sampling_temp:**\\n   - **Value:** `0.1` to `0.3`\\n   - **Reasoning:** The `sampling_temp` parameter controls the diversity of the generated sequences; lower values result in sequences closer to the most likely prediction while higher values allow more exploration and diversity. A value of `0.1` to `0.3` is a typical starting point, ensuring that the diversity is reasonable while still being relatively focused on high-probability sequences.\\n\\n### Conclusion:\\nThese parameters are chosen based on the given PDB structure and typical use cases for ProteinMPNN. Adjust the parameters further based on the specific biological context, computational resources, or after evaluating preliminary results."",""refusal"":null,""role"":""assistant""}}],""created"":1731065249,""model"":""gpt-4o-2024-08-06"",""object"":""chat.completion"",""system_fingerprint"":""fp_159d8341cc"",""usage"":{""completion_tokens"":538,""prompt_tokens"":369,""total_tokens"":907,""completion_tokens_details"":{""accepted_prediction_tokens"":0,""audio_tokens"":0,""reasoning_tokens"":0,""rejected_prediction_tokens"":0},""prompt_tokens_details"":{""audio_tokens"":0,""cached_tokens"":0}}}'}","{'invocation_parameters': '{""model"": ""gpt-4o""}', 'input_messages': [{'message': {'content': 'You are an expert in ProteinMPNN modeling.', 'role': 'system'}}, {'message': {'content': '\n        Analyze the following PDB content and suggest appropriate parameters for running ProteinMPNN:\n\n        ATOM      1  N   GLY A   1      -7.161   1.641 -19.813  1.00  0.00\nATOM      2  CA  GLY A   1      -7.289   0.193 -19.927  1.00  0.00\nATOM      3  C   GLY A   1      -6.237  -0.520 -19.088  1.00  0.00\nATOM      4  O   GLY A   1      -5.146   0.006 -18.863  1.00  0.00\nATOM      5  N   GLY A   2      -6.620  -1.470 -18.394  1.00  0.00\nATOM      6  CA  GLY A   2      -5.691  -2.169 -17.514  1.00  0.00\nATOM      7  C   GLY A   2      -4.499  -2.712 -18.292  1.00  0.00\nATOM      8  O   GLY A   2     ...\n\n        Provide the following parameters:\n        1. ca_only\n        2. use_soluble_model\n        3. num_seq_per_target\n        4. sampling_temp\n\n        Explain your reasoning for each parameter.\n        ', 'role': 'user'}}], 'token_count': {'total': 907, 'prompt': 369, 'completion': 538}, 'system': 'openai', 'provider': 'openai', 'output_messages': [{'message': {'content': ""Certainly! Let's analyze the provided PDB content and determine the appropriate parameters for running ProteinMPNN:\n\n### PDB Content Analysis:\nThe PDB content provided consists of atoms from a protein chain, labeled with chain identifier 'A'. The residues are listed sequentially and include atomic coordinates. The specific sequence of residues highlighted appears to be composed solely of Glycine residues.\n\n### Suggested Parameters for ProteinMPNN:\n\n1. **ca_only:**\n   - **Value:** `False`\n   - **Reasoning:** The `ca_only` parameter in ProteinMPNN determines whether only C-alpha (CA) atoms are used for structure representation. Since you provided a full atomic representation (including backbone atoms like N, CA, C, and O), setting `ca_only` to `False` is suitable because it allows the use of detailed atomic data, which can potentially lead to more accurate modeling.\n\n2. **use_soluble_model:**\n   - **Value:** `True` or `False` (Context-Dependent)\n   - **Reasoning:** The `use_soluble_model` parameter should be set according to the environment in which you expect this protein to function. If this protein is mainly expressed or functions in a soluble environment (e.g., cytoplasm), you would typically set `use_soluble_model` to `True`. If it is a membrane protein or part of insoluble complexes, `False` might be more appropriate. If no contextual information is provided, defaulting to `True` for general use is a safe choice.\n\n3. **num_seq_per_target:**\n   - **Value:** `10` (for initial exploration)\n   - **Reasoning:** The `num_seq_per_target` parameter specifies the number of sequence predictions or designs generated per structure. Starting with a moderate number like 10 gives a balanced initial sample of sequences for evaluation without overwhelming computational resources. Depending on the results, you can adjust higher to explore more diverse sequences.\n\n4. **sampling_temp:**\n   - **Value:** `0.1` to `0.3`\n   - **Reasoning:** The `sampling_temp` parameter controls the diversity of the generated sequences; lower values result in sequences closer to the most likely prediction while higher values allow more exploration and diversity. A value of `0.1` to `0.3` is a typical starting point, ensuring that the diversity is reasonable while still being relatively focused on high-probability sequences.\n\n### Conclusion:\nThese parameters are chosen based on the given PDB structure and typical use cases for ProteinMPNN. Adjust the parameters further based on the specific biological context, computational resources, or after evaluating preliminary results."", 'role': 'assistant'}}], 'model_name': 'gpt-4o-2024-08-06'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""messages"": [{""role"": ""system"", ""content"": ""You are an expert in ProteinMPNN modeling.""}, {""role"": ""user"", ""content"": ""\\n        Analyze the following PDB content and suggest appropriate parameters for running ProteinMPNN:\\n\\n        ATOM      1  N   GLY A   1      -7.161   1.641 -19.813  1.00  0.00\\nATOM      2  CA  GLY A   1      -7.289   0.193 -19.927  1.00  0.00\\nATOM      3  C   GLY A   1      -6.237  -0.520 -19.088  1.00  0.00\\nATOM      4  O   GLY A   1      -5.146   0.006 -18.863  1.00  0.00\\nATOM      5  N   GLY A   2      -6.620  -1.470 -18.394  1.00  0.00\\nATOM      6  CA  GLY A   2      -5.691  -2.169 -17.514  1.00  0.00\\nATOM      7  C   GLY A   2      -4.499  -2.712 -18.292  1.00  0.00\\nATOM      8  O   GLY A   2     ...\\n\\n        Provide the following parameters:\\n        1. ca_only\\n        2. use_soluble_model\\n        3. num_seq_per_target\\n        4. sampling_temp\\n\\n        Explain your reasoning for each parameter.\\n        ""}], ""model"": ""gpt-4o""}'}",{},,,,,
RGF0YXNldEV4YW1wbGU6MTIx,"[{'role': 'system', 'content': 'You are an expert in ProteinMPNN modeling.'}, {'role': 'user', 'content': '\n        Analyze the following PDB content and suggest appropriate parameters for running ProteinMPNN:\n\n        ATOM      1  N   GLY A   1      -7.161   1.641 -19.813  1.00  0.00\nATOM      2  CA  GLY A   1      -7.289   0.193 -19.927  1.00  0.00\nATOM      3  C   GLY A   1      -6.237  -0.520 -19.088  1.00  0.00\nATOM      4  O   GLY A   1      -5.146   0.006 -18.863  1.00  0.00\nATOM      5  N   GLY A   2      -6.620  -1.470 -18.394  1.00  0.00\nATOM      6  CA  GLY A   2      -5.691  -2.169 -17.514  1.00  0.00\nATOM      7  C   GLY A   2      -4.499  -2.712 -18.292  1.00  0.00\nATOM      8  O   GLY A   2     ...\n\n        Provide the following parameters:\n        1. ca_only\n        2. use_soluble_model\n        3. num_seq_per_target\n        4. sampling_temp\n\n        Explain your reasoning for each parameter.\n        '}]","[{'role': 'assistant', 'content': ""To determine the appropriate parameters for running ProteinMPNN on the given PDB content, let's analyze the structure and apply that knowledge to each parameter:\n\n1. **ca_only**:\n   - The provided PDB content includes full atomic coordinates, with entries for nitrogen (N), carbon alpha (CA), carbon (C), and oxygen (O) atoms. This suggests that both backbone and side chain data might be available. However, often only the CA atoms are used if a simplified model is sufficient or if focusing on folding patterns and backbones is the goal.\n   - **Recommendation**: `ca_only = False`. This is because while the provided snippet only explicitly shows a few atoms, if more comprehensive data is available, using the entire backbone can potentially yield more precise results unless there are memory constraints or specific simplicity needs.\n\n2. **use_soluble_model**:\n   - This parameter is used for distinguishing between proteins expected to function in soluble environments versus others, like membrane proteins. Without specific environmental context suggesting a membrane localization, and given that glycine-rich sequences commonly appear in soluble proteins, it's generally safe to use a soluble model.\n   - **Recommendation**: `use_soluble_model = True`. Given the lack of context suggesting otherwise, and the fact simple glycine structures are commonly found in soluble environments.\n\n3. **num_seq_per_target**:\n   - This parameter determines how many sequence variations are generated for the target structure. More sequences provide better insights into variability and potential function but increase computational demands.\n   - **Recommendation**: `num_seq_per_target = 10`. This is a balanced number that provides sufficient diversity for most analyses without overloading computational resources, assuming this is a preliminary analysis. This number can be increased if further exploration is required.\n\n4. **sampling_temp**:\n   - The sampling temperature affects how much diversity is introduced in the generated sequences. Higher temperatures allow for more variations, but can lead to less native-like predictions.\n   - **Recommendation**: `sampling_temp = 0.1`. A lower value is often used for refinement and accuracy-focused tasks, ensuring the generated sequences are close to native-like conformations. This assumes an interest in accuracy over exploration; if exploring diverse sequences is of interest, this can be increased.\n\nThe above recommendations are based on typical scenarios and may be fine-tuned with additional context or specific objectives for the protein analyses. If computational resources allow, experimenting with different parameters might give deeper insights.""}]","{'mime_type': 'application/json', 'value': '{""id"":""chatcmpl-ARHRKC2pXDrMxvC1WRlPy27YQ5kjn"",""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""message"":{""content"":""To determine the appropriate parameters for running ProteinMPNN on the given PDB content, let\'s analyze the structure and apply that knowledge to each parameter:\\n\\n1. **ca_only**:\\n   - The provided PDB content includes full atomic coordinates, with entries for nitrogen (N), carbon alpha (CA), carbon (C), and oxygen (O) atoms. This suggests that both backbone and side chain data might be available. However, often only the CA atoms are used if a simplified model is sufficient or if focusing on folding patterns and backbones is the goal.\\n   - **Recommendation**: `ca_only = False`. This is because while the provided snippet only explicitly shows a few atoms, if more comprehensive data is available, using the entire backbone can potentially yield more precise results unless there are memory constraints or specific simplicity needs.\\n\\n2. **use_soluble_model**:\\n   - This parameter is used for distinguishing between proteins expected to function in soluble environments versus others, like membrane proteins. Without specific environmental context suggesting a membrane localization, and given that glycine-rich sequences commonly appear in soluble proteins, it\'s generally safe to use a soluble model.\\n   - **Recommendation**: `use_soluble_model = True`. Given the lack of context suggesting otherwise, and the fact simple glycine structures are commonly found in soluble environments.\\n\\n3. **num_seq_per_target**:\\n   - This parameter determines how many sequence variations are generated for the target structure. More sequences provide better insights into variability and potential function but increase computational demands.\\n   - **Recommendation**: `num_seq_per_target = 10`. This is a balanced number that provides sufficient diversity for most analyses without overloading computational resources, assuming this is a preliminary analysis. This number can be increased if further exploration is required.\\n\\n4. **sampling_temp**:\\n   - The sampling temperature affects how much diversity is introduced in the generated sequences. Higher temperatures allow for more variations, but can lead to less native-like predictions.\\n   - **Recommendation**: `sampling_temp = 0.1`. A lower value is often used for refinement and accuracy-focused tasks, ensuring the generated sequences are close to native-like conformations. This assumes an interest in accuracy over exploration; if exploring diverse sequences is of interest, this can be increased.\\n\\nThe above recommendations are based on typical scenarios and may be fine-tuned with additional context or specific objectives for the protein analyses. If computational resources allow, experimenting with different parameters might give deeper insights."",""refusal"":null,""role"":""assistant""}}],""created"":1731065258,""model"":""gpt-4o-2024-08-06"",""object"":""chat.completion"",""system_fingerprint"":""fp_159d8341cc"",""usage"":{""completion_tokens"":495,""prompt_tokens"":369,""total_tokens"":864,""completion_tokens_details"":{""accepted_prediction_tokens"":0,""audio_tokens"":0,""reasoning_tokens"":0,""rejected_prediction_tokens"":0},""prompt_tokens_details"":{""audio_tokens"":0,""cached_tokens"":0}}}'}","{'invocation_parameters': '{""model"": ""gpt-4o""}', 'input_messages': [{'message': {'content': 'You are an expert in ProteinMPNN modeling.', 'role': 'system'}}, {'message': {'content': '\n        Analyze the following PDB content and suggest appropriate parameters for running ProteinMPNN:\n\n        ATOM      1  N   GLY A   1      -7.161   1.641 -19.813  1.00  0.00\nATOM      2  CA  GLY A   1      -7.289   0.193 -19.927  1.00  0.00\nATOM      3  C   GLY A   1      -6.237  -0.520 -19.088  1.00  0.00\nATOM      4  O   GLY A   1      -5.146   0.006 -18.863  1.00  0.00\nATOM      5  N   GLY A   2      -6.620  -1.470 -18.394  1.00  0.00\nATOM      6  CA  GLY A   2      -5.691  -2.169 -17.514  1.00  0.00\nATOM      7  C   GLY A   2      -4.499  -2.712 -18.292  1.00  0.00\nATOM      8  O   GLY A   2     ...\n\n        Provide the following parameters:\n        1. ca_only\n        2. use_soluble_model\n        3. num_seq_per_target\n        4. sampling_temp\n\n        Explain your reasoning for each parameter.\n        ', 'role': 'user'}}], 'token_count': {'total': 864, 'prompt': 369, 'completion': 495}, 'system': 'openai', 'provider': 'openai', 'output_messages': [{'message': {'content': ""To determine the appropriate parameters for running ProteinMPNN on the given PDB content, let's analyze the structure and apply that knowledge to each parameter:\n\n1. **ca_only**:\n   - The provided PDB content includes full atomic coordinates, with entries for nitrogen (N), carbon alpha (CA), carbon (C), and oxygen (O) atoms. This suggests that both backbone and side chain data might be available. However, often only the CA atoms are used if a simplified model is sufficient or if focusing on folding patterns and backbones is the goal.\n   - **Recommendation**: `ca_only = False`. This is because while the provided snippet only explicitly shows a few atoms, if more comprehensive data is available, using the entire backbone can potentially yield more precise results unless there are memory constraints or specific simplicity needs.\n\n2. **use_soluble_model**:\n   - This parameter is used for distinguishing between proteins expected to function in soluble environments versus others, like membrane proteins. Without specific environmental context suggesting a membrane localization, and given that glycine-rich sequences commonly appear in soluble proteins, it's generally safe to use a soluble model.\n   - **Recommendation**: `use_soluble_model = True`. Given the lack of context suggesting otherwise, and the fact simple glycine structures are commonly found in soluble environments.\n\n3. **num_seq_per_target**:\n   - This parameter determines how many sequence variations are generated for the target structure. More sequences provide better insights into variability and potential function but increase computational demands.\n   - **Recommendation**: `num_seq_per_target = 10`. This is a balanced number that provides sufficient diversity for most analyses without overloading computational resources, assuming this is a preliminary analysis. This number can be increased if further exploration is required.\n\n4. **sampling_temp**:\n   - The sampling temperature affects how much diversity is introduced in the generated sequences. Higher temperatures allow for more variations, but can lead to less native-like predictions.\n   - **Recommendation**: `sampling_temp = 0.1`. A lower value is often used for refinement and accuracy-focused tasks, ensuring the generated sequences are close to native-like conformations. This assumes an interest in accuracy over exploration; if exploring diverse sequences is of interest, this can be increased.\n\nThe above recommendations are based on typical scenarios and may be fine-tuned with additional context or specific objectives for the protein analyses. If computational resources allow, experimenting with different parameters might give deeper insights."", 'role': 'assistant'}}], 'model_name': 'gpt-4o-2024-08-06'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""messages"": [{""role"": ""system"", ""content"": ""You are an expert in ProteinMPNN modeling.""}, {""role"": ""user"", ""content"": ""\\n        Analyze the following PDB content and suggest appropriate parameters for running ProteinMPNN:\\n\\n        ATOM      1  N   GLY A   1      -7.161   1.641 -19.813  1.00  0.00\\nATOM      2  CA  GLY A   1      -7.289   0.193 -19.927  1.00  0.00\\nATOM      3  C   GLY A   1      -6.237  -0.520 -19.088  1.00  0.00\\nATOM      4  O   GLY A   1      -5.146   0.006 -18.863  1.00  0.00\\nATOM      5  N   GLY A   2      -6.620  -1.470 -18.394  1.00  0.00\\nATOM      6  CA  GLY A   2      -5.691  -2.169 -17.514  1.00  0.00\\nATOM      7  C   GLY A   2      -4.499  -2.712 -18.292  1.00  0.00\\nATOM      8  O   GLY A   2     ...\\n\\n        Provide the following parameters:\\n        1. ca_only\\n        2. use_soluble_model\\n        3. num_seq_per_target\\n        4. sampling_temp\\n\\n        Explain your reasoning for each parameter.\\n        ""}], ""model"": ""gpt-4o""}'}",{},,,,,
RGF0YXNldEV4YW1wbGU6MTIy,"[{'role': 'system', 'content': 'You are an expert in ProteinMPNN modeling. Provide parameters followed by a clear, conversational explanation.'}, {'role': 'user', 'content': ""\n        Analyze the following PDB content and suggest appropriate parameters for running ProteinMPNN:\n\n        ATOM      1  N   GLY A   1      -7.161   1.641 -19.813  1.00  0.00\nATOM      2  CA  GLY A   1      -7.289   0.193 -19.927  1.00  0.00\nATOM      3  C   GLY A   1      -6.237  -0.520 -19.088  1.00  0.00\nATOM      4  O   GLY A   1      -5.146   0.006 -18.863  1.00  0.00\nATOM      5  N   GLY A   2      -6.620  -1.470 -18.394  1.00  0.00\nATOM      6  CA  GLY A   2      -5.691  -2.169 -17.514  1.00  0.00\nATOM      7  C   GLY A   2      -4.499  -2.712 -18.292  1.00  0.00\nATOM      8  O   GLY A   2     ...\n\n        Provide a clear, conversational explanation of:\n        1. ca_only setting and why it's appropriate\n        2. use_soluble_model choice based on the structure\n        3. num_seq_per_target recommendation\n        4. sampling_temp selection\n\n        Format your response with clear parameter values first, followed by a natural explanation suitable for voice synthesis.\n        ""}]","[{'role': 'assistant', 'content': ""**Parameters:**\n\n1. `ca_only`: **False**\n2. `use_soluble_model`: **Not explicitly recommended**, assess hydrophobicity and context\n3. `num_seq_per_target`: **20**\n4. `sampling_temp`: **0.1**\n\n---\n\nHello! Let's dive into how we might configure ProteinMPNN to analyze the structure you've provided.\n\nFirst up, the `ca_only` setting: this parameter determines whether the model should consider only the alpha carbon (CA) atoms of the protein backbone, or utilize all atom positions provided in the PDB file. In your case, the PDB data you've provided includes detailed atomic information, not just the alpha carbons. Using `ca_only: False` allows the model to leverage this comprehensive structure data, giving it more context about the protein's geometry and potentially improving accuracy in modeling interactions and structure.\n\nNext is the `use_soluble_model` option, which would determine if a model optimized for soluble proteins should be used. To pinpoint whether this applies, you need to consider the protein's intrinsic properties and its environment. If the protein is part of a membrane or is an integral membrane protein, you may want a model fine-tuned for that context instead. Given just the snippet of structure, a decision here isn't explicit unless further biochemical context suggests solubility. So, generally, you might default to the context inferred from any additional biological information about the protein.\n\nThen, let’s talk about `num_seq_per_target`. This parameter indicates how many sequences you'd like the model to generate per target structure. Setting this to about **20** strikes a good balance between getting a variety of sequences to explore the diversity of potential interactions and keeping computational demands reasonable. It's often enough to provide insightful variations without overwhelming analysis requirements.\n\nLastly, for `sampling_temp`, this parameter affects the randomness of sequence sampling — akin to how 'explorative' the model should be. A lower temperature like **0.1** encourages the generation of sequences that are more energetically favorable or stable, essentially prioritizing quality over variety. This low temperature is a reasonable starting point if you're looking for reliable sequences that are more likely to maintain the structural integrity of your protein when you're inferring structure-function relationships.\n\nHopefully, this gives you a clear pathway to setting up your ProteinMPNN run optimally for analyzing that intriguing protein structure! Let me know if you need any further explanation or additional parameters.""}]","{'mime_type': 'application/json', 'value': '{""id"":""chatcmpl-ARHvtkWEWKoJ1lmAzbSVALBmtXtur"",""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""message"":{""content"":""**Parameters:**\\n\\n1. `ca_only`: **False**\\n2. `use_soluble_model`: **Not explicitly recommended**, assess hydrophobicity and context\\n3. `num_seq_per_target`: **20**\\n4. `sampling_temp`: **0.1**\\n\\n---\\n\\nHello! Let\'s dive into how we might configure ProteinMPNN to analyze the structure you\'ve provided.\\n\\nFirst up, the `ca_only` setting: this parameter determines whether the model should consider only the alpha carbon (CA) atoms of the protein backbone, or utilize all atom positions provided in the PDB file. In your case, the PDB data you\'ve provided includes detailed atomic information, not just the alpha carbons. Using `ca_only: False` allows the model to leverage this comprehensive structure data, giving it more context about the protein\'s geometry and potentially improving accuracy in modeling interactions and structure.\\n\\nNext is the `use_soluble_model` option, which would determine if a model optimized for soluble proteins should be used. To pinpoint whether this applies, you need to consider the protein\'s intrinsic properties and its environment. If the protein is part of a membrane or is an integral membrane protein, you may want a model fine-tuned for that context instead. Given just the snippet of structure, a decision here isn\'t explicit unless further biochemical context suggests solubility. So, generally, you might default to the context inferred from any additional biological information about the protein.\\n\\nThen, let’s talk about `num_seq_per_target`. This parameter indicates how many sequences you\'d like the model to generate per target structure. Setting this to about **20** strikes a good balance between getting a variety of sequences to explore the diversity of potential interactions and keeping computational demands reasonable. It\'s often enough to provide insightful variations without overwhelming analysis requirements.\\n\\nLastly, for `sampling_temp`, this parameter affects the randomness of sequence sampling — akin to how \'explorative\' the model should be. A lower temperature like **0.1** encourages the generation of sequences that are more energetically favorable or stable, essentially prioritizing quality over variety. This low temperature is a reasonable starting point if you\'re looking for reliable sequences that are more likely to maintain the structural integrity of your protein when you\'re inferring structure-function relationships.\\n\\nHopefully, this gives you a clear pathway to setting up your ProteinMPNN run optimally for analyzing that intriguing protein structure! Let me know if you need any further explanation or additional parameters."",""refusal"":null,""role"":""assistant""}}],""created"":1731067153,""model"":""gpt-4o-2024-08-06"",""object"":""chat.completion"",""system_fingerprint"":""fp_45cf54deae"",""usage"":{""completion_tokens"":490,""prompt_tokens"":406,""total_tokens"":896,""completion_tokens_details"":{""accepted_prediction_tokens"":0,""audio_tokens"":0,""reasoning_tokens"":0,""rejected_prediction_tokens"":0},""prompt_tokens_details"":{""audio_tokens"":0,""cached_tokens"":0}}}'}","{'invocation_parameters': '{""model"": ""gpt-4o""}', 'input_messages': [{'message': {'content': 'You are an expert in ProteinMPNN modeling. Provide parameters followed by a clear, conversational explanation.', 'role': 'system'}}, {'message': {'content': ""\n        Analyze the following PDB content and suggest appropriate parameters for running ProteinMPNN:\n\n        ATOM      1  N   GLY A   1      -7.161   1.641 -19.813  1.00  0.00\nATOM      2  CA  GLY A   1      -7.289   0.193 -19.927  1.00  0.00\nATOM      3  C   GLY A   1      -6.237  -0.520 -19.088  1.00  0.00\nATOM      4  O   GLY A   1      -5.146   0.006 -18.863  1.00  0.00\nATOM      5  N   GLY A   2      -6.620  -1.470 -18.394  1.00  0.00\nATOM      6  CA  GLY A   2      -5.691  -2.169 -17.514  1.00  0.00\nATOM      7  C   GLY A   2      -4.499  -2.712 -18.292  1.00  0.00\nATOM      8  O   GLY A   2     ...\n\n        Provide a clear, conversational explanation of:\n        1. ca_only setting and why it's appropriate\n        2. use_soluble_model choice based on the structure\n        3. num_seq_per_target recommendation\n        4. sampling_temp selection\n\n        Format your response with clear parameter values first, followed by a natural explanation suitable for voice synthesis.\n        "", 'role': 'user'}}], 'token_count': {'total': 896, 'prompt': 406, 'completion': 490}, 'system': 'openai', 'provider': 'openai', 'output_messages': [{'message': {'content': ""**Parameters:**\n\n1. `ca_only`: **False**\n2. `use_soluble_model`: **Not explicitly recommended**, assess hydrophobicity and context\n3. `num_seq_per_target`: **20**\n4. `sampling_temp`: **0.1**\n\n---\n\nHello! Let's dive into how we might configure ProteinMPNN to analyze the structure you've provided.\n\nFirst up, the `ca_only` setting: this parameter determines whether the model should consider only the alpha carbon (CA) atoms of the protein backbone, or utilize all atom positions provided in the PDB file. In your case, the PDB data you've provided includes detailed atomic information, not just the alpha carbons. Using `ca_only: False` allows the model to leverage this comprehensive structure data, giving it more context about the protein's geometry and potentially improving accuracy in modeling interactions and structure.\n\nNext is the `use_soluble_model` option, which would determine if a model optimized for soluble proteins should be used. To pinpoint whether this applies, you need to consider the protein's intrinsic properties and its environment. If the protein is part of a membrane or is an integral membrane protein, you may want a model fine-tuned for that context instead. Given just the snippet of structure, a decision here isn't explicit unless further biochemical context suggests solubility. So, generally, you might default to the context inferred from any additional biological information about the protein.\n\nThen, let’s talk about `num_seq_per_target`. This parameter indicates how many sequences you'd like the model to generate per target structure. Setting this to about **20** strikes a good balance between getting a variety of sequences to explore the diversity of potential interactions and keeping computational demands reasonable. It's often enough to provide insightful variations without overwhelming analysis requirements.\n\nLastly, for `sampling_temp`, this parameter affects the randomness of sequence sampling — akin to how 'explorative' the model should be. A lower temperature like **0.1** encourages the generation of sequences that are more energetically favorable or stable, essentially prioritizing quality over variety. This low temperature is a reasonable starting point if you're looking for reliable sequences that are more likely to maintain the structural integrity of your protein when you're inferring structure-function relationships.\n\nHopefully, this gives you a clear pathway to setting up your ProteinMPNN run optimally for analyzing that intriguing protein structure! Let me know if you need any further explanation or additional parameters."", 'role': 'assistant'}}], 'model_name': 'gpt-4o-2024-08-06'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""messages"": [{""role"": ""system"", ""content"": ""You are an expert in ProteinMPNN modeling. Provide parameters followed by a clear, conversational explanation.""}, {""role"": ""user"", ""content"": ""\\n        Analyze the following PDB content and suggest appropriate parameters for running ProteinMPNN:\\n\\n        ATOM      1  N   GLY A   1      -7.161   1.641 -19.813  1.00  0.00\\nATOM      2  CA  GLY A   1      -7.289   0.193 -19.927  1.00  0.00\\nATOM      3  C   GLY A   1      -6.237  -0.520 -19.088  1.00  0.00\\nATOM      4  O   GLY A   1      -5.146   0.006 -18.863  1.00  0.00\\nATOM      5  N   GLY A   2      -6.620  -1.470 -18.394  1.00  0.00\\nATOM      6  CA  GLY A   2      -5.691  -2.169 -17.514  1.00  0.00\\nATOM      7  C   GLY A   2      -4.499  -2.712 -18.292  1.00  0.00\\nATOM      8  O   GLY A   2     ...\\n\\n        Provide a clear, conversational explanation of:\\n        1. ca_only setting and why it\'s appropriate\\n        2. use_soluble_model choice based on the structure\\n        3. num_seq_per_target recommendation\\n        4. sampling_temp selection\\n\\n        Format your response with clear parameter values first, followed by a natural explanation suitable for voice synthesis.\\n        ""}], ""model"": ""gpt-4o""}'}",{},,,,,
RGF0YXNldEV4YW1wbGU6MTIz,,,{'value': '<openai._legacy_response.HttpxBinaryResponseContent object at 0x7a6eabb3d1b0>'},"{'invocation_parameters': '{""input"": ""1. `ca_only`: **False**\\n2. `use_soluble_model`: **Not explicitly recommended**, assess hydrophobicity and context\\n3. `num_seq_per_target`: **20**\\n4. `sampling_temp`: **0.1**\\n\\n---\\n\\nHello! Let\'s dive into how we might configure ProteinMPNN to analyze the structure you\'ve provided.\\n\\nFirst up, the `ca_only` setting: this parameter determines whether the model should consider only the alpha carbon (CA) atoms of the protein backbone, or utilize all atom positions provided in the PDB file. In your case, the PDB data you\'ve provided includes detailed atomic information, not just the alpha carbons. Using `ca_only: False` allows the model to leverage this comprehensive structure data, giving it more context about the protein\'s geometry and potentially improving accuracy in modeling interactions and structure.\\n\\nNext is the `use_soluble_model` option, which would determine if a model optimized for soluble proteins should be used. To pinpoint whether this applies, you need to consider the protein\'s intrinsic properties and its environment. If the protein is part of a membrane or is an integral membrane protein, you may want a model fine-tuned for that context instead. Given just the snippet of structure, a decision here isn\'t explicit unless further biochemical context suggests solubility. So, generally, you might default to the context inferred from any additional biological information about the protein.\\n\\nThen, let’s talk about `num_seq_per_target`. This parameter indicates how many sequences you\'d like the model to generate per target structure. Setting this to about **20** strikes a good balance between getting a variety of sequences to explore the diversity of potential interactions and keeping computational demands reasonable. It\'s often enough to provide insightful variations without overwhelming analysis requirements.\\n\\nLastly, for `sampling_temp`, this parameter affects the randomness of sequence sampling — akin to how \'explorative\' the model should be. A lower temperature like **0.1** encourages the generation of sequences that are more energetically favorable or stable, essentially prioritizing quality over variety. This low temperature is a reasonable starting point if you\'re looking for reliable sequences that are more likely to maintain the structural integrity of your protein when you\'re inferring structure-function relationships.\\n\\nHopefully, this gives you a clear pathway to setting up your ProteinMPNN run optimally for analyzing that intriguing protein structure! Let me know if you need any further explanation or additional parameters."", ""model"": ""tts-1"", ""voice"": ""alloy""}', 'provider': 'openai', 'system': 'openai'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""input"": ""1. `ca_only`: **False**\\n2. `use_soluble_model`: **Not explicitly recommended**, assess hydrophobicity and context\\n3. `num_seq_per_target`: **20**\\n4. `sampling_temp`: **0.1**\\n\\n---\\n\\nHello! Let\'s dive into how we might configure ProteinMPNN to analyze the structure you\'ve provided.\\n\\nFirst up, the `ca_only` setting: this parameter determines whether the model should consider only the alpha carbon (CA) atoms of the protein backbone, or utilize all atom positions provided in the PDB file. In your case, the PDB data you\'ve provided includes detailed atomic information, not just the alpha carbons. Using `ca_only: False` allows the model to leverage this comprehensive structure data, giving it more context about the protein\'s geometry and potentially improving accuracy in modeling interactions and structure.\\n\\nNext is the `use_soluble_model` option, which would determine if a model optimized for soluble proteins should be used. To pinpoint whether this applies, you need to consider the protein\'s intrinsic properties and its environment. If the protein is part of a membrane or is an integral membrane protein, you may want a model fine-tuned for that context instead. Given just the snippet of structure, a decision here isn\'t explicit unless further biochemical context suggests solubility. So, generally, you might default to the context inferred from any additional biological information about the protein.\\n\\nThen, let’s talk about `num_seq_per_target`. This parameter indicates how many sequences you\'d like the model to generate per target structure. Setting this to about **20** strikes a good balance between getting a variety of sequences to explore the diversity of potential interactions and keeping computational demands reasonable. It\'s often enough to provide insightful variations without overwhelming analysis requirements.\\n\\nLastly, for `sampling_temp`, this parameter affects the randomness of sequence sampling — akin to how \'explorative\' the model should be. A lower temperature like **0.1** encourages the generation of sequences that are more energetically favorable or stable, essentially prioritizing quality over variety. This low temperature is a reasonable starting point if you\'re looking for reliable sequences that are more likely to maintain the structural integrity of your protein when you\'re inferring structure-function relationships.\\n\\nHopefully, this gives you a clear pathway to setting up your ProteinMPNN run optimally for analyzing that intriguing protein structure! Let me know if you need any further explanation or additional parameters."", ""model"": ""tts-1"", ""voice"": ""alloy""}'}",{},"1. `ca_only`: **False**
2. `use_soluble_model`: **Not explicitly recommended**, assess hydrophobicity and context
3. `num_seq_per_target`: **20**
4. `sampling_temp`: **0.1**

---

Hello! Let's dive into how we might configure ProteinMPNN to analyze the structure you've provided.

First up, the `ca_only` setting: this parameter determines whether the model should consider only the alpha carbon (CA) atoms of the protein backbone, or utilize all atom positions provided in the PDB file. In your case, the PDB data you've provided includes detailed atomic information, not just the alpha carbons. Using `ca_only: False` allows the model to leverage this comprehensive structure data, giving it more context about the protein's geometry and potentially improving accuracy in modeling interactions and structure.

Next is the `use_soluble_model` option, which would determine if a model optimized for soluble proteins should be used. To pinpoint whether this applies, you need to consider the protein's intrinsic properties and its environment. If the protein is part of a membrane or is an integral membrane protein, you may want a model fine-tuned for that context instead. Given just the snippet of structure, a decision here isn't explicit unless further biochemical context suggests solubility. So, generally, you might default to the context inferred from any additional biological information about the protein.

Then, let’s talk about `num_seq_per_target`. This parameter indicates how many sequences you'd like the model to generate per target structure. Setting this to about **20** strikes a good balance between getting a variety of sequences to explore the diversity of potential interactions and keeping computational demands reasonable. It's often enough to provide insightful variations without overwhelming analysis requirements.

Lastly, for `sampling_temp`, this parameter affects the randomness of sequence sampling — akin to how 'explorative' the model should be. A lower temperature like **0.1** encourages the generation of sequences that are more energetically favorable or stable, essentially prioritizing quality over variety. This low temperature is a reasonable starting point if you're looking for reliable sequences that are more likely to maintain the structural integrity of your protein when you're inferring structure-function relationships.

Hopefully, this gives you a clear pathway to setting up your ProteinMPNN run optimally for analyzing that intriguing protein structure! Let me know if you need any further explanation or additional parameters.",tts-1,alloy,<openai._legacy_response.HttpxBinaryResponseContent object at 0x7a6eabb3d1b0>,
RGF0YXNldEV4YW1wbGU6MTI0,"[{'role': 'system', 'content': 'You are an expert in molecular generation and optimization using MolMIM.'}, {'role': 'user', 'content': '\n        Analyze the following SMILES string and suggest appropriate parameters for running MolMIM:\n\n        CN1C=C(C2=CC=CC=C21)C3=NC(=NC=C3)NC4=C(C=C(C(=C4)NC(=O)C=C)N(C)CCN(C)C)OC\n\n        Provide the following parameters:\n        1. algorithm\n        2. num_molecules\n        3. property_name\n        4. minimize\n        5. min_similarity\n        6. particles\n        7. iterations\n\n        Explain your reasoning for each parameter.\n        '}]","[{'role': 'assistant', 'content': 'To analyze the given SMILES string and suggest appropriate parameters for running MolMIM, we should first understand the nature of the molecule and the potential goals of its optimization. The SMILES string provided:\n\n```\nCN1C=C(C2=CC=CC=C21)C3=NC(=NC=C3)NC4=C(C=C(C(=C4)NC(=O)C=C)N(C)CCN(C)C)OC\n```\n\ncorresponds to a complex structure featuring multiple rings and functionalities. Let\'s consider the parameters:\n\n1. **Algorithm**: The choice of algorithm largely depends on the optimization goals, such as optimizing molecular properties or exploring chemical space. For complex polycyclic aromatic compounds like this one, using a genetic algorithm may be beneficial due to its robustness in exploring diverse chemical spaces and optimizing multiple properties simultaneously.\n\n   **Suggested value**: `""genetic_algorithm""`\n\n2. **Num_molecules**: This parameter defines how many new molecules you want to generate. Given the complexity of the parent molecule, a moderate number of molecules would be suitable to allow enough diversity while managing computational resources.\n\n   **Suggested value**: `100`\n\n3. **Property_name**: The property of interest should be directly related to the intended application of the molecule. Common properties include drug-likeness, solubility, binding affinity (if a target site is known), or a specific pharmacokinetic property. For a molecule with potential pharmaceutical applications, you might choose ""binding affinity"" if a target is known.\n\n   **Suggested value**: `""binding_affinity""` (or another property relevant to your application)\n\n4. **Minimize**: Whether to minimize or maximize the selected property depends on the property itself. For properties like energy, you typically minimize, whereas for affinity or logP, you might maximize to enhance interaction or solubility.\n\n   **Suggested value**: `true` (if minimizing energy) / `false` (if maximizing binding affinity)\n\n5. **Min_similarity**: This parameter sets the threshold for similarity to the original molecule, balancing exploration and exploitation. A typical value might range from 0.4 to 0.6 to retain some structural similarity but allow meaningful exploration.\n\n   **Suggested value**: `0.5`\n\n6. **Particles**: Particles represent the population size in optimization algorithms. A larger number allows more exploration but increases computational cost. For our molecule, a moderate particle size is recommended.\n\n   **Suggested value**: `50`\n\n7. **Iterations**: The number of optimization steps should be sufficient to allow convergence but not excessive to save computational resources. For complex molecules, more iterations might be necessary.\n\n   **Suggested value**: `500`\n\nEach of these parameters is selected to balance computational efficiency with the need to explore the chemical space thoroughly, depending on the specific goals of molecular optimization. Adjustments might be needed based on specific experimental goals or computational resources.'}]","{'mime_type': 'application/json', 'value': '{""id"":""chatcmpl-ARSbCj2wWKfKqrZY07JBORwwOEVGc"",""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""message"":{""content"":""To analyze the given SMILES string and suggest appropriate parameters for running MolMIM, we should first understand the nature of the molecule and the potential goals of its optimization. The SMILES string provided:\\n\\n```\\nCN1C=C(C2=CC=CC=C21)C3=NC(=NC=C3)NC4=C(C=C(C(=C4)NC(=O)C=C)N(C)CCN(C)C)OC\\n```\\n\\ncorresponds to a complex structure featuring multiple rings and functionalities. Let\'s consider the parameters:\\n\\n1. **Algorithm**: The choice of algorithm largely depends on the optimization goals, such as optimizing molecular properties or exploring chemical space. For complex polycyclic aromatic compounds like this one, using a genetic algorithm may be beneficial due to its robustness in exploring diverse chemical spaces and optimizing multiple properties simultaneously.\\n\\n   **Suggested value**: `\\""genetic_algorithm\\""`\\n\\n2. **Num_molecules**: This parameter defines how many new molecules you want to generate. Given the complexity of the parent molecule, a moderate number of molecules would be suitable to allow enough diversity while managing computational resources.\\n\\n   **Suggested value**: `100`\\n\\n3. **Property_name**: The property of interest should be directly related to the intended application of the molecule. Common properties include drug-likeness, solubility, binding affinity (if a target site is known), or a specific pharmacokinetic property. For a molecule with potential pharmaceutical applications, you might choose \\""binding affinity\\"" if a target is known.\\n\\n   **Suggested value**: `\\""binding_affinity\\""` (or another property relevant to your application)\\n\\n4. **Minimize**: Whether to minimize or maximize the selected property depends on the property itself. For properties like energy, you typically minimize, whereas for affinity or logP, you might maximize to enhance interaction or solubility.\\n\\n   **Suggested value**: `true` (if minimizing energy) / `false` (if maximizing binding affinity)\\n\\n5. **Min_similarity**: This parameter sets the threshold for similarity to the original molecule, balancing exploration and exploitation. A typical value might range from 0.4 to 0.6 to retain some structural similarity but allow meaningful exploration.\\n\\n   **Suggested value**: `0.5`\\n\\n6. **Particles**: Particles represent the population size in optimization algorithms. A larger number allows more exploration but increases computational cost. For our molecule, a moderate particle size is recommended.\\n\\n   **Suggested value**: `50`\\n\\n7. **Iterations**: The number of optimization steps should be sufficient to allow convergence but not excessive to save computational resources. For complex molecules, more iterations might be necessary.\\n\\n   **Suggested value**: `500`\\n\\nEach of these parameters is selected to balance computational efficiency with the need to explore the chemical space thoroughly, depending on the specific goals of molecular optimization. Adjustments might be needed based on specific experimental goals or computational resources."",""refusal"":null,""role"":""assistant""}}],""created"":1731108154,""model"":""gpt-4o-2024-08-06"",""object"":""chat.completion"",""system_fingerprint"":""fp_45cf54deae"",""usage"":{""completion_tokens"":600,""prompt_tokens"":158,""total_tokens"":758,""completion_tokens_details"":{""accepted_prediction_tokens"":0,""audio_tokens"":0,""reasoning_tokens"":0,""rejected_prediction_tokens"":0},""prompt_tokens_details"":{""audio_tokens"":0,""cached_tokens"":0}}}'}","{'invocation_parameters': '{""model"": ""gpt-4o""}', 'input_messages': [{'message': {'content': 'You are an expert in molecular generation and optimization using MolMIM.', 'role': 'system'}}, {'message': {'content': '\n        Analyze the following SMILES string and suggest appropriate parameters for running MolMIM:\n\n        CN1C=C(C2=CC=CC=C21)C3=NC(=NC=C3)NC4=C(C=C(C(=C4)NC(=O)C=C)N(C)CCN(C)C)OC\n\n        Provide the following parameters:\n        1. algorithm\n        2. num_molecules\n        3. property_name\n        4. minimize\n        5. min_similarity\n        6. particles\n        7. iterations\n\n        Explain your reasoning for each parameter.\n        ', 'role': 'user'}}], 'token_count': {'total': 758, 'prompt': 158, 'completion': 600}, 'system': 'openai', 'provider': 'openai', 'output_messages': [{'message': {'content': 'To analyze the given SMILES string and suggest appropriate parameters for running MolMIM, we should first understand the nature of the molecule and the potential goals of its optimization. The SMILES string provided:\n\n```\nCN1C=C(C2=CC=CC=C21)C3=NC(=NC=C3)NC4=C(C=C(C(=C4)NC(=O)C=C)N(C)CCN(C)C)OC\n```\n\ncorresponds to a complex structure featuring multiple rings and functionalities. Let\'s consider the parameters:\n\n1. **Algorithm**: The choice of algorithm largely depends on the optimization goals, such as optimizing molecular properties or exploring chemical space. For complex polycyclic aromatic compounds like this one, using a genetic algorithm may be beneficial due to its robustness in exploring diverse chemical spaces and optimizing multiple properties simultaneously.\n\n   **Suggested value**: `""genetic_algorithm""`\n\n2. **Num_molecules**: This parameter defines how many new molecules you want to generate. Given the complexity of the parent molecule, a moderate number of molecules would be suitable to allow enough diversity while managing computational resources.\n\n   **Suggested value**: `100`\n\n3. **Property_name**: The property of interest should be directly related to the intended application of the molecule. Common properties include drug-likeness, solubility, binding affinity (if a target site is known), or a specific pharmacokinetic property. For a molecule with potential pharmaceutical applications, you might choose ""binding affinity"" if a target is known.\n\n   **Suggested value**: `""binding_affinity""` (or another property relevant to your application)\n\n4. **Minimize**: Whether to minimize or maximize the selected property depends on the property itself. For properties like energy, you typically minimize, whereas for affinity or logP, you might maximize to enhance interaction or solubility.\n\n   **Suggested value**: `true` (if minimizing energy) / `false` (if maximizing binding affinity)\n\n5. **Min_similarity**: This parameter sets the threshold for similarity to the original molecule, balancing exploration and exploitation. A typical value might range from 0.4 to 0.6 to retain some structural similarity but allow meaningful exploration.\n\n   **Suggested value**: `0.5`\n\n6. **Particles**: Particles represent the population size in optimization algorithms. A larger number allows more exploration but increases computational cost. For our molecule, a moderate particle size is recommended.\n\n   **Suggested value**: `50`\n\n7. **Iterations**: The number of optimization steps should be sufficient to allow convergence but not excessive to save computational resources. For complex molecules, more iterations might be necessary.\n\n   **Suggested value**: `500`\n\nEach of these parameters is selected to balance computational efficiency with the need to explore the chemical space thoroughly, depending on the specific goals of molecular optimization. Adjustments might be needed based on specific experimental goals or computational resources.', 'role': 'assistant'}}], 'model_name': 'gpt-4o-2024-08-06'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""messages"": [{""role"": ""system"", ""content"": ""You are an expert in molecular generation and optimization using MolMIM.""}, {""role"": ""user"", ""content"": ""\\n        Analyze the following SMILES string and suggest appropriate parameters for running MolMIM:\\n\\n        CN1C=C(C2=CC=CC=C21)C3=NC(=NC=C3)NC4=C(C=C(C(=C4)NC(=O)C=C)N(C)CCN(C)C)OC\\n\\n        Provide the following parameters:\\n        1. algorithm\\n        2. num_molecules\\n        3. property_name\\n        4. minimize\\n        5. min_similarity\\n        6. particles\\n        7. iterations\\n\\n        Explain your reasoning for each parameter.\\n        ""}], ""model"": ""gpt-4o""}'}",{},,,,,
RGF0YXNldEV4YW1wbGU6MTI1,"[{'role': 'system', 'content': 'You are an expert in molecular generation and optimization using MolMIM.'}, {'role': 'user', 'content': '\n        Analyze the following SMILES string and suggest appropriate parameters for running MolMIM:\n\n        CN1C=C(C2=CC=CC=C21)C3=NC(=NC=C3)NC4=C(C=C(C(=C4)NC(=O)C=C)N(C)CCN(C)C)OC\n\n        Provide the following parameters:\n        1. algorithm\n        2. num_molecules\n        3. property_name\n        4. minimize\n        5. min_similarity\n        6. particles\n        7. iterations\n\n        Explain your reasoning for each parameter.\n        '}]","[{'role': 'assistant', 'content': 'Analyzing the provided SMILES string, which corresponds to a complex heterocyclic compound containing an indole and a pyrimidine core with various substitutions:\n\n1. **Algorithm**: Considering the complexity and functionalized nature of the molecule, a robust algorithm that can handle multiple objectives and complex molecule structures like ""Genetic Algorithm"" or ""Simulated Annealing"" is suitable for exploring a diverse chemical space and optimizing molecular properties efficiently.\n\n2. **Num_molecules**: With the goal of exploring a variety of potential derivatives, setting this parameter to 100-200 can provide a sufficiently diverse set of analogues while being computationally feasible.\n\n3. **Property_name**: Depending on the intended application of the molecule, properties such as ""drug-likeness"", ""solubility"", or ""binding affinity"" could be relevant. For a broad application, starting with ""drug-likeness"" is often beneficial, as it encompasses a range of pharmacokinetic and pharmacodynamic parameters.\n\n4. **Minimize**: To enhance properties like binding affinity or reduce molecular weight, minimize parameters related to steric hindrance or molecular size. Set this to `True` to ensure the algorithm will focus on optimizing these properties.\n\n5. **Min_similarity**: Setting a moderate similarity threshold (e.g., 0.4 to 0.6) would be advantageous. This allows for exploration of novel chemical spaces while maintaining a degree of structural similarity to ensure that new molecules retain desired core structures or functionalities.\n\n6. **Particles**: Setting this between 20-40 can strike a balance between exploration and computational efficiency, particularly if a swarm-based algorithm like Particle Swarm Optimization (PSO) is being used.\n\n7. **Iterations**: Depending on the complexity of the search space and the desired thoroughness of the search, setting the number to 500-1000 iterations should allow sufficient exploration to identify optimal structures.\n\n**Reasoning**:\n- A complex molecule with multiple rings and functional groups requires a sophisticated algorithm to navigate the multidimensional optimization landscape. Genetic algorithms or similar methods are well-suited for this purpose.\n- A moderate number of molecules and particles ensures a good balance between computational load and the breadth of the chemical space explored.\n- Selecting appropriate properties to optimize and setting a suitable similarity threshold helps to target molecules that are both novel and relevant to the desired application.\n- A sufficient number of iterations allows the algorithm to converge on optimized molecules within the set constraints and parameters efficiently.'}]","{'mime_type': 'application/json', 'value': '{""id"":""chatcmpl-ARSgJtHKKldFspNXB0snozEnGYDsA"",""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""message"":{""content"":""Analyzing the provided SMILES string, which corresponds to a complex heterocyclic compound containing an indole and a pyrimidine core with various substitutions:\\n\\n1. **Algorithm**: Considering the complexity and functionalized nature of the molecule, a robust algorithm that can handle multiple objectives and complex molecule structures like \\""Genetic Algorithm\\"" or \\""Simulated Annealing\\"" is suitable for exploring a diverse chemical space and optimizing molecular properties efficiently.\\n\\n2. **Num_molecules**: With the goal of exploring a variety of potential derivatives, setting this parameter to 100-200 can provide a sufficiently diverse set of analogues while being computationally feasible.\\n\\n3. **Property_name**: Depending on the intended application of the molecule, properties such as \\""drug-likeness\\"", \\""solubility\\"", or \\""binding affinity\\"" could be relevant. For a broad application, starting with \\""drug-likeness\\"" is often beneficial, as it encompasses a range of pharmacokinetic and pharmacodynamic parameters.\\n\\n4. **Minimize**: To enhance properties like binding affinity or reduce molecular weight, minimize parameters related to steric hindrance or molecular size. Set this to `True` to ensure the algorithm will focus on optimizing these properties.\\n\\n5. **Min_similarity**: Setting a moderate similarity threshold (e.g., 0.4 to 0.6) would be advantageous. This allows for exploration of novel chemical spaces while maintaining a degree of structural similarity to ensure that new molecules retain desired core structures or functionalities.\\n\\n6. **Particles**: Setting this between 20-40 can strike a balance between exploration and computational efficiency, particularly if a swarm-based algorithm like Particle Swarm Optimization (PSO) is being used.\\n\\n7. **Iterations**: Depending on the complexity of the search space and the desired thoroughness of the search, setting the number to 500-1000 iterations should allow sufficient exploration to identify optimal structures.\\n\\n**Reasoning**:\\n- A complex molecule with multiple rings and functional groups requires a sophisticated algorithm to navigate the multidimensional optimization landscape. Genetic algorithms or similar methods are well-suited for this purpose.\\n- A moderate number of molecules and particles ensures a good balance between computational load and the breadth of the chemical space explored.\\n- Selecting appropriate properties to optimize and setting a suitable similarity threshold helps to target molecules that are both novel and relevant to the desired application.\\n- A sufficient number of iterations allows the algorithm to converge on optimized molecules within the set constraints and parameters efficiently."",""refusal"":null,""role"":""assistant""}}],""created"":1731108471,""model"":""gpt-4o-2024-08-06"",""object"":""chat.completion"",""system_fingerprint"":""fp_45cf54deae"",""usage"":{""completion_tokens"":500,""prompt_tokens"":158,""total_tokens"":658,""completion_tokens_details"":{""accepted_prediction_tokens"":0,""audio_tokens"":0,""reasoning_tokens"":0,""rejected_prediction_tokens"":0},""prompt_tokens_details"":{""audio_tokens"":0,""cached_tokens"":0}}}'}","{'invocation_parameters': '{""model"": ""gpt-4o""}', 'input_messages': [{'message': {'content': 'You are an expert in molecular generation and optimization using MolMIM.', 'role': 'system'}}, {'message': {'content': '\n        Analyze the following SMILES string and suggest appropriate parameters for running MolMIM:\n\n        CN1C=C(C2=CC=CC=C21)C3=NC(=NC=C3)NC4=C(C=C(C(=C4)NC(=O)C=C)N(C)CCN(C)C)OC\n\n        Provide the following parameters:\n        1. algorithm\n        2. num_molecules\n        3. property_name\n        4. minimize\n        5. min_similarity\n        6. particles\n        7. iterations\n\n        Explain your reasoning for each parameter.\n        ', 'role': 'user'}}], 'token_count': {'total': 658, 'prompt': 158, 'completion': 500}, 'system': 'openai', 'provider': 'openai', 'output_messages': [{'message': {'content': 'Analyzing the provided SMILES string, which corresponds to a complex heterocyclic compound containing an indole and a pyrimidine core with various substitutions:\n\n1. **Algorithm**: Considering the complexity and functionalized nature of the molecule, a robust algorithm that can handle multiple objectives and complex molecule structures like ""Genetic Algorithm"" or ""Simulated Annealing"" is suitable for exploring a diverse chemical space and optimizing molecular properties efficiently.\n\n2. **Num_molecules**: With the goal of exploring a variety of potential derivatives, setting this parameter to 100-200 can provide a sufficiently diverse set of analogues while being computationally feasible.\n\n3. **Property_name**: Depending on the intended application of the molecule, properties such as ""drug-likeness"", ""solubility"", or ""binding affinity"" could be relevant. For a broad application, starting with ""drug-likeness"" is often beneficial, as it encompasses a range of pharmacokinetic and pharmacodynamic parameters.\n\n4. **Minimize**: To enhance properties like binding affinity or reduce molecular weight, minimize parameters related to steric hindrance or molecular size. Set this to `True` to ensure the algorithm will focus on optimizing these properties.\n\n5. **Min_similarity**: Setting a moderate similarity threshold (e.g., 0.4 to 0.6) would be advantageous. This allows for exploration of novel chemical spaces while maintaining a degree of structural similarity to ensure that new molecules retain desired core structures or functionalities.\n\n6. **Particles**: Setting this between 20-40 can strike a balance between exploration and computational efficiency, particularly if a swarm-based algorithm like Particle Swarm Optimization (PSO) is being used.\n\n7. **Iterations**: Depending on the complexity of the search space and the desired thoroughness of the search, setting the number to 500-1000 iterations should allow sufficient exploration to identify optimal structures.\n\n**Reasoning**:\n- A complex molecule with multiple rings and functional groups requires a sophisticated algorithm to navigate the multidimensional optimization landscape. Genetic algorithms or similar methods are well-suited for this purpose.\n- A moderate number of molecules and particles ensures a good balance between computational load and the breadth of the chemical space explored.\n- Selecting appropriate properties to optimize and setting a suitable similarity threshold helps to target molecules that are both novel and relevant to the desired application.\n- A sufficient number of iterations allows the algorithm to converge on optimized molecules within the set constraints and parameters efficiently.', 'role': 'assistant'}}], 'model_name': 'gpt-4o-2024-08-06'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""messages"": [{""role"": ""system"", ""content"": ""You are an expert in molecular generation and optimization using MolMIM.""}, {""role"": ""user"", ""content"": ""\\n        Analyze the following SMILES string and suggest appropriate parameters for running MolMIM:\\n\\n        CN1C=C(C2=CC=CC=C21)C3=NC(=NC=C3)NC4=C(C=C(C(=C4)NC(=O)C=C)N(C)CCN(C)C)OC\\n\\n        Provide the following parameters:\\n        1. algorithm\\n        2. num_molecules\\n        3. property_name\\n        4. minimize\\n        5. min_similarity\\n        6. particles\\n        7. iterations\\n\\n        Explain your reasoning for each parameter.\\n        ""}], ""model"": ""gpt-4o""}'}",{},,,,,
RGF0YXNldEV4YW1wbGU6MTI2,,,,,,,{},,,,,
RGF0YXNldEV4YW1wbGU6MTI3,,,,,,,{},,,,,
RGF0YXNldEV4YW1wbGU6MTI4,"[{'role': 'system', 'content': 'You are an expert in molecular generation and optimization using MolMIM.'}, {'role': 'user', 'content': '\n        Analyze the following SMILES string and suggest appropriate parameters for running MolMIM:\n\n        CN1C=C(C2=CC=CC=C21)C3=NC(=NC=C3)NC4=C(C=C(C(=C4)NC(=O)C=C)N(C)CCN(C)C)OC\n\n        Provide the following parameters:\n        1. algorithm\n        2. num_molecules\n        3. property_name\n        4. minimize\n        5. min_similarity\n        6. particles\n        7. iterations\n\n        Explain your reasoning for each parameter.\n        '}]","[{'role': 'assistant', 'content': ""Analyzing the provided SMILES string, we can see that the molecule is quite complex, featuring multiple aromatic rings, nitrogen-containing heterocycles, and functional groups like amides and ethers. This complexity suggests potential challenges in generating and optimizing similar structures, which should be reflected in the choice of parameters for MolMIM.\n\nHere's a suggested set of parameters with reasoning:\n\n1. **Algorithm**: Genetic Algorithm (GA)\n    - **Reasoning**: Genetic algorithms are effective for exploring large chemical spaces with diverse structures, especially when dealing with complex organic molecules featuring multifaceted functional groups. They help in navigating through various possible mutations and recombinations to find optimal structures.\n\n2. **Num_molecules**: 50-100\n    - **Reasoning**: Starting with 50-100 molecules provides a good balance, offering sufficient diversity to explore the chemical space while being computationally manageable. This number serves as a reasonable starting point for generating diverse candidate molecules similar to the given structure.\n\n3. **Property_name**: Drug-likeness or LogP\n    - **Reasoning**: Given the presence of multiple aromatic rings and nitrogen atoms, assessing properties like drug-likeness (using Lipinski's Rule of Five) or LogP (partition coefficient) is crucial. These properties give insights into the potential pharmacokinetic and distribution properties of the molecule, important for medicinal chemistry applications.\n\n4. **Minimize**: True\n    - **Reasoning**: Setting this parameter to 'True' ensures that the generated molecules are geometry optimized, leading to stable and energetically favorable conformations. It's crucial for ensuring the molecules can realistically exist and for accurate property evaluations.\n\n5. **Min_similarity**: 0.4-0.6\n    - **Reasoning**: A moderate minimum similarity requirement balances the need to generate molecules that are both novel and maintain key structural features of the original molecule. This range allows for structural exploration while retaining essential moieties that could be critical to the molecule's function.\n\n6. **Particles**: 20-30\n    - **Reasoning**: Utilizing 20-30 particles provides sufficient parallel processes to explore multiple molecular pathways simultaneously, increasing the likelihood of identifying high-quality candidates. It's a good number for balancing computational efficiency and thoroughness in exploration of the chemical space.\n\n7. **Iterations**: 1000-1500\n    - **Reasoning**: A higher number of iterations (1000-1500) allows for adequate exploration and optimization of the chemical space, especially given the complexity of the molecule. It ensures that the algorithm has enough time to converge on potentially optimized structures that meet the desired properties.\n\nThese parameters are designed to guide MolMIM in efficiently exploring the chemical space related to the supplied SMILES string, while considering both structural complexity and potential functional applications.""}]","{'mime_type': 'application/json', 'value': '{""id"":""chatcmpl-ARSmKPcQ5SSgCCgRIDj7IssABHCD9"",""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""message"":{""content"":""Analyzing the provided SMILES string, we can see that the molecule is quite complex, featuring multiple aromatic rings, nitrogen-containing heterocycles, and functional groups like amides and ethers. This complexity suggests potential challenges in generating and optimizing similar structures, which should be reflected in the choice of parameters for MolMIM.\\n\\nHere\'s a suggested set of parameters with reasoning:\\n\\n1. **Algorithm**: Genetic Algorithm (GA)\\n    - **Reasoning**: Genetic algorithms are effective for exploring large chemical spaces with diverse structures, especially when dealing with complex organic molecules featuring multifaceted functional groups. They help in navigating through various possible mutations and recombinations to find optimal structures.\\n\\n2. **Num_molecules**: 50-100\\n    - **Reasoning**: Starting with 50-100 molecules provides a good balance, offering sufficient diversity to explore the chemical space while being computationally manageable. This number serves as a reasonable starting point for generating diverse candidate molecules similar to the given structure.\\n\\n3. **Property_name**: Drug-likeness or LogP\\n    - **Reasoning**: Given the presence of multiple aromatic rings and nitrogen atoms, assessing properties like drug-likeness (using Lipinski\'s Rule of Five) or LogP (partition coefficient) is crucial. These properties give insights into the potential pharmacokinetic and distribution properties of the molecule, important for medicinal chemistry applications.\\n\\n4. **Minimize**: True\\n    - **Reasoning**: Setting this parameter to \'True\' ensures that the generated molecules are geometry optimized, leading to stable and energetically favorable conformations. It\'s crucial for ensuring the molecules can realistically exist and for accurate property evaluations.\\n\\n5. **Min_similarity**: 0.4-0.6\\n    - **Reasoning**: A moderate minimum similarity requirement balances the need to generate molecules that are both novel and maintain key structural features of the original molecule. This range allows for structural exploration while retaining essential moieties that could be critical to the molecule\'s function.\\n\\n6. **Particles**: 20-30\\n    - **Reasoning**: Utilizing 20-30 particles provides sufficient parallel processes to explore multiple molecular pathways simultaneously, increasing the likelihood of identifying high-quality candidates. It\'s a good number for balancing computational efficiency and thoroughness in exploration of the chemical space.\\n\\n7. **Iterations**: 1000-1500\\n    - **Reasoning**: A higher number of iterations (1000-1500) allows for adequate exploration and optimization of the chemical space, especially given the complexity of the molecule. It ensures that the algorithm has enough time to converge on potentially optimized structures that meet the desired properties.\\n\\nThese parameters are designed to guide MolMIM in efficiently exploring the chemical space related to the supplied SMILES string, while considering both structural complexity and potential functional applications."",""refusal"":null,""role"":""assistant""}}],""created"":1731108844,""model"":""gpt-4o-2024-08-06"",""object"":""chat.completion"",""system_fingerprint"":""fp_45cf54deae"",""usage"":{""completion_tokens"":572,""prompt_tokens"":158,""total_tokens"":730,""completion_tokens_details"":{""accepted_prediction_tokens"":0,""audio_tokens"":0,""reasoning_tokens"":0,""rejected_prediction_tokens"":0},""prompt_tokens_details"":{""audio_tokens"":0,""cached_tokens"":0}}}'}","{'invocation_parameters': '{""model"": ""gpt-4o""}', 'input_messages': [{'message': {'content': 'You are an expert in molecular generation and optimization using MolMIM.', 'role': 'system'}}, {'message': {'content': '\n        Analyze the following SMILES string and suggest appropriate parameters for running MolMIM:\n\n        CN1C=C(C2=CC=CC=C21)C3=NC(=NC=C3)NC4=C(C=C(C(=C4)NC(=O)C=C)N(C)CCN(C)C)OC\n\n        Provide the following parameters:\n        1. algorithm\n        2. num_molecules\n        3. property_name\n        4. minimize\n        5. min_similarity\n        6. particles\n        7. iterations\n\n        Explain your reasoning for each parameter.\n        ', 'role': 'user'}}], 'token_count': {'total': 730, 'prompt': 158, 'completion': 572}, 'system': 'openai', 'provider': 'openai', 'output_messages': [{'message': {'content': ""Analyzing the provided SMILES string, we can see that the molecule is quite complex, featuring multiple aromatic rings, nitrogen-containing heterocycles, and functional groups like amides and ethers. This complexity suggests potential challenges in generating and optimizing similar structures, which should be reflected in the choice of parameters for MolMIM.\n\nHere's a suggested set of parameters with reasoning:\n\n1. **Algorithm**: Genetic Algorithm (GA)\n    - **Reasoning**: Genetic algorithms are effective for exploring large chemical spaces with diverse structures, especially when dealing with complex organic molecules featuring multifaceted functional groups. They help in navigating through various possible mutations and recombinations to find optimal structures.\n\n2. **Num_molecules**: 50-100\n    - **Reasoning**: Starting with 50-100 molecules provides a good balance, offering sufficient diversity to explore the chemical space while being computationally manageable. This number serves as a reasonable starting point for generating diverse candidate molecules similar to the given structure.\n\n3. **Property_name**: Drug-likeness or LogP\n    - **Reasoning**: Given the presence of multiple aromatic rings and nitrogen atoms, assessing properties like drug-likeness (using Lipinski's Rule of Five) or LogP (partition coefficient) is crucial. These properties give insights into the potential pharmacokinetic and distribution properties of the molecule, important for medicinal chemistry applications.\n\n4. **Minimize**: True\n    - **Reasoning**: Setting this parameter to 'True' ensures that the generated molecules are geometry optimized, leading to stable and energetically favorable conformations. It's crucial for ensuring the molecules can realistically exist and for accurate property evaluations.\n\n5. **Min_similarity**: 0.4-0.6\n    - **Reasoning**: A moderate minimum similarity requirement balances the need to generate molecules that are both novel and maintain key structural features of the original molecule. This range allows for structural exploration while retaining essential moieties that could be critical to the molecule's function.\n\n6. **Particles**: 20-30\n    - **Reasoning**: Utilizing 20-30 particles provides sufficient parallel processes to explore multiple molecular pathways simultaneously, increasing the likelihood of identifying high-quality candidates. It's a good number for balancing computational efficiency and thoroughness in exploration of the chemical space.\n\n7. **Iterations**: 1000-1500\n    - **Reasoning**: A higher number of iterations (1000-1500) allows for adequate exploration and optimization of the chemical space, especially given the complexity of the molecule. It ensures that the algorithm has enough time to converge on potentially optimized structures that meet the desired properties.\n\nThese parameters are designed to guide MolMIM in efficiently exploring the chemical space related to the supplied SMILES string, while considering both structural complexity and potential functional applications."", 'role': 'assistant'}}], 'model_name': 'gpt-4o-2024-08-06'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""messages"": [{""role"": ""system"", ""content"": ""You are an expert in molecular generation and optimization using MolMIM.""}, {""role"": ""user"", ""content"": ""\\n        Analyze the following SMILES string and suggest appropriate parameters for running MolMIM:\\n\\n        CN1C=C(C2=CC=CC=C21)C3=NC(=NC=C3)NC4=C(C=C(C(=C4)NC(=O)C=C)N(C)CCN(C)C)OC\\n\\n        Provide the following parameters:\\n        1. algorithm\\n        2. num_molecules\\n        3. property_name\\n        4. minimize\\n        5. min_similarity\\n        6. particles\\n        7. iterations\\n\\n        Explain your reasoning for each parameter.\\n        ""}], ""model"": ""gpt-4o""}'}",{},,,,,
RGF0YXNldEV4YW1wbGU6MTI5,"[{'role': 'system', 'content': 'You are an expert in molecular generation and optimization using MolMIM.'}, {'role': 'user', 'content': '\n        Analyze the following SMILES string and suggest appropriate parameters for running MolMIM:\n\n        CN1C=C(C2=CC=CC=C21)C3=NC(=NC=C3)NC4=C(C=C(C(=C4)NC(=O)C=C)N(C)CCN(C)C)OC\n\n        Provide the following parameters:\n        1. algorithm\n        2. num_molecules\n        3. property_name\n        4. minimize\n        5. min_similarity\n        6. particles\n        7. iterations\n\n        Explain your reasoning for each parameter.\n        '}]","[{'role': 'assistant', 'content': ""To suggest appropriate parameters for running MolMIM on the given SMILES string, we need to consider the complexity and characteristics of the molecule represented, as well as the typical setups used in molecular generation and optimization tasks. Let's analyze the given molecule first.\n\n### Analysis:\nThe SMILES string provided represents a complex organic molecule with several aromatic rings, nitrogen atoms, and functional groups like amine, ether, and a ketone component. This structure suggests that the molecule is relatively large and complex, which often implies specific challenges in optimization and molecular generation processes, such as computational demands and chemical feasibility.\n\n### Suggested Parameters:\n\n1. **Algorithm:**\n   - **Suggested value:** Genetic Algorithm (GA)\n   - **Reasoning:** Given the complexity of the molecule, a Genetic Algorithm is often suitable for navigating large search spaces effectively, especially for complex molecules with multiple rings and heteroatoms. GA can efficiently find diverse and optimal solutions through evolutionary strategies.\n\n2. **Num_molecules:**\n   - **Suggested value:** 100 to 200\n   - **Reasoning:** A moderate number of molecular candidates ensures a good balance between diversity and computational feasibility. This allows sufficient exploration of the molecular space while keeping calculations manageable.\n\n3. **Property_name:**\n   - **Suggested value:** Drug-likeness or solubility\n   - **Reasoning:** Based on the complexity of the molecule, improving its drug-likeness or optimizing for better solubility could be prioritized as these properties are crucial for pharmacologically relevant compounds, especially those with multiple rings and functional groups.\n\n4. **Minimize:**\n   - **Suggested value:** True\n   - **Reasoning:** Optimize the geometry of the generated molecules to ensure they are conformationally stable and energetically favorable. This is crucial for realistic molecule assessments.\n\n5. **Min_similarity:**\n   - **Suggested value:** 0.4 to 0.5\n   - **Reasoning:** A moderate minimum similarity helps maintain some structural features of the original while allowing sufficient diversity for generating novel structures, benefiting from structural complexity present.\n\n6. **Particles:**\n   - **Suggested value:** 50\n   - **Reasoning:** A balanced number of particles can provide a good balance between exploration and convergence but can adjust more based on computational resources and desired thoroughness in search.\n\n7. **Iterations:**\n   - **Suggested value:** 500 to 1000\n   - **Reasoning:** A higher number of iterations are generally necessary to ensure the GA has sufficient time to evolve toward an optimal solution, especially for complex molecules. This range allows deeper iterations to refine molecule optimization effectively.\n\nThese suggestions are designed to leverage MolMIM's capabilities while facilitating efficient molecular generation and optimization for a complex organic molecule. Adjustments might be necessary based on specific hardware capabilities or additional objectives you may want to consider.""}]","{'mime_type': 'application/json', 'value': '{""id"":""chatcmpl-ARSntKLWvkutQKCzex91rcEEZ8Veq"",""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""message"":{""content"":""To suggest appropriate parameters for running MolMIM on the given SMILES string, we need to consider the complexity and characteristics of the molecule represented, as well as the typical setups used in molecular generation and optimization tasks. Let\'s analyze the given molecule first.\\n\\n### Analysis:\\nThe SMILES string provided represents a complex organic molecule with several aromatic rings, nitrogen atoms, and functional groups like amine, ether, and a ketone component. This structure suggests that the molecule is relatively large and complex, which often implies specific challenges in optimization and molecular generation processes, such as computational demands and chemical feasibility.\\n\\n### Suggested Parameters:\\n\\n1. **Algorithm:**\\n   - **Suggested value:** Genetic Algorithm (GA)\\n   - **Reasoning:** Given the complexity of the molecule, a Genetic Algorithm is often suitable for navigating large search spaces effectively, especially for complex molecules with multiple rings and heteroatoms. GA can efficiently find diverse and optimal solutions through evolutionary strategies.\\n\\n2. **Num_molecules:**\\n   - **Suggested value:** 100 to 200\\n   - **Reasoning:** A moderate number of molecular candidates ensures a good balance between diversity and computational feasibility. This allows sufficient exploration of the molecular space while keeping calculations manageable.\\n\\n3. **Property_name:**\\n   - **Suggested value:** Drug-likeness or solubility\\n   - **Reasoning:** Based on the complexity of the molecule, improving its drug-likeness or optimizing for better solubility could be prioritized as these properties are crucial for pharmacologically relevant compounds, especially those with multiple rings and functional groups.\\n\\n4. **Minimize:**\\n   - **Suggested value:** True\\n   - **Reasoning:** Optimize the geometry of the generated molecules to ensure they are conformationally stable and energetically favorable. This is crucial for realistic molecule assessments.\\n\\n5. **Min_similarity:**\\n   - **Suggested value:** 0.4 to 0.5\\n   - **Reasoning:** A moderate minimum similarity helps maintain some structural features of the original while allowing sufficient diversity for generating novel structures, benefiting from structural complexity present.\\n\\n6. **Particles:**\\n   - **Suggested value:** 50\\n   - **Reasoning:** A balanced number of particles can provide a good balance between exploration and convergence but can adjust more based on computational resources and desired thoroughness in search.\\n\\n7. **Iterations:**\\n   - **Suggested value:** 500 to 1000\\n   - **Reasoning:** A higher number of iterations are generally necessary to ensure the GA has sufficient time to evolve toward an optimal solution, especially for complex molecules. This range allows deeper iterations to refine molecule optimization effectively.\\n\\nThese suggestions are designed to leverage MolMIM\'s capabilities while facilitating efficient molecular generation and optimization for a complex organic molecule. Adjustments might be necessary based on specific hardware capabilities or additional objectives you may want to consider."",""refusal"":null,""role"":""assistant""}}],""created"":1731108941,""model"":""gpt-4o-2024-08-06"",""object"":""chat.completion"",""system_fingerprint"":""fp_45cf54deae"",""usage"":{""completion_tokens"":577,""prompt_tokens"":158,""total_tokens"":735,""completion_tokens_details"":{""accepted_prediction_tokens"":0,""audio_tokens"":0,""reasoning_tokens"":0,""rejected_prediction_tokens"":0},""prompt_tokens_details"":{""audio_tokens"":0,""cached_tokens"":0}}}'}","{'invocation_parameters': '{""model"": ""gpt-4o""}', 'input_messages': [{'message': {'content': 'You are an expert in molecular generation and optimization using MolMIM.', 'role': 'system'}}, {'message': {'content': '\n        Analyze the following SMILES string and suggest appropriate parameters for running MolMIM:\n\n        CN1C=C(C2=CC=CC=C21)C3=NC(=NC=C3)NC4=C(C=C(C(=C4)NC(=O)C=C)N(C)CCN(C)C)OC\n\n        Provide the following parameters:\n        1. algorithm\n        2. num_molecules\n        3. property_name\n        4. minimize\n        5. min_similarity\n        6. particles\n        7. iterations\n\n        Explain your reasoning for each parameter.\n        ', 'role': 'user'}}], 'token_count': {'total': 735, 'prompt': 158, 'completion': 577}, 'system': 'openai', 'provider': 'openai', 'output_messages': [{'message': {'content': ""To suggest appropriate parameters for running MolMIM on the given SMILES string, we need to consider the complexity and characteristics of the molecule represented, as well as the typical setups used in molecular generation and optimization tasks. Let's analyze the given molecule first.\n\n### Analysis:\nThe SMILES string provided represents a complex organic molecule with several aromatic rings, nitrogen atoms, and functional groups like amine, ether, and a ketone component. This structure suggests that the molecule is relatively large and complex, which often implies specific challenges in optimization and molecular generation processes, such as computational demands and chemical feasibility.\n\n### Suggested Parameters:\n\n1. **Algorithm:**\n   - **Suggested value:** Genetic Algorithm (GA)\n   - **Reasoning:** Given the complexity of the molecule, a Genetic Algorithm is often suitable for navigating large search spaces effectively, especially for complex molecules with multiple rings and heteroatoms. GA can efficiently find diverse and optimal solutions through evolutionary strategies.\n\n2. **Num_molecules:**\n   - **Suggested value:** 100 to 200\n   - **Reasoning:** A moderate number of molecular candidates ensures a good balance between diversity and computational feasibility. This allows sufficient exploration of the molecular space while keeping calculations manageable.\n\n3. **Property_name:**\n   - **Suggested value:** Drug-likeness or solubility\n   - **Reasoning:** Based on the complexity of the molecule, improving its drug-likeness or optimizing for better solubility could be prioritized as these properties are crucial for pharmacologically relevant compounds, especially those with multiple rings and functional groups.\n\n4. **Minimize:**\n   - **Suggested value:** True\n   - **Reasoning:** Optimize the geometry of the generated molecules to ensure they are conformationally stable and energetically favorable. This is crucial for realistic molecule assessments.\n\n5. **Min_similarity:**\n   - **Suggested value:** 0.4 to 0.5\n   - **Reasoning:** A moderate minimum similarity helps maintain some structural features of the original while allowing sufficient diversity for generating novel structures, benefiting from structural complexity present.\n\n6. **Particles:**\n   - **Suggested value:** 50\n   - **Reasoning:** A balanced number of particles can provide a good balance between exploration and convergence but can adjust more based on computational resources and desired thoroughness in search.\n\n7. **Iterations:**\n   - **Suggested value:** 500 to 1000\n   - **Reasoning:** A higher number of iterations are generally necessary to ensure the GA has sufficient time to evolve toward an optimal solution, especially for complex molecules. This range allows deeper iterations to refine molecule optimization effectively.\n\nThese suggestions are designed to leverage MolMIM's capabilities while facilitating efficient molecular generation and optimization for a complex organic molecule. Adjustments might be necessary based on specific hardware capabilities or additional objectives you may want to consider."", 'role': 'assistant'}}], 'model_name': 'gpt-4o-2024-08-06'}",{'span': {'kind': 'LLM'}},"{'mime_type': 'application/json', 'value': '{""messages"": [{""role"": ""system"", ""content"": ""You are an expert in molecular generation and optimization using MolMIM.""}, {""role"": ""user"", ""content"": ""\\n        Analyze the following SMILES string and suggest appropriate parameters for running MolMIM:\\n\\n        CN1C=C(C2=CC=CC=C21)C3=NC(=NC=C3)NC4=C(C=C(C(=C4)NC(=O)C=C)N(C)CCN(C)C)OC\\n\\n        Provide the following parameters:\\n        1. algorithm\\n        2. num_molecules\\n        3. property_name\\n        4. minimize\\n        5. min_similarity\\n        6. particles\\n        7. iterations\\n\\n        Explain your reasoning for each parameter.\\n        ""}], ""model"": ""gpt-4o""}'}",{},,,,,
