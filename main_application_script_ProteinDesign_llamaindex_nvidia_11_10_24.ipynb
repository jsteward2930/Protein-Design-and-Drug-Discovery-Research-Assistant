{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-multipart==0.0.12\n",
        "!pip install openai requests pathlib\n",
        "!pip install pypdf2\n",
        "!pip install pinecone-client openai\n",
        "!pip install --upgrade llama-index\n",
        "!pip install llama-index-vector-stores-pinecone\n",
        "!pip install llama-index-readers-papers\n",
        "!pip install llama-index-readers-semanticscholar\n",
        "!pip install llama-index-llms-openai\n",
        "!pip install langchain langchain-openai\n",
        "!pip install PyPDF2\n",
        "!pip install pydantic\n",
        "!pip install llama-index-readers-pdb\n",
        "!pip install biopython\n",
        "!pip install gradio_molecule3d\n",
        "!pip install opentelemetry-api\n",
        "!pip install opentelemetry-sdk\n",
        "!pip install opentelemetry-instrumentation-openai\n",
        "!pip install phoenix-ai\n",
        "!pip install \"arize-phoenix>=4.29.0\" openinference-instrumentation-openai\n",
        "!pip install opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp-proto-http\n",
        "!pip install openinference-instrumentation-openai openinference-instrumentation-llama-index\n",
        "!pip install arize-phoenix-otel\n",
        "!pip uninstall tenacity -y\n",
        "!pip install tenacity==8.2.3\n",
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKiDl_uZwImo",
        "outputId": "90635e80-0e92-4fc4-f6b3-ea50873e524a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-multipart==0.0.12\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Downloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: python-multipart\n",
            "Successfully installed python-multipart-0.0.12\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.52.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Collecting pypdf2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf2\n",
            "Successfully installed pypdf2-3.0.1\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.52.2)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.8.30)\n",
            "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone-client\n",
            "Successfully installed pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7\n",
            "Collecting llama-index\n",
            "  Downloading llama_index-0.11.22-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-agent-openai<0.4.0,>=0.3.4 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting llama-index-cli<0.4.0,>=0.3.1 (from llama-index)\n",
            "  Downloading llama_index_cli-0.3.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.12.0,>=0.11.22 (from llama-index)\n",
            "  Downloading llama_index_core-0.11.22-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.4 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.3.0,>=0.2.10 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.2.16-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.2.3-py3-none-any.whl.metadata (729 bytes)\n",
            "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting llama-index-readers-file<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.2.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting nltk>3.8.1 (from llama-index)\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.52.2)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.22->llama-index) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (3.10.10)\n",
            "Collecting dataclasses-json (from llama-index-core<0.12.0,>=0.11.22->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (1.2.14)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.22->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (10.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.12.0,>=0.11.22->llama-index)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.12.0,>=0.11.22->llama-index)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.12.0,>=0.11.22->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (1.16.0)\n",
            "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.4-py3-none-any.whl.metadata (814 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index)\n",
            "  Downloading llama_parse-0.5.13-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.22->llama-index) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.22->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.22->llama-index) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.22->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.22->llama-index) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.22->llama-index) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.22->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.22->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.22->llama-index) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.22->llama-index) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.22->llama-index) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.22->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.22->llama-index) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (0.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.22->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.22->llama-index) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.22->llama-index) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.22->llama-index) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.22->llama-index) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.22->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.12.0,>=0.11.22->llama-index)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.22->llama-index) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.22->llama-index) (24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.22->llama-index) (0.2.0)\n",
            "Downloading llama_index-0.11.22-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.3.1-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_core-0.11.22-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl (10 kB)\n",
            "Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.2.16-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.2.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.2.2-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading llama_cloud-0.1.4-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.8/176.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.5.13-py3-none-any.whl (13 kB)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: striprtf, dirtyjson, tenacity, pypdf, nltk, mypy-extensions, marshmallow, typing-inspect, tiktoken, llama-cloud, dataclasses-json, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed dataclasses-json-0.6.7 dirtyjson-1.0.8 llama-cloud-0.1.4 llama-index-0.11.22 llama-index-agent-openai-0.3.4 llama-index-cli-0.3.1 llama-index-core-0.11.22 llama-index-embeddings-openai-0.2.5 llama-index-indices-managed-llama-cloud-0.4.0 llama-index-legacy-0.9.48.post4 llama-index-llms-openai-0.2.16 llama-index-multi-modal-llms-openai-0.2.3 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.2 llama-index-readers-llama-parse-0.3.0 llama-parse-0.5.13 marshmallow-3.23.1 mypy-extensions-1.0.0 nltk-3.9.1 pypdf-4.3.1 striprtf-0.0.26 tenacity-8.5.0 tiktoken-0.8.0 typing-inspect-0.9.0\n",
            "Collecting llama-index-vector-stores-pinecone\n",
            "  Downloading llama_index_vector_stores_pinecone-0.3.1-py3-none-any.whl.metadata (719 bytes)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.10 in /usr/local/lib/python3.10/dist-packages (from llama-index-vector-stores-pinecone) (0.11.22)\n",
            "Requirement already satisfied: pinecone-client<6.0.0,>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from llama-index-vector-stores-pinecone) (5.0.1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (3.10.10)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (10.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<6.0.0,>=3.2.2->llama-index-vector-stores-pinecone) (2024.8.30)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<6.0.0,>=3.2.2->llama-index-vector-stores-pinecone) (1.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<6.0.0,>=3.2.2->llama-index-vector-stores-pinecone) (0.0.7)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<6.0.0,>=3.2.2->llama-index-vector-stores-pinecone) (2.2.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (4.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (3.10)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (3.23.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (24.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.10->llama-index-vector-stores-pinecone) (1.2.2)\n",
            "Downloading llama_index_vector_stores_pinecone-0.3.1-py3-none-any.whl (7.5 kB)\n",
            "Installing collected packages: llama-index-vector-stores-pinecone\n",
            "Successfully installed llama-index-vector-stores-pinecone-0.3.1\n",
            "Collecting llama-index-readers-papers\n",
            "  Downloading llama_index_readers_papers-0.2.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting arxiv<3.0.0,>=2.1.0 (from llama-index-readers-papers)\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-papers) (0.11.22)\n",
            "Collecting feedparser~=6.0.10 (from arxiv<3.0.0,>=2.1.0->llama-index-readers-papers)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.10/dist-packages (from arxiv<3.0.0,>=2.1.0->llama-index-readers-papers) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (3.10.10)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (10.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (4.0.3)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv<3.0.0,>=2.1.0->llama-index-readers-papers)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv<3.0.0,>=2.1.0->llama-index-readers-papers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv<3.0.0,>=2.1.0->llama-index-readers-papers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv<3.0.0,>=2.1.0->llama-index-readers-papers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv<3.0.0,>=2.1.0->llama-index-readers-papers) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (3.23.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (24.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-papers) (1.2.2)\n",
            "Downloading llama_index_readers_papers-0.2.0-py3-none-any.whl (5.9 kB)\n",
            "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=b71bcf8cb7577994c98b2dbef6dae0ab1408bd937597800407c6ea8601c3ca0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv, llama-index-readers-papers\n",
            "Successfully installed arxiv-2.1.3 feedparser-6.0.11 llama-index-readers-papers-0.2.0 sgmllib3k-1.0.0\n",
            "Collecting llama-index-readers-semanticscholar\n",
            "  Downloading llama_index_readers_semanticscholar-0.2.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting arxiv==1.4.8 (from llama-index-readers-semanticscholar)\n",
            "  Downloading arxiv-1.4.8-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-semanticscholar) (0.11.22)\n",
            "Requirement already satisfied: pypdf2==3.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-semanticscholar) (3.0.1)\n",
            "Collecting semanticscholar==0.4.1 (from llama-index-readers-semanticscholar)\n",
            "  Downloading semanticscholar-0.4.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (from arxiv==1.4.8->llama-index-readers-semanticscholar) (6.0.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from semanticscholar==0.4.1->llama-index-readers-semanticscholar) (2.32.3)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from semanticscholar==0.4.1->llama-index-readers-semanticscholar) (8.5.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (3.10.10)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (10.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (2.9.2)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (4.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->semanticscholar==0.4.1->llama-index-readers-semanticscholar) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->semanticscholar==0.4.1->llama-index-readers-semanticscholar) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->semanticscholar==0.4.1->llama-index-readers-semanticscholar) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->semanticscholar==0.4.1->llama-index-readers-semanticscholar) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (3.23.1)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser->arxiv==1.4.8->llama-index-readers-semanticscholar) (1.0.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (24.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-semanticscholar) (1.2.2)\n",
            "Downloading llama_index_readers_semanticscholar-0.2.0-py3-none-any.whl (5.5 kB)\n",
            "Downloading arxiv-1.4.8-py3-none-any.whl (12 kB)\n",
            "Downloading semanticscholar-0.4.1-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: semanticscholar, arxiv, llama-index-readers-semanticscholar\n",
            "  Attempting uninstall: arxiv\n",
            "    Found existing installation: arxiv 2.1.3\n",
            "    Uninstalling arxiv-2.1.3:\n",
            "      Successfully uninstalled arxiv-2.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llama-index-readers-papers 0.2.0 requires arxiv<3.0.0,>=2.1.0, but you have arxiv 1.4.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed arxiv-1.4.8 llama-index-readers-semanticscholar-0.2.0 semanticscholar-0.4.1\n",
            "Requirement already satisfied: llama-index-llms-openai in /usr/local/lib/python3.10/dist-packages (0.2.16)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.7 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-openai) (0.11.22)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-openai) (1.52.2)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.10.10)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (10.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.16.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (0.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->llama-index-llms-openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->llama-index-llms-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.23.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (24.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.2.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.13)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.137)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Collecting langchain-core<0.4.0,>=0.3.12 (from langchain)\n",
            "  Downloading langchain_core-0.3.15-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting openai<2.0.0,>=1.54.0 (from langchain-openai)\n",
            "  Downloading openai-1.54.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.8.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (4.66.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (3.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Downloading langchain_openai-0.2.6-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.15-py3-none-any.whl (408 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.54.3-py3-none-any.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai, langchain-core, langchain-openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.52.2\n",
            "    Uninstalling openai-1.52.2:\n",
            "      Successfully uninstalled openai-1.52.2\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.13\n",
            "    Uninstalling langchain-core-0.3.13:\n",
            "      Successfully uninstalled langchain-core-0.3.13\n",
            "Successfully installed langchain-core-0.3.15 langchain-openai-0.2.6 openai-1.54.3\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.9.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.12.2)\n",
            "Collecting llama-index-readers-pdb\n",
            "  Downloading llama_index_readers_pdb-0.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-pdb) (0.11.22)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-pdb) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (3.10.10)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (10.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->llama-index-readers-pdb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->llama-index-readers-pdb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->llama-index-readers-pdb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->llama-index-readers-pdb) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (4.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (2.23.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (3.23.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (24.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-pdb) (1.2.2)\n",
            "Downloading llama_index_readers_pdb-0.2.0-py3-none-any.whl (3.4 kB)\n",
            "Installing collected packages: llama-index-readers-pdb\n",
            "Successfully installed llama-index-readers-pdb-0.2.0\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\n",
            "Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.84\n",
            "Collecting gradio_molecule3d\n",
            "  Downloading gradio_molecule3d-0.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting gradio<6.0,>=4.0 (from gradio_molecule3d)\n",
            "  Downloading gradio-5.5.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio<6.0,>=4.0->gradio_molecule3d)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<6.0,>=4.0->gradio_molecule3d) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio<6.0,>=4.0->gradio_molecule3d)\n",
            "  Downloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio<6.0,>=4.0->gradio_molecule3d)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.4.2 (from gradio<6.0,>=4.0->gradio_molecule3d)\n",
            "  Downloading gradio_client-1.4.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<6.0,>=4.0->gradio_molecule3d) (0.27.2)\n",
            "Collecting huggingface-hub>=0.25.1 (from gradio<6.0,>=4.0->gradio_molecule3d)\n",
            "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<6.0,>=4.0->gradio_molecule3d) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio<6.0,>=4.0->gradio_molecule3d)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio<6.0,>=4.0->gradio_molecule3d) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<6.0,>=4.0->gradio_molecule3d) (3.10.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio<6.0,>=4.0->gradio_molecule3d) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio<6.0,>=4.0->gradio_molecule3d) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<6.0,>=4.0->gradio_molecule3d) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<6.0,>=4.0->gradio_molecule3d) (2.9.2)\n",
            "Collecting pydub (from gradio<6.0,>=4.0->gradio_molecule3d)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: python-multipart==0.0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<6.0,>=4.0->gradio_molecule3d) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio<6.0,>=4.0->gradio_molecule3d) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio<6.0,>=4.0->gradio_molecule3d)\n",
            "  Downloading ruff-0.7.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<1.0,>=0.1.1 (from gradio<6.0,>=4.0->gradio_molecule3d)\n",
            "  Downloading safehttpx-0.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio<6.0,>=4.0->gradio_molecule3d)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio<6.0,>=4.0->gradio_molecule3d)\n",
            "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio<6.0,>=4.0->gradio_molecule3d)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<6.0,>=4.0->gradio_molecule3d) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<6.0,>=4.0->gradio_molecule3d) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio<6.0,>=4.0->gradio_molecule3d)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio<6.0,>=4.0->gradio_molecule3d) (2024.10.0)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.2->gradio<6.0,>=4.0->gradio_molecule3d)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<6.0,>=4.0->gradio_molecule3d) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<6.0,>=4.0->gradio_molecule3d) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<6.0,>=4.0->gradio_molecule3d) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<6.0,>=4.0->gradio_molecule3d) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<6.0,>=4.0->gradio_molecule3d) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio<6.0,>=4.0->gradio_molecule3d) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio<6.0,>=4.0->gradio_molecule3d) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio<6.0,>=4.0->gradio_molecule3d) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio<6.0,>=4.0->gradio_molecule3d) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio<6.0,>=4.0->gradio_molecule3d) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio<6.0,>=4.0->gradio_molecule3d) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio<6.0,>=4.0->gradio_molecule3d) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio<6.0,>=4.0->gradio_molecule3d) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio<6.0,>=4.0->gradio_molecule3d) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<6.0,>=4.0->gradio_molecule3d) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<6.0,>=4.0->gradio_molecule3d) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<6.0,>=4.0->gradio_molecule3d) (13.9.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio<6.0,>=4.0->gradio_molecule3d) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<6.0,>=4.0->gradio_molecule3d) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<6.0,>=4.0->gradio_molecule3d) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio<6.0,>=4.0->gradio_molecule3d) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio<6.0,>=4.0->gradio_molecule3d) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio<6.0,>=4.0->gradio_molecule3d) (0.1.2)\n",
            "Downloading gradio_molecule3d-0.0.6-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.5.0-py3-none-any.whl (56.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.4.2-py3-none-any.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.4-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading ruff-0.7.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.1-py3-none-any.whl (8.4 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, semantic-version, ruff, markupsafe, ffmpy, aiofiles, starlette, huggingface-hub, safehttpx, gradio-client, fastapi, gradio, gradio_molecule3d\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.24.7\n",
            "    Uninstalling huggingface-hub-0.24.7:\n",
            "      Successfully uninstalled huggingface-hub-0.24.7\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.4 ffmpy-0.4.0 gradio-5.5.0 gradio-client-1.4.2 gradio_molecule3d-0.0.6 huggingface-hub-0.26.2 markupsafe-2.1.5 pydub-0.25.1 ruff-0.7.3 safehttpx-0.1.1 semantic-version-2.10.0 starlette-0.41.2 tomlkit-0.12.0 uvicorn-0.32.0 websockets-12.0\n",
            "Requirement already satisfied: opentelemetry-api in /usr/local/lib/python3.10/dist-packages (1.16.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api) (1.2.14)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api) (75.1.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api) (1.16.0)\n",
            "Requirement already satisfied: opentelemetry-sdk in /usr/local/lib/python3.10/dist-packages (1.16.0)\n",
            "Requirement already satisfied: opentelemetry-api==1.16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk) (1.16.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.37b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk) (0.37b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk) (4.12.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api==1.16.0->opentelemetry-sdk) (1.2.14)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api==1.16.0->opentelemetry-sdk) (1.16.0)\n",
            "Collecting opentelemetry-instrumentation-openai\n",
            "  Downloading opentelemetry_instrumentation_openai-0.33.11-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-api<2.0.0,>=1.28.0 (from opentelemetry-instrumentation-openai)\n",
            "  Downloading opentelemetry_api-1.28.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-instrumentation<0.50,>=0.49b0 (from opentelemetry-instrumentation-openai)\n",
            "  Downloading opentelemetry_instrumentation-0.49b1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting opentelemetry-semantic-conventions<0.50,>=0.49b0 (from opentelemetry-instrumentation-openai)\n",
            "  Downloading opentelemetry_semantic_conventions-0.49b1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-semantic-conventions-ai==0.4.2 (from opentelemetry-instrumentation-openai)\n",
            "  Downloading opentelemetry_semantic_conventions_ai-0.4.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: tiktoken<1,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-openai) (0.8.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<2.0.0,>=1.28.0->opentelemetry-instrumentation-openai) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<2.0.0,>=1.28.0->opentelemetry-instrumentation-openai) (8.5.0)\n",
            "Requirement already satisfied: packaging>=18.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation<0.50,>=0.49b0->opentelemetry-instrumentation-openai) (24.1)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation<0.50,>=0.49b0->opentelemetry-instrumentation-openai) (1.16.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.6.0->opentelemetry-instrumentation-openai) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.6.0->opentelemetry-instrumentation-openai) (2.32.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api<2.0.0,>=1.28.0->opentelemetry-instrumentation-openai) (3.20.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.6.0->opentelemetry-instrumentation-openai) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.6.0->opentelemetry-instrumentation-openai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.6.0->opentelemetry-instrumentation-openai) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.6.0->opentelemetry-instrumentation-openai) (2024.8.30)\n",
            "Downloading opentelemetry_instrumentation_openai-0.33.11-py3-none-any.whl (22 kB)\n",
            "Downloading opentelemetry_semantic_conventions_ai-0.4.2-py3-none-any.whl (5.3 kB)\n",
            "Downloading opentelemetry_api-1.28.1-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation-0.49b1-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.49b1-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.2/159.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opentelemetry-semantic-conventions-ai, opentelemetry-api, opentelemetry-semantic-conventions, opentelemetry-instrumentation, opentelemetry-instrumentation-openai\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.16.0\n",
            "    Uninstalling opentelemetry-api-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.16.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-sdk 1.16.0 requires opentelemetry-api==1.16.0, but you have opentelemetry-api 1.28.1 which is incompatible.\n",
            "opentelemetry-sdk 1.16.0 requires opentelemetry-semantic-conventions==0.37b0, but you have opentelemetry-semantic-conventions 0.49b1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed opentelemetry-api-1.28.1 opentelemetry-instrumentation-0.49b1 opentelemetry-instrumentation-openai-0.33.11 opentelemetry-semantic-conventions-0.49b1 opentelemetry-semantic-conventions-ai-0.4.2\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement phoenix-ai (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for phoenix-ai\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting arize-phoenix>=4.29.0\n",
            "  Downloading arize_phoenix-5.7.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting openinference-instrumentation-openai\n",
            "  Downloading openinference_instrumentation_openai-0.1.17-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting aioitertools (from arize-phoenix>=4.29.0)\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting aiosqlite (from arize-phoenix>=4.29.0)\n",
            "  Downloading aiosqlite-0.20.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting alembic<2,>=1.3.0 (from arize-phoenix>=4.29.0)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting arize-phoenix-evals>=0.13.1 (from arize-phoenix>=4.29.0)\n",
            "  Downloading arize_phoenix_evals-0.17.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting arize-phoenix-otel>=0.5.1 (from arize-phoenix>=4.29.0)\n",
            "  Downloading arize_phoenix_otel-0.6.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting authlib (from arize-phoenix>=4.29.0)\n",
            "  Downloading Authlib-1.3.2-py2.py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (5.5.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (0.115.4)\n",
            "Collecting grpc-interceptor (from arize-phoenix>=4.29.0)\n",
            "  Downloading grpc_interceptor-0.15.4-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (1.64.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (0.27.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (3.1.4)\n",
            "Requirement already satisfied: numpy!=2.0.0 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (1.26.4)\n",
            "Collecting openinference-instrumentation>=0.1.12 (from arize-phoenix>=4.29.0)\n",
            "  Downloading openinference_instrumentation-0.1.18-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting openinference-semantic-conventions>=0.1.12 (from arize-phoenix>=4.29.0)\n",
            "  Downloading openinference_semantic_conventions-0.1.12-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting opentelemetry-exporter-otlp (from arize-phoenix>=4.29.0)\n",
            "  Downloading opentelemetry_exporter_otlp-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-proto>=1.12.0 (from arize-phoenix>=4.29.0)\n",
            "  Downloading opentelemetry_proto-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (1.16.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (0.49b1)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (2.2.2)\n",
            "Requirement already satisfied: protobuf<6.0,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (3.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (17.0.0)\n",
            "Requirement already satisfied: pydantic!=2.0.*,<3,>=1.0 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (2.9.2)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (0.0.12)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (1.13.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix>=4.29.0) (2.0.36)\n",
            "Collecting sqlean-py>=3.45.1 (from arize-phoenix>=4.29.0)\n",
            "  Downloading sqlean.py-3.47.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: starlette in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (0.41.2)\n",
            "Collecting strawberry-graphql==0.243.1 (from arize-phoenix>=4.29.0)\n",
            "  Downloading strawberry_graphql-0.243.1-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (4.12.2)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (0.32.0)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (12.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from arize-phoenix>=4.29.0) (1.16.0)\n",
            "Collecting graphql-core<3.4.0,>=3.2.0 (from strawberry-graphql==0.243.1->arize-phoenix>=4.29.0)\n",
            "  Downloading graphql_core-3.2.5-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.243.1->arize-phoenix>=4.29.0) (2.8.2)\n",
            "Requirement already satisfied: opentelemetry-api in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation-openai) (1.28.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation-openai) (0.49b1)\n",
            "Collecting Mako (from alembic<2,>=1.3.0->arize-phoenix>=4.29.0)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting protobuf<6.0,>=3.20.2 (from arize-phoenix>=4.29.0)\n",
            "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->arize-phoenix>=4.29.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->arize-phoenix>=4.29.0) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.0.*,<3,>=1.0->arize-phoenix>=4.29.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.0.*,<3,>=1.0->arize-phoenix>=4.29.0) (2.23.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=2.0.4->sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix>=4.29.0) (3.1.1)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from authlib->arize-phoenix>=4.29.0) (43.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette->arize-phoenix>=4.29.0) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->arize-phoenix>=4.29.0) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->arize-phoenix>=4.29.0) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->arize-phoenix>=4.29.0) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->arize-phoenix>=4.29.0) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->arize-phoenix>=4.29.0) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->arize-phoenix>=4.29.0) (2.1.5)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api->openinference-instrumentation-openai) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api->openinference-instrumentation-openai) (8.5.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.28.1 (from opentelemetry-exporter-otlp->arize-phoenix>=4.29.0)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.28.1 (from opentelemetry-exporter-otlp->arize-phoenix>=4.29.0)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.1->opentelemetry-exporter-otlp->arize-phoenix>=4.29.0) (1.65.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.28.1 (from opentelemetry-exporter-otlp-proto-grpc==1.28.1->opentelemetry-exporter-otlp->arize-phoenix>=4.29.0)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-sdk (from arize-phoenix>=4.29.0)\n",
            "  Downloading opentelemetry_sdk-1.28.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: requests~=2.7 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http==1.28.1->opentelemetry-exporter-otlp->arize-phoenix>=4.29.0) (2.32.3)\n",
            "Requirement already satisfied: packaging>=18.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation->openinference-instrumentation-openai) (24.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->arize-phoenix>=4.29.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->arize-phoenix>=4.29.0) (3.5.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->arize-phoenix>=4.29.0) (8.1.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->arize-phoenix>=4.29.0) (1.2.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api->openinference-instrumentation-openai) (3.20.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.7.0->strawberry-graphql==0.243.1->arize-phoenix>=4.29.0) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->authlib->arize-phoenix>=4.29.0) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->authlib->arize-phoenix>=4.29.0) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.1->opentelemetry-exporter-otlp->arize-phoenix>=4.29.0) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.1->opentelemetry-exporter-otlp->arize-phoenix>=4.29.0) (2.2.3)\n",
            "Downloading arize_phoenix-5.7.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading strawberry_graphql-0.243.1-py3-none-any.whl (306 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.0/306.0 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openinference_instrumentation_openai-0.1.17-py3-none-any.whl (23 kB)\n",
            "Downloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arize_phoenix_evals-0.17.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arize_phoenix_otel-0.6.1-py3-none-any.whl (10 kB)\n",
            "Downloading openinference_instrumentation-0.1.18-py3-none-any.whl (14 kB)\n",
            "Downloading openinference_semantic_conventions-0.1.12-py3-none-any.whl (9.1 kB)\n",
            "Downloading opentelemetry_proto-1.28.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlean.py-3.47.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
            "Downloading aiosqlite-0.20.0-py3-none-any.whl (15 kB)\n",
            "Downloading Authlib-1.3.2-py2.py3-none-any.whl (225 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpc_interceptor-0.15.4-py3-none-any.whl (20 kB)\n",
            "Downloading opentelemetry_exporter_otlp-1.28.1-py3-none-any.whl (7.0 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_http-1.28.1-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.28.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_sdk-1.28.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.5-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sqlean-py, protobuf, openinference-semantic-conventions, Mako, grpc-interceptor, graphql-core, aiosqlite, aioitertools, strawberry-graphql, opentelemetry-proto, alembic, opentelemetry-exporter-otlp-proto-common, authlib, arize-phoenix-evals, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, openinference-instrumentation, opentelemetry-exporter-otlp, openinference-instrumentation-openai, arize-phoenix-otel, arize-phoenix\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.16.0\n",
            "    Uninstalling opentelemetry-sdk-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.3 which is incompatible.\n",
            "google-cloud-firestore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.3 which is incompatible.\n",
            "tensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.28.3 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.28.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.6 aioitertools-0.12.0 aiosqlite-0.20.0 alembic-1.14.0 arize-phoenix-5.7.0 arize-phoenix-evals-0.17.3 arize-phoenix-otel-0.6.1 authlib-1.3.2 graphql-core-3.2.5 grpc-interceptor-0.15.4 openinference-instrumentation-0.1.18 openinference-instrumentation-openai-0.1.17 openinference-semantic-conventions-0.1.12 opentelemetry-exporter-otlp-1.28.1 opentelemetry-exporter-otlp-proto-common-1.28.1 opentelemetry-exporter-otlp-proto-grpc-1.28.1 opentelemetry-exporter-otlp-proto-http-1.28.1 opentelemetry-proto-1.28.1 opentelemetry-sdk-1.28.1 protobuf-5.28.3 sqlean-py-3.47.0 strawberry-graphql-0.243.1\n",
            "Requirement already satisfied: opentelemetry-api in /usr/local/lib/python3.10/dist-packages (1.28.1)\n",
            "Requirement already satisfied: opentelemetry-sdk in /usr/local/lib/python3.10/dist-packages (1.28.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http in /usr/local/lib/python3.10/dist-packages (1.28.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api) (8.5.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk) (0.49b1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk) (4.12.2)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http) (1.65.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http) (1.28.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.28.1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http) (1.28.1)\n",
            "Requirement already satisfied: requests~=2.7 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http) (2.32.3)\n",
            "Requirement already satisfied: protobuf<6.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-proto==1.28.1->opentelemetry-exporter-otlp-proto-http) (5.28.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api) (1.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api) (3.20.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http) (2024.8.30)\n",
            "Requirement already satisfied: openinference-instrumentation-openai in /usr/local/lib/python3.10/dist-packages (0.1.17)\n",
            "Collecting openinference-instrumentation-llama-index\n",
            "  Downloading openinference_instrumentation_llama_index-3.0.3-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: openinference-instrumentation>=0.1.18 in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation-openai) (0.1.18)\n",
            "Requirement already satisfied: openinference-semantic-conventions>=0.1.12 in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation-openai) (0.1.12)\n",
            "Requirement already satisfied: opentelemetry-api in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation-openai) (1.28.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation-openai) (0.49b1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation-openai) (0.49b1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation-openai) (4.12.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation-openai) (1.16.0)\n",
            "Requirement already satisfied: opentelemetry-sdk in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation>=0.1.18->openinference-instrumentation-openai) (1.28.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api->openinference-instrumentation-openai) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api->openinference-instrumentation-openai) (8.5.0)\n",
            "Requirement already satisfied: packaging>=18.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation->openinference-instrumentation-openai) (24.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api->openinference-instrumentation-openai) (3.20.2)\n",
            "Downloading openinference_instrumentation_llama_index-3.0.3-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: openinference-instrumentation-llama-index\n",
            "Successfully installed openinference-instrumentation-llama-index-3.0.3\n",
            "Requirement already satisfied: arize-phoenix-otel in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: openinference-semantic-conventions>=0.1.9 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix-otel) (0.1.12)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp in /usr/local/lib/python3.10/dist-packages (from arize-phoenix-otel) (1.28.1)\n",
            "Requirement already satisfied: opentelemetry-proto>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix-otel) (1.28.1)\n",
            "Requirement already satisfied: opentelemetry-sdk in /usr/local/lib/python3.10/dist-packages (from arize-phoenix-otel) (1.28.1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions in /usr/local/lib/python3.10/dist-packages (from arize-phoenix-otel) (0.49b1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix-otel) (4.12.2)\n",
            "Requirement already satisfied: protobuf<6.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-proto>=1.12.0->arize-phoenix-otel) (5.28.3)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.28.1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp->arize-phoenix-otel) (1.28.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.28.1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp->arize-phoenix-otel) (1.28.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.1->opentelemetry-exporter-otlp->arize-phoenix-otel) (1.2.14)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.1->opentelemetry-exporter-otlp->arize-phoenix-otel) (1.65.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.1->opentelemetry-exporter-otlp->arize-phoenix-otel) (1.64.1)\n",
            "Requirement already satisfied: opentelemetry-api~=1.15 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.1->opentelemetry-exporter-otlp->arize-phoenix-otel) (1.28.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.1->opentelemetry-exporter-otlp->arize-phoenix-otel) (1.28.1)\n",
            "Requirement already satisfied: requests~=2.7 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http==1.28.1->opentelemetry-exporter-otlp->arize-phoenix-otel) (2.32.3)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api~=1.15->opentelemetry-exporter-otlp-proto-grpc==1.28.1->opentelemetry-exporter-otlp->arize-phoenix-otel) (8.5.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-exporter-otlp-proto-grpc==1.28.1->opentelemetry-exporter-otlp->arize-phoenix-otel) (1.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api~=1.15->opentelemetry-exporter-otlp-proto-grpc==1.28.1->opentelemetry-exporter-otlp->arize-phoenix-otel) (3.20.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.1->opentelemetry-exporter-otlp->arize-phoenix-otel) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.1->opentelemetry-exporter-otlp->arize-phoenix-otel) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.1->opentelemetry-exporter-otlp->arize-phoenix-otel) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.1->opentelemetry-exporter-otlp->arize-phoenix-otel) (2024.8.30)\n",
            "Found existing installation: tenacity 8.5.0\n",
            "Uninstalling tenacity-8.5.0:\n",
            "  Successfully uninstalled tenacity-8.5.0\n",
            "Collecting tenacity==8.2.3\n",
            "  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
            "Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: tenacity\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llama-index-readers-papers 0.2.0 requires arxiv<3.0.0,>=2.1.0, but you have arxiv 1.4.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tenacity-8.2.3\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.py\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "import logging\n",
        "from typing import Dict, Optional\n",
        "\n",
        "class EnvironmentManager:\n",
        "    \"\"\"Manages environment variables and configuration for the protein design application.\"\"\"\n",
        "\n",
        "    required_vars = {\n",
        "        'PINECONE_API_KEY': 'Your Pinecone API key',\n",
        "        'OPENAI_API_KEY': 'Your OpenAI API key',\n",
        "        'PHOENIX_API_KEY': 'Your Phoenix API key',\n",
        "        'NVIDIA_API_KEY': 'Your NVIDIA API key'\n",
        "    }\n",
        "\n",
        "    def __init__(self, env_path: Optional[str] = None):\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.env_path = env_path or '.env'\n",
        "        self.env_vars: Dict[str, str] = {}\n",
        "\n",
        "    def setup_environment(self) -> bool:\n",
        "        try:\n",
        "            if not os.path.exists(self.env_path):\n",
        "                self._create_env_file()\n",
        "                self.logger.info(f\"Created new .env file at {self.env_path}\")\n",
        "                return False\n",
        "\n",
        "            load_dotenv(self.env_path)\n",
        "\n",
        "            missing_vars = []\n",
        "            for var_name, description in self.required_vars.items():\n",
        "                value = os.getenv(var_name)\n",
        "                if not value:\n",
        "                    missing_vars.append(var_name)\n",
        "                else:\n",
        "                    self.env_vars[var_name] = value\n",
        "\n",
        "            if missing_vars:\n",
        "                self.logger.warning(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n",
        "                return False\n",
        "\n",
        "            self.logger.info(\"Environment setup completed successfully\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error setting up environment: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def _create_env_file(self) -> None:\n",
        "        template = \"\\n\".join([f\"{var}='{desc}'\" for var, desc in self.required_vars.items()])\n",
        "\n",
        "        with open(self.env_path, 'w') as f:\n",
        "            f.write(template)\n",
        "\n",
        "    def get_var(self, var_name: str) -> Optional[str]:\n",
        "        return self.env_vars.get(var_name)\n",
        "\n",
        "    @property\n",
        "    def is_configured(self) -> bool:\n",
        "        return all(os.getenv(var) for var in self.required_vars)\n",
        "\n",
        "    def update_var(self, var_name: str, value: str) -> bool:\n",
        "        try:\n",
        "            if var_name not in self.required_vars:\n",
        "                self.logger.error(f\"Invalid environment variable: {var_name}\")\n",
        "                return False\n",
        "\n",
        "            if os.path.exists(self.env_path):\n",
        "                with open(self.env_path, 'r') as f:\n",
        "                    lines = f.readlines()\n",
        "            else:\n",
        "                lines = []\n",
        "\n",
        "            var_found = False\n",
        "            for i, line in enumerate(lines):\n",
        "                if line.startswith(f\"{var_name}=\"):\n",
        "                    lines[i] = f\"{var_name}='{value}'\\n\"\n",
        "                    var_found = True\n",
        "                    break\n",
        "\n",
        "            if not var_found:\n",
        "                lines.append(f\"{var_name}='{value}'\\n\")\n",
        "\n",
        "            with open(self.env_path, 'w') as f:\n",
        "                f.writelines(lines)\n",
        "\n",
        "            os.environ[var_name] = value\n",
        "            self.env_vars[var_name] = value\n",
        "\n",
        "            self.logger.info(f\"Successfully updated {var_name}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error updating environment variable: {str(e)}\")\n",
        "            return False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GivezXAEBasG",
        "outputId": "7f1aab57-9065-4b5f-8f83-eb1c91d891e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from config import EnvironmentManager\n",
        "\n",
        "# Initialize environment manager\n",
        "env_manager = EnvironmentManager()\n",
        "\n",
        "# Set up environment (this will create .env file if it doesn't exist)\n",
        "if not env_manager.setup_environment():\n",
        "    print(\"Please update your .env file with your API keys\")\n",
        "else:\n",
        "    print(\"Environment configured successfully!\")\n",
        "\n",
        "# Update with your actual API keys\n",
        "env_manager.update_var('PINECONE_API_KEY', 'your_pinecone_key_here')\n",
        "env_manager.update_var('OPENAI_API_KEY', 'your_openai_key_here')\n",
        "env_manager.update_var('PHOENIX_API_KEY', 'your_phoenix_key_here')\n",
        "env_manager.update_var('NVIDIA_API_KEY', 'your_nvidia_key_here')"
      ],
      "metadata": {
        "id": "BtKaxGJiBzvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "import logging\n",
        "import traceback\n",
        "import io\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "from typing import List, Optional, Tuple, Union, Dict, Any\n",
        "from pydantic import BaseModel, Field\n",
        "import pinecone\n",
        "from llama_index.core import VectorStoreIndex, StorageContext, Document, Settings\n",
        "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "import requests\n",
        "import json\n",
        "from pathlib import Path\n",
        "from llama_index.readers.papers import ArxivReader, PubmedReader\n",
        "from llama_index.readers.pdb import PdbAbstractReader\n",
        "from Bio import PDB\n",
        "from gradio_molecule3d import Molecule3D\n",
        "from Bio.PDB.DSSP import dssp_dict_from_pdb_file\n",
        "import tempfile\n",
        "from llama_index.llms.openai import OpenAI as LlamaOpenAI\n",
        "import base64\n",
        "import tempfile\n",
        "from config import EnvironmentManager\n",
        "from pathlib import Path\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from contextlib import nullcontext\n",
        "from opentelemetry import trace\n",
        "from opentelemetry.sdk.trace import TracerProvider\n",
        "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
        "from opentelemetry.sdk.resources import Resource\n",
        "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
        "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
        "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
        "\n",
        "# Initialize environment\n",
        "env_manager = EnvironmentManager()\n",
        "if not env_manager.setup_environment():\n",
        "    raise RuntimeError(\"Environment not properly configured\")\n",
        "\n",
        "# Use the environment variables\n",
        "PINECONE_API_KEY = env_manager.get_var('PINECONE_API_KEY')\n",
        "OPENAI_API_KEY = env_manager.get_var('OPENAI_API_KEY')\n",
        "PHOENIX_API_KEY = env_manager.get_var('PHOENIX_API_KEY')\n",
        "NVIDIA_API_KEY = env_manager.get_var('NVIDIA_API_KEY')\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class AlphaFold2Output(BaseModel):\n",
        "    analysis: str = Field(description=\"Analysis of AlphaFold2 parameters\")\n",
        "    code: str = Field(description=\"Generated Python code for running AlphaFold2\")\n",
        "\n",
        "# Add the GROMACS-specific Protocol class with your existing classes\n",
        "class Protocol(BaseModel):\n",
        "    steps: List[str] = Field(description=\"Steps for the GROMACS simulation protocol using NVIDIA tools\")\n",
        "\n",
        "# 2. Proper OpenTelemetry initialization and error handling\n",
        "class TracingSetup:\n",
        "    def __init__(\n",
        "        self,\n",
        "        project_name: str = \"protein-design-assistant\",\n",
        "        phoenix_api_key: Optional[str] = None,\n",
        "    ):\n",
        "        self.project_name = project_name\n",
        "        self.phoenix_api_key = phoenix_api_key or os.getenv(\"PHOENIX_API_KEY\")\n",
        "        self.tracer_provider = None\n",
        "        self.tracer = None\n",
        "\n",
        "    def initialize_tracing(self) -> None:\n",
        "        \"\"\"Initialize OpenTelemetry tracing with Phoenix integration.\"\"\"\n",
        "        try:\n",
        "            if not self.phoenix_api_key:\n",
        "                logger.warning(\"Phoenix API key not found. Tracing will be disabled.\")\n",
        "                return None\n",
        "\n",
        "            # Set the tracer provider as the global default\n",
        "            resource = Resource.create({\n",
        "                \"service.name\": self.project_name,\n",
        "                \"deployment.environment\": \"colab\"\n",
        "            })\n",
        "\n",
        "            self.tracer_provider = TracerProvider(resource=resource)\n",
        "            trace.set_tracer_provider(self.tracer_provider)\n",
        "\n",
        "            # Configure Phoenix exporter\n",
        "            exporter = OTLPSpanExporter(\n",
        "                endpoint=\"https://app.phoenix.arize.com/v1/traces\",\n",
        "                headers={\"api_key\": self.phoenix_api_key}\n",
        "            )\n",
        "\n",
        "            # Add BatchSpanProcessor with the Phoenix exporter\n",
        "            processor = BatchSpanProcessor(exporter)\n",
        "            self.tracer_provider.add_span_processor(processor)\n",
        "\n",
        "            # Initialize instrumentors only if tracer provider is set\n",
        "            if self.tracer_provider:\n",
        "                OpenAIInstrumentor().instrument(tracer_provider=self.tracer_provider)\n",
        "                LlamaIndexInstrumentor().instrument(tracer_provider=self.tracer_provider)\n",
        "\n",
        "            # Create tracer\n",
        "            self.tracer = trace.get_tracer(self.project_name)\n",
        "            logger.info(f\"OpenTelemetry tracing initialized for project: {self.project_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize tracing: {str(e)}\")\n",
        "            self.tracer_provider = None\n",
        "            self.tracer = None\n",
        "\n",
        "    def start_span(self, name: str):\n",
        "        \"\"\"Helper method to start a new span with error handling\"\"\"\n",
        "        try:\n",
        "            if self.tracer is None:\n",
        "                return nullcontext()\n",
        "            return self.tracer.start_span(name)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error starting span: {str(e)}\")\n",
        "            return nullcontext()\n",
        "\n",
        "    def start_as_current_span(self, name: str):\n",
        "        \"\"\"Helper method to start a new span as current with error handling\"\"\"\n",
        "        try:\n",
        "            if self.tracer is None:\n",
        "                return nullcontext()\n",
        "            return self.tracer.start_as_current_span(name)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error starting current span: {str(e)}\")\n",
        "            return nullcontext()\n",
        "\n",
        "class AudioResponseHandler:\n",
        "    def __init__(self, openai_client: OpenAI):\n",
        "        self.client = openai_client\n",
        "        self.voices = [\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"]\n",
        "        self.current_voice = \"alloy\"\n",
        "\n",
        "    def text_to_speech(self, text: str) -> str:\n",
        "        \"\"\"Convert text to speech and return the path to the audio file\"\"\"\n",
        "        if not text.strip():\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Create a temporary file with .mp3 extension\n",
        "            with tempfile.NamedTemporaryFile(suffix=\".mp3\", delete=False) as temp_file:\n",
        "                response = self.client.audio.speech.create(\n",
        "                    model=\"tts-1\",\n",
        "                    voice=self.current_voice,\n",
        "                    input=text\n",
        "                )\n",
        "                response.stream_to_file(temp_file.name)\n",
        "                return temp_file.name\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in text to speech conversion: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def set_voice(self, voice: str):\n",
        "        \"\"\"Set the TTS voice\"\"\"\n",
        "        if voice in self.voices:\n",
        "            self.current_voice = voice\n",
        "\n",
        "# AlphaFold2 specific state management\n",
        "class AlphaFold2State:\n",
        "    def __init__(self):\n",
        "        self.parameters = None\n",
        "        self.code = None\n",
        "\n",
        "af2_state = AlphaFold2State()\n",
        "\n",
        "class PineconeManager:\n",
        "    def __init__(self, api_key: str = None, index_name: str = \"llama-index-nvidia\"):\n",
        "        self.api_key = api_key or os.getenv(\"PINECONE_API_KEY\")\n",
        "        self.index_name = index_name\n",
        "        self.pc = None\n",
        "        self.vector_store_index = None\n",
        "        self.query_engine = None\n",
        "\n",
        "    def initialize(self) -> None:\n",
        "        \"\"\"Initialize Pinecone connection and index\"\"\"\n",
        "        try:\n",
        "            # Initialize Pinecone\n",
        "            self.pc = Pinecone(api_key=self.api_key)\n",
        "\n",
        "            # Check if index exists, create if it doesn't\n",
        "            if self.index_name not in self.pc.list_indexes().names():\n",
        "                self.pc.create_index(\n",
        "                    name=self.index_name,\n",
        "                    dimension=1536,  # OpenAI embedding dimension\n",
        "                    metric=\"cosine\",\n",
        "                    spec={'pod_type': 'p1'}\n",
        "                )\n",
        "                logger.info(f\"Created new Pinecone index: {self.index_name}\")\n",
        "\n",
        "            # Get the index\n",
        "            pinecone_index = self.pc.Index(self.index_name)\n",
        "\n",
        "            # Set up vector store\n",
        "            vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
        "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "            # Configure Llama-index settings\n",
        "            Settings.chunk_size = 1024\n",
        "            Settings.chunk_overlap = 20\n",
        "\n",
        "            # Create vector store index\n",
        "            self.vector_store_index = VectorStoreIndex.from_vector_store(\n",
        "                vector_store=vector_store,\n",
        "                storage_context=storage_context\n",
        "            )\n",
        "\n",
        "            # Initialize query engine\n",
        "            self.query_engine = self.vector_store_index.as_query_engine()\n",
        "\n",
        "            logger.info(\"Pinecone initialization completed successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error initializing Pinecone: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "    def query(self, query_text: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Query the Pinecone index with retry logic\n",
        "\n",
        "        Args:\n",
        "            query_text: The text to query\n",
        "\n",
        "        Returns:\n",
        "            Query response or None if failed\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if not self.query_engine:\n",
        "                raise ValueError(\"Query engine not initialized. Call initialize() first.\")\n",
        "\n",
        "            response = self.query_engine.query(query_text)\n",
        "            return str(response.response)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error querying Pinecone: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def add_documents(self, documents: Union[Document, List[Document]]) -> None:\n",
        "        \"\"\"\n",
        "        Add documents to the Pinecone index\n",
        "\n",
        "        Args:\n",
        "            documents: Single document or list of documents to add\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if not self.vector_store_index:\n",
        "                raise ValueError(\"Vector store index not initialized. Call initialize() first.\")\n",
        "\n",
        "            if isinstance(documents, Document):\n",
        "                documents = [documents]\n",
        "\n",
        "            self.vector_store_index.insert_nodes(documents)\n",
        "            logger.info(f\"Successfully added {len(documents)} documents to Pinecone index\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error adding documents to Pinecone: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "# Add a monitor to track the chat system's health\n",
        "class ChatSystemMonitor:\n",
        "    def __init__(self):\n",
        "        self.total_calls = 0\n",
        "        self.successful_calls = 0\n",
        "        self.failed_calls = 0\n",
        "        self.last_error = None\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def record_success(self):\n",
        "        self.total_calls += 1\n",
        "        self.successful_calls += 1\n",
        "\n",
        "    def record_failure(self, error):\n",
        "        self.total_calls += 1\n",
        "        self.failed_calls += 1\n",
        "        self.last_error = error\n",
        "\n",
        "    def get_status(self):\n",
        "        uptime = time.time() - self.start_time\n",
        "        success_rate = (self.successful_calls / self.total_calls * 100) if self.total_calls > 0 else 0\n",
        "        return {\n",
        "            \"uptime\": uptime,\n",
        "            \"total_calls\": self.total_calls,\n",
        "            \"success_rate\": success_rate,\n",
        "            \"last_error\": str(self.last_error) if self.last_error else None\n",
        "        }\n",
        "\n",
        "# Initialize the monitor\n",
        "chat_monitor = ChatSystemMonitor()\n",
        "\n",
        "# Initialize tracing setup\n",
        "tracing = TracingSetup(phoenix_api_key=os.getenv(\"PHOENIX_API_KEY\"))\n",
        "\n",
        "# Initialize OpenAI client and Pinecone\n",
        "openai_client = OpenAI()\n",
        "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
        "\n",
        "# Initialize DeepSeek client\n",
        "deepseek_client = OpenAI(\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "    api_key=os.getenv(\"NVIDIA_API_KEY\")\n",
        ")\n",
        "\n",
        "# Add this after your other client initializations\n",
        "llm = LlamaOpenAI(\n",
        "    model=\"gpt-4o\",  # or whatever model you prefer\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "def setup_pinecone_index():\n",
        "    \"\"\"\n",
        "    Initialize and configure Pinecone index with proper error handling\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.info(\"Starting Pinecone index setup\")\n",
        "\n",
        "        # Validate environment variables\n",
        "        pinecone_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "        if not pinecone_key:\n",
        "            raise ValueError(\"PINECONE_API_KEY environment variable is not set\")\n",
        "\n",
        "        # Initialize Pinecone client with retry logic\n",
        "        @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "        def init_pinecone():\n",
        "            return Pinecone(api_key=pinecone_key)\n",
        "\n",
        "        pc = init_pinecone()\n",
        "        index_name = \"llama-index-nvidia\"\n",
        "\n",
        "        # Check if index exists, create if it doesn't\n",
        "        existing_indexes = pc.list_indexes().names()\n",
        "        if index_name not in existing_indexes:\n",
        "            logger.info(f\"Creating new Pinecone index: {index_name}\")\n",
        "            pc.create_index(\n",
        "                name=index_name,\n",
        "                dimension=1536,  # OpenAI embedding dimension\n",
        "                metric=\"cosine\",\n",
        "                spec={'pod_type': 'p1'}\n",
        "            )\n",
        "\n",
        "        # Get the index\n",
        "        pinecone_index = pc.Index(index_name)\n",
        "\n",
        "        # Set up vector store\n",
        "        vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
        "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "        # Configure Llama-index settings\n",
        "        Settings.chunk_size = 1024\n",
        "        Settings.chunk_overlap = 20\n",
        "\n",
        "        # Create and return vector store index\n",
        "        vector_store_index = VectorStoreIndex.from_vector_store(\n",
        "            vector_store=vector_store,\n",
        "            storage_context=storage_context\n",
        "        )\n",
        "\n",
        "        logger.info(\"Successfully initialized Pinecone index\")\n",
        "        return vector_store_index\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to set up Pinecone index: {str(e)}\")\n",
        "        logger.error(f\"Full traceback: {traceback.format_exc()}\")\n",
        "        raise RuntimeError(f\"Pinecone setup failed: {str(e)}\")\n",
        "\n",
        "# Usage example:\n",
        "def setup_pinecone_and_query_engine():\n",
        "    try:\n",
        "        pinecone_manager = PineconeManager()\n",
        "        pinecone_manager.initialize()\n",
        "        return pinecone_manager.query_engine\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error setting up Pinecone: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Update your existing query engine initialization\n",
        "query_engine = setup_pinecone_and_query_engine()\n",
        "\n",
        "def chat_with_gpt4o(message, chat_history, history_state):\n",
        "    \"\"\"\n",
        "    Enhanced NIMs chat assistant with proper flow control and error handling\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if history_state is None:\n",
        "            history_state = []\n",
        "\n",
        "        logger.info(\"Starting chat interaction\")\n",
        "\n",
        "        # Phase 1: Query Preparation\n",
        "        try:\n",
        "            logger.info(\"Preparing query and context\")\n",
        "            # Create a synchronous query to prevent stalling\n",
        "            query_result = query_engine.query(\n",
        "                message,\n",
        "                response_mode=\"compact\",  # Use compact mode for faster processing\n",
        "                timeout=30  # Add timeout to prevent hanging\n",
        "            )\n",
        "            relevant_context = str(query_result.response)\n",
        "            logger.info(\"Context retrieved successfully\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in query phase: {str(e)}\")\n",
        "            relevant_context = \"Unable to retrieve context. Proceeding with basic response.\"\n",
        "\n",
        "        # Phase 2: Chat Completion\n",
        "        try:\n",
        "            logger.info(\"Initiating chat completion\")\n",
        "            system_message = {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"\"\"You are an AI assistant specializing in protein design and NVIDIA technologies.\n",
        "                Use this relevant information to inform your response: {relevant_context}\"\"\"\n",
        "            }\n",
        "\n",
        "            messages = [system_message] + history_state + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "            response = openai_client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=messages,\n",
        "                max_tokens=16384,\n",
        "                temperature=0,\n",
        "                stream=False,\n",
        "                timeout=60  # Add timeout for chat completion\n",
        "            )\n",
        "\n",
        "            response_text = response.choices[0].message.content\n",
        "            logger.info(\"Chat completion successful\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in chat completion: {str(e)}\")\n",
        "            return message, chat_history, history_state, None\n",
        "\n",
        "        # Phase 3: Audio Generation\n",
        "        try:\n",
        "            logger.info(\"Starting audio generation\")\n",
        "            audio_handler = AudioResponseHandler(openai_client)\n",
        "            audio_file_path = audio_handler.text_to_speech(response_text)\n",
        "\n",
        "            if not audio_file_path:\n",
        "                logger.warning(\"Audio generation returned no file path\")\n",
        "                raise ValueError(\"Audio generation failed\")\n",
        "\n",
        "            logger.info(f\"Audio generated successfully: {audio_file_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in audio generation: {str(e)}\")\n",
        "            audio_file_path = None\n",
        "            response_text += \"\\n\\n(Note: Audio response generation failed. Text response is still available.)\"\n",
        "\n",
        "        # Update conversation state\n",
        "        chat_history = chat_history + [(message, response_text)]\n",
        "        history_state.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "\n",
        "        logger.info(\"Chat interaction completed successfully\")\n",
        "        return message, chat_history, history_state, audio_file_path\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Critical error in chat system: {str(e)}\")\n",
        "        error_message = \"I apologize, but I encountered an unexpected error. Please try again.\"\n",
        "        chat_history = chat_history + [(message, error_message)]\n",
        "        return message, chat_history, history_state, None\n",
        "\n",
        "\n",
        "def fetch_papers(query: str, source: str, max_results: int = 10) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Fetch papers from specified source with improved error handling.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with tracing.start_as_current_span(\"fetch_papers\") as span:\n",
        "            span.set_attribute(\"query\", query)\n",
        "            span.set_attribute(\"source\", source)\n",
        "            span.set_attribute(\"max_results\", max_results)\n",
        "\n",
        "            documents = []\n",
        "            if source == \"arxiv\":\n",
        "                reader = ArxivReader()\n",
        "                documents = reader.load_data(search_query=query, max_results=max_results)\n",
        "            elif source == \"pubmed\":\n",
        "                reader = PubmedReader()\n",
        "                documents = reader.load_data(search_query=query, max_results=max_results)\n",
        "\n",
        "            span.set_attribute(\"documents_found\", len(documents))\n",
        "            return documents\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching papers: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def search_and_summarize_papers(query: str, source: str) -> tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Search and summarize scientific papers with proper error handling, token limits,\n",
        "    tracing spans, system messages for the chatbot, and audio output generation.\n",
        "    Returns both text summary and audio file path.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with tracing.start_as_current_span(\"search_and_summarize_papers\") as span:\n",
        "            span.set_attribute(\"query\", query)\n",
        "            span.set_attribute(\"source\", source)\n",
        "\n",
        "            # Input validation\n",
        "            if not query.strip():\n",
        "                return \"Please enter a search query.\", None\n",
        "\n",
        "            if source not in [\"arxiv\", \"pubmed\"]:\n",
        "                return \"Invalid source. Please select either 'arxiv' or 'pubmed'.\", None\n",
        "\n",
        "            # Fetch papers with tracing\n",
        "            with tracing.start_as_current_span(\"fetch_papers\") as fetch_span:\n",
        "                papers = fetch_papers(query, source, max_results=1)\n",
        "                if not papers:\n",
        "                    return \"No papers found matching the query.\", None\n",
        "                fetch_span.set_attribute(\"papers_found\", len(papers))\n",
        "\n",
        "            # Create a concise summary prompt with system message\n",
        "            paper = papers[0]\n",
        "            title = paper.metadata.get('title', 'N/A')[:200]\n",
        "            abstract = paper.text[:500] + \"...\" if len(paper.text) > 500 else paper.text\n",
        "\n",
        "            system_message = {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"\"\"You are an AI assistant specializing in scientific literature analysis\n",
        "                and NVIDIA technologies. Analyze the given paper and explain:\n",
        "                1. The main findings and their significance\n",
        "                2. How the research relates to NVIDIA technologies and GPU computing\n",
        "                3. Potential applications and impact on computational biology\n",
        "                4. Technical implementation considerations\n",
        "                Be concise but thorough in your analysis.\"\"\"\n",
        "            }\n",
        "\n",
        "            summary_prompt = f\"\"\"Analyze this scientific paper in the context of NVIDIA technologies:\n",
        "\n",
        "            Title: {title}\n",
        "            Abstract: {abstract}\n",
        "\n",
        "            Focus on:\n",
        "            1. Core findings and methodology\n",
        "            2. Relevance to GPU computing and NVIDIA platforms\n",
        "            3. Potential applications in computational biology\n",
        "            4. Implementation considerations\n",
        "            \"\"\"\n",
        "\n",
        "            # Generate summary with tracing and error handling\n",
        "            with tracing.start_as_current_span(\"query_engine_summary\") as query_span:\n",
        "                try:\n",
        "                    response = openai_client.chat.completions.create(\n",
        "                        model=\"gpt-4o\",\n",
        "                        messages=[\n",
        "                            system_message,\n",
        "                            {\"role\": \"user\", \"content\": summary_prompt}\n",
        "                        ],\n",
        "                        max_tokens=1024,\n",
        "                        temperature=0.7\n",
        "                    )\n",
        "                    summary = response.choices[0].message.content\n",
        "                    query_span.set_attribute(\"summary_length\", len(summary))\n",
        "\n",
        "                    # Generate audio for the summary\n",
        "                    with tracing.start_as_current_span(\"generate_audio_summary\") as audio_span:\n",
        "                        try:\n",
        "                            audio_handler = AudioResponseHandler(openai_client)\n",
        "                            audio_file_path = audio_handler.text_to_speech(summary)\n",
        "                            audio_span.set_attribute(\"audio_generated\", bool(audio_file_path))\n",
        "                            logger.info(\"Generated audio summary successfully\")\n",
        "                        except Exception as audio_error:\n",
        "                            logger.error(f\"Error generating audio summary: {str(audio_error)}\")\n",
        "                            audio_file_path = None\n",
        "\n",
        "                    return summary, audio_file_path\n",
        "\n",
        "                except Exception as query_error:\n",
        "                    error_msg = f\"Error generating summary: {str(query_error)}\"\n",
        "                    query_span.set_attribute(\"error\", error_msg)\n",
        "                    logger.error(error_msg)\n",
        "                    raise\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error in paper search: {str(e)}\"\n",
        "        logger.error(error_msg)\n",
        "        if span:  # Check if span exists in case error occurred before span creation\n",
        "            span.set_attribute(\"error\", error_msg)\n",
        "        return error_msg, None\n",
        "\n",
        "# Function to fetch PDB abstract\n",
        "def fetch_pdb_abstract(pdb_id):\n",
        "    try:\n",
        "        loader = PdbAbstractReader()\n",
        "        documents = loader.load_data([pdb_id])\n",
        "        return documents[0].text if documents else \"No abstract found for the given PDB ID.\"\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching PDB abstract: {e}\")\n",
        "        return f\"An error occurred while fetching the PDB abstract: {str(e)}\"\n",
        "\n",
        "# Function to download PDB file\n",
        "def download_pdb(pdb_id):\n",
        "    try:\n",
        "        pdb_list = PDB.PDBList()\n",
        "        file_path = pdb_list.retrieve_pdb_file(\n",
        "            pdb_id, pdir=\"./\", file_format=\"pdb\"\n",
        "        )\n",
        "\n",
        "        # Rename the file to ensure .pdb extension\n",
        "        new_file_path = f\"{pdb_id}.pdb\"\n",
        "        os.rename(file_path, new_file_path)\n",
        "\n",
        "        return f\"PDB file for {pdb_id} downloaded successfully to {new_file_path}\"\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error downloading PDB file: {str(e)}\")\n",
        "        return f\"Error downloading PDB file: {str(e)}\"\n",
        "\n",
        "def process_pdb_file(file):\n",
        "    try:\n",
        "        if hasattr(file, 'read'):\n",
        "            content = file.read()\n",
        "            if isinstance(content, bytes):\n",
        "                content = content.decode('utf-8')\n",
        "        elif hasattr(file, 'name'):\n",
        "            with open(file.name, 'r') as f:\n",
        "                content = f.read()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file object type\")\n",
        "\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing PDB file: {str(e)}\")\n",
        "        raise gr.Error(f\"Error processing PDB file: {str(e)}\")\n",
        "\n",
        "def get_rfdiffusion_parameters(pdb_content):\n",
        "    \"\"\"\n",
        "    Generate simplified, reliable parameters for RFdiffusion based on PDB content,\n",
        "    now with voice explanation.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a simplified analysis query that focuses on core requirements\n",
        "        query = f\"\"\"\n",
        "        Analyze this PDB content and suggest basic parameters for RFdiffusion that will work reliably.\n",
        "        Follow this exact format in your response:\n",
        "\n",
        "        PARAMETERS:\n",
        "        1. contigs: [value]\n",
        "        2. hotspot_res: [value]\n",
        "        3. diffusion_steps: [value]\n",
        "\n",
        "        EXPLANATION:\n",
        "        Provide a clear, conversational explanation of why these parameters were chosen and how they will affect the protein design process.\n",
        "        Keep the explanation natural and suitable for voice output.\n",
        "\n",
        "        Base the analysis on this PDB structure:\n",
        "        {pdb_content[:500]}...\n",
        "        \"\"\"\n",
        "\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"\"\"You are an expert in RFdiffusion parameter optimization.\n",
        "                Always ensure contigs parameter has both target specification AND binder length range after /0.\n",
        "                Example: 'A20-60/0 50-100' NOT just 'A20-60/0'\"\"\"},\n",
        "                {\"role\": \"user\", \"content\": query}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Split response into parameters and explanation\n",
        "        response_text = response.choices[0].message.content\n",
        "        parts = response_text.split(\"EXPLANATION:\", 1)\n",
        "        parameters = parts[0].strip()\n",
        "        explanation = parts[1].strip() if len(parts) > 1 else \"No explanation provided.\"\n",
        "\n",
        "        # Generate audio for the explanation\n",
        "        audio_handler = AudioResponseHandler(openai_client)\n",
        "        audio_file_path = audio_handler.text_to_speech(explanation)\n",
        "\n",
        "        return parameters, explanation, audio_file_path\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in GPT-4o analysis: {str(e)}\")\n",
        "        raise gr.Error(f\"Error in parameter analysis: {str(e)}\")\n",
        "\n",
        "def generate_rfdiffusion_code(pdb_content, parameters):\n",
        "    \"\"\"\n",
        "    Generate reliable RFdiffusion code using simplified parameters.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse only essential parameters\n",
        "        params = {}\n",
        "        for line in parameters.split('\\n'):\n",
        "            if ':' in line:\n",
        "                key, value = line.split(':', 1)\n",
        "                key = key.strip()\n",
        "                value = value.strip()\n",
        "\n",
        "                if key == 'contigs':\n",
        "                    # Ensure contig format includes both target and binder specifications\n",
        "                    value = value.strip('\"\\' ')\n",
        "                    if '/0' not in value:\n",
        "                        # If missing proper format, use safe default\n",
        "                        value = 'A20-60/0 50-100'\n",
        "                    elif len(value.split('/0')) != 2:\n",
        "                        # If no binder length specified after /0, add default\n",
        "                        value = f\"{value.split('/0')[0]}/0 50-100\"\n",
        "                    params[key] = value\n",
        "                elif key == 'hotspot_res':\n",
        "                    # Convert string representation to list\n",
        "                    value = eval(value) if '[' in value else [x.strip() for x in value.split(',')]\n",
        "                    params[key] = value\n",
        "                elif key == 'diffusion_steps':\n",
        "                    # Ensure minimum of 15 steps\n",
        "                    try:\n",
        "                        steps = max(15, int(value))\n",
        "                        params[key] = steps\n",
        "                    except ValueError:\n",
        "                        params[key] = 15\n",
        "\n",
        "        code = f\"\"\"#!/usr/bin/env python3\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def get_api_key():\n",
        "    return os.getenv(\"NVCF_RUN_KEY\") or input(\"Paste the Run Key: \")\n",
        "\n",
        "def get_pdb_content():\n",
        "    return \\\"\\\"\\\"\n",
        "{pdb_content}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "def run_rfdiffusion(api_key, input_pdb):\n",
        "    try:\n",
        "        url = \"https://health.api.nvidia.com/v1/biology/ipd/rfdiffusion/generate\"\n",
        "        headers = {{\n",
        "            \"Authorization\": f\"Bearer {{api_key}}\",\n",
        "            \"Accept\": \"application/json\"\n",
        "        }}\n",
        "\n",
        "        # Core parameters only - keeping it simple and reliable\n",
        "        payload = {{\n",
        "            \"input_pdb\": input_pdb,\n",
        "            \"contigs\": \"{params.get('contigs', 'A20-60/0 50-100')}\",  # Default includes both target and binder specs\n",
        "            \"hotspot_res\": {params.get('hotspot_res', ['A50', 'A51', 'A52'])},\n",
        "            \"diffusion_steps\": {params.get('diffusion_steps', 15)}\n",
        "        }}\n",
        "\n",
        "        logger.info(f\"Sending request with payload: {{payload}}\")\n",
        "        response = requests.post(url=url, headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"API request failed: {{str(e)}}\")\n",
        "        if hasattr(e.response, 'text'):\n",
        "            logger.error(f\"Response content: {{e.response.text}}\")\n",
        "        raise\n",
        "\n",
        "def save_output(response, output_file=\"output.pdb\"):\n",
        "    try:\n",
        "        logger.info(f\"Processing response to save to {{output_file}}\")\n",
        "        response_json = response.json()\n",
        "\n",
        "        if \"output_pdb\" not in response_json:\n",
        "            logger.error(f\"Expected 'output_pdb' in response, got: {{list(response_json.keys())}}\")\n",
        "            raise KeyError(\"Response does not contain 'output_pdb' key\")\n",
        "\n",
        "        Path(output_file).write_text(response_json[\"output_pdb\"])\n",
        "        logger.info(f\"Successfully saved output to {{output_file}}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error saving output: {{str(e)}}\")\n",
        "        raise\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        api_key = get_api_key()\n",
        "        input_pdb = get_pdb_content()\n",
        "        response = run_rfdiffusion(api_key, input_pdb)\n",
        "        save_output(response)\n",
        "        logger.info(\"RFdiffusion process completed successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in main execution: {{str(e)}}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "        return code\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating RFdiffusion code: {str(e)}\")\n",
        "        raise gr.Error(f\"Error generating code: {str(e)}\")\n",
        "\n",
        "def execute_rfdiffusion_script(api_key: str, pdb_content: str, parameters: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Execute the RFdiffusion script with enhanced error handling and simplified parameters.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not api_key or not api_key.strip():\n",
        "            raise ValueError(\"API key is required\")\n",
        "\n",
        "        # Parse only essential parameters\n",
        "        params = {\n",
        "            \"contigs\": \"A20-60/0 50-100\",  # Default includes both target and binder specs\n",
        "            \"hotspot_res\": [\"A50\", \"A51\", \"A52\"],\n",
        "            \"diffusion_steps\": 15\n",
        "        }\n",
        "\n",
        "        for line in parameters.split('\\n'):\n",
        "            if ':' in line and not line.startswith('-'):\n",
        "                key, value = line.split(':', 1)\n",
        "                key = key.strip()\n",
        "                value = value.strip()\n",
        "\n",
        "                if key == 'contigs':\n",
        "                    value = value.strip('\"\\' ')\n",
        "                    if '/0' not in value:\n",
        "                        value = 'A20-60/0 50-100'\n",
        "                    elif len(value.split('/0')) != 2:\n",
        "                        value = f\"{value.split('/0')[0]}/0 50-100\"\n",
        "                    params[key] = value\n",
        "                elif key == 'hotspot_res':\n",
        "                    if isinstance(value, str):\n",
        "                        if '[' in value:\n",
        "                            params[key] = eval(value)\n",
        "                        else:\n",
        "                            params[key] = [x.strip() for x in value.split(',')]\n",
        "                elif key == 'diffusion_steps':\n",
        "                    try:\n",
        "                        params[key] = max(15, int(value))\n",
        "                    except ValueError:\n",
        "                        params[key] = 15\n",
        "\n",
        "        # Make API request\n",
        "        url = \"https://health.api.nvidia.com/v1/biology/ipd/rfdiffusion/generate\"\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {api_key}\",\n",
        "            \"Accept\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        payload = {\n",
        "            \"input_pdb\": pdb_content,\n",
        "            \"contigs\": params[\"contigs\"],\n",
        "            \"hotspot_res\": params[\"hotspot_res\"],\n",
        "            \"diffusion_steps\": params[\"diffusion_steps\"]\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Making RFdiffusion API request with parameters: {params}\")\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        result = response.json()\n",
        "\n",
        "        if \"output_pdb\" in result:\n",
        "            output_pdb_content = result[\"output_pdb\"]\n",
        "            output_file = Path(\"rfdiffusion_output.pdb\")\n",
        "            output_file.write_text(output_pdb_content)\n",
        "\n",
        "            status_message = (\n",
        "                f\"Success! Generated structure saved to: {output_file}\\n\"\n",
        "                f\"Parameters used:\\n\"\n",
        "                f\"- Contigs: {params['contigs']}\\n\"\n",
        "                f\"- Hotspot residues: {params['hotspot_res']}\\n\"\n",
        "                f\"- Diffusion steps: {params['diffusion_steps']}\"\n",
        "            )\n",
        "            return status_message, output_pdb_content, True\n",
        "        else:\n",
        "            return f\"Error: Unexpected response format\\n{response.text}\", None, False\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"API Request Error: {str(e)}\")\n",
        "        if hasattr(e.response, 'text'):\n",
        "            logger.error(f\"Response content: {e.response.text}\")\n",
        "        return f\"API Request Error: {str(e)}\", None, False\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error executing RFdiffusion: {str(e)}\")\n",
        "        return f\"Error executing RFdiffusion: {str(e)}\", None, False\n",
        "\n",
        "def rfdiffusion_app(file):\n",
        "    try:\n",
        "        if file is None:\n",
        "            raise gr.Error(\"No file uploaded. Please upload a PDB file.\")\n",
        "\n",
        "        pdb_content = process_pdb_file(file)\n",
        "        parameters, explanation, audio_file = get_rfdiffusion_parameters(pdb_content)\n",
        "        code = generate_rfdiffusion_code(pdb_content, parameters)\n",
        "\n",
        "        return parameters, code, explanation, audio_file\n",
        "    except gr.Error as e:\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error in rfdiffusion_app: {str(e)}\")\n",
        "        raise gr.Error(f\"An unexpected error occurred: {str(e)}\")\n",
        "\n",
        "def get_proteinmpnn_parameters(pdb_content):\n",
        "    try:\n",
        "        query = f\"\"\"\n",
        "        Analyze the following PDB content and suggest appropriate parameters for running ProteinMPNN:\n",
        "\n",
        "        {pdb_content[:500]}...\n",
        "\n",
        "        Provide a clear, conversational explanation of:\n",
        "        1. ca_only setting and why it's appropriate\n",
        "        2. use_soluble_model choice based on the structure\n",
        "        3. num_seq_per_target recommendation\n",
        "        4. sampling_temp selection\n",
        "\n",
        "        Format your response with clear parameter values first, followed by a natural explanation suitable for voice synthesis.\n",
        "        \"\"\"\n",
        "\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert in ProteinMPNN modeling. Provide parameters followed by a clear, conversational explanation.\"},\n",
        "                {\"role\": \"user\", \"content\": query}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Split response into parameters and explanation\n",
        "        response_text = response.choices[0].message.content\n",
        "        parts = response_text.split(\"\\n\\n\", 1)\n",
        "        parameters = parts[0]\n",
        "        explanation = parts[1] if len(parts) > 1 else \"No explanation provided.\"\n",
        "\n",
        "        # Generate audio for the explanation\n",
        "        audio_handler = AudioResponseHandler(openai_client)\n",
        "        audio_file_path = audio_handler.text_to_speech(explanation)\n",
        "\n",
        "        return parameters, explanation, audio_file_path\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in GPT-4o analysis: {str(e)}\")\n",
        "        raise gr.Error(f\"Error in GPT-4o analysis: {str(e)}\")\n",
        "\n",
        "def generate_proteinmpnn_code(pdb_content, parameters):\n",
        "    try:\n",
        "        # Parse parameters from the analysis\n",
        "        params = {}\n",
        "        explanation = []\n",
        "        current_param = None\n",
        "\n",
        "        for line in parameters.split('\\n'):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            # Check if this is a parameter line\n",
        "            if ': ' in line and not line.startswith('-'):\n",
        "                key, value = line.split(':', 1)\n",
        "                key = key.strip()\n",
        "                value = value.strip()\n",
        "\n",
        "                # Convert string representations to appropriate Python types\n",
        "                if key == 'sampling_temp':\n",
        "                    try:\n",
        "                        temp = float(value)\n",
        "                        params[key] = [temp]\n",
        "                    except ValueError:\n",
        "                        params[key] = [0.1]\n",
        "                elif key in ['ca_only', 'use_soluble_model']:\n",
        "                    params[key] = value.lower() == 'true'\n",
        "                elif key == 'num_seq_per_target':\n",
        "                    try:\n",
        "                        params[key] = int(value)\n",
        "                    except ValueError:\n",
        "                        params[key] = 1\n",
        "\n",
        "                current_param = key\n",
        "            elif line.startswith('-') and current_param:\n",
        "                explanation.append((current_param, line[1:].strip()))\n",
        "\n",
        "        code = f\"\"\"#!/usr/bin/env python3\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def get_api_key():\n",
        "    return os.getenv(\"NVCF_RUN_KEY\") or input(\"Paste the Run Key: \")\n",
        "\n",
        "def get_pdb_content():\n",
        "    return \\\"\\\"\\\"\n",
        "{pdb_content}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "def run_proteinmpnn(api_key, input_pdb, ca_only, use_soluble_model, num_seq_per_target, sampling_temp):\n",
        "    url = \"https://health.api.nvidia.com/v1/biology/ipd/proteinmpnn/predict\"\n",
        "    headers = {{\"Authorization\": f\"Bearer {{api_key}}\", \"Content-Type\": \"application/json\"}}\n",
        "    payload = {{\n",
        "        \"input_pdb\": input_pdb,\n",
        "        \"ca_only\": ca_only,\n",
        "        \"use_soluble_model\": use_soluble_model,\n",
        "        \"num_seq_per_target\": num_seq_per_target,\n",
        "        \"sampling_temp\": sampling_temp\n",
        "    }}\n",
        "\n",
        "    response = requests.post(url=url, headers=headers, json=payload)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"API request failed with status {{response.status_code}}: {{response.text}}\")\n",
        "    return response\n",
        "\n",
        "def save_output(response, output_file=\"output.fasta\"):\n",
        "    try:\n",
        "        response_data = json.loads(response.text)\n",
        "        if \"mfasta\" not in response_data:\n",
        "            raise KeyError(\"Response does not contain 'mfasta' key\")\n",
        "        print(response, f\"Saving to {{output_file}}:\\\\n\", response.text[:200], \"...\")\n",
        "        Path(output_file).write_text(response_data[\"mfasta\"])\n",
        "    except json.JSONDecodeError as e:\n",
        "        raise Exception(f\"Failed to parse response JSON: {{str(e)}}\")\n",
        "    except KeyError as e:\n",
        "        raise Exception(f\"Invalid response format: {{str(e)}}\")\n",
        "\n",
        "def main():\n",
        "    # Parameters explanation from structure analysis:\n",
        "{chr(10).join(f'    # {param}: {exp}' for param, exp in explanation)}\n",
        "\n",
        "    api_key = get_api_key()\n",
        "    input_pdb = get_pdb_content()\n",
        "    ca_only = {str(params.get('ca_only', False))}\n",
        "    use_soluble_model = {str(params.get('use_soluble_model', False))}\n",
        "    num_seq_per_target = {params.get('num_seq_per_target', 1)}\n",
        "    sampling_temp = {params.get('sampling_temp', [0.1])}\n",
        "\n",
        "    try:\n",
        "        response = run_proteinmpnn(api_key, input_pdb, ca_only, use_soluble_model, num_seq_per_target, sampling_temp)\n",
        "        save_output(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {{str(e)}}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "        return code\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating ProteinMPNN code: {{str(e)}}\")\n",
        "        raise gr.Error(f\"Error generating ProteinMPNN code: {{str(e)}}\")\n",
        "\n",
        "# Add this function to execute ProteinMPNN script\n",
        "def execute_proteinmpnn_script(api_key: str, pdb_content: str, parameters: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Execute the ProteinMPNN script and return both results and FASTA content.\n",
        "    Returns tuple of (status_message, fasta_content, success_bool)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not api_key or not api_key.strip():\n",
        "            raise ValueError(\"API key is required\")\n",
        "\n",
        "        # Parse parameters from the analysis\n",
        "        params = {}\n",
        "        for line in parameters.split('\\n'):\n",
        "            if ':' in line and not line.startswith('-'):\n",
        "                key, value = line.split(':', 1)\n",
        "                key = key.strip()\n",
        "                value = value.strip()\n",
        "\n",
        "                # Convert string representations to appropriate Python types\n",
        "                if key == 'sampling_temp':\n",
        "                    try:\n",
        "                        temp = float(value)\n",
        "                        params[key] = [temp]\n",
        "                    except ValueError:\n",
        "                        params[key] = [0.1]\n",
        "                elif key in ['ca_only', 'use_soluble_model']:\n",
        "                    params[key] = value.lower() == 'true'\n",
        "                elif key == 'num_seq_per_target':\n",
        "                    try:\n",
        "                        params[key] = int(value)\n",
        "                    except ValueError:\n",
        "                        params[key] = 1\n",
        "\n",
        "        # Make API request\n",
        "        url = \"https://health.api.nvidia.com/v1/biology/ipd/proteinmpnn/predict\"\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"Authorization\": f\"Bearer {api_key}\"\n",
        "        }\n",
        "        payload = {\n",
        "            \"input_pdb\": pdb_content,\n",
        "            \"ca_only\": params.get('ca_only', False),\n",
        "            \"use_soluble_model\": params.get('use_soluble_model', False),\n",
        "            \"num_seq_per_target\": params.get('num_seq_per_target', 1),\n",
        "            \"sampling_temp\": params.get('sampling_temp', [0.1])\n",
        "        }\n",
        "\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse the response\n",
        "        result = response.json()\n",
        "\n",
        "        if \"mfasta\" in result:\n",
        "            fasta_content = result[\"mfasta\"]\n",
        "\n",
        "            # Save the output FASTA to a temporary file\n",
        "            output_file = Path(\"proteinmpnn_output.fasta\")\n",
        "            output_file.write_text(fasta_content)\n",
        "\n",
        "            status_message = (\n",
        "                f\"Success! Generated sequences saved to: {output_file}\\n\\n\"\n",
        "                f\"Output FASTA summary:\\n\"\n",
        "                f\"- File size: {len(fasta_content)} bytes\\n\"\n",
        "                f\"- First few lines:\\n{fasta_content[:200]}...\"\n",
        "            )\n",
        "            return status_message, fasta_content, True\n",
        "        else:\n",
        "            return f\"Error: Unexpected response format\\n{response.text}\", None, False\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"API Request Error: {str(e)}\", None, False\n",
        "    except Exception as e:\n",
        "        return f\"Error executing ProteinMPNN: {str(e)}\", None, False\n",
        "\n",
        "def proteinmpnn_app(file, api_key: str = None):\n",
        "    try:\n",
        "        if file is None:\n",
        "            raise gr.Error(\"No file uploaded. Please upload a PDB file.\")\n",
        "\n",
        "        pdb_content = process_pdb_file(file)\n",
        "        parameters, explanation, audio_file = get_proteinmpnn_parameters(pdb_content)\n",
        "        code = generate_proteinmpnn_code(pdb_content, parameters)\n",
        "\n",
        "        # If API key is provided, execute the script\n",
        "        if api_key:\n",
        "            status, fasta_content, success = execute_proteinmpnn_script(api_key, pdb_content, parameters)\n",
        "            return parameters, code, explanation, audio_file, status, fasta_content\n",
        "\n",
        "        return parameters, code, explanation, audio_file, \"\", \"\"\n",
        "\n",
        "    except gr.Error as e:\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error in proteinmpnn_app: {str(e)}\")\n",
        "        raise gr.Error(f\"An unexpected error occurred: {str(e)}\")\n",
        "\n",
        "# New function for PDB visualization\n",
        "def visualize_pdb(file):\n",
        "    if file is None:\n",
        "        return None\n",
        "    return file\n",
        "\n",
        "# New function for Nvidia NIMs chat\n",
        "def chat_with_nims(message, chat_history, history_state):\n",
        "    try:\n",
        "        if history_state is None:\n",
        "            history_state = []\n",
        "\n",
        "        # Query the Pinecone database for relevant information\n",
        "        query_result = query_engine.query(message)\n",
        "        relevant_info = query_result.response\n",
        "\n",
        "        # Prepare the system message\n",
        "        system_message = f\"\"\"You are an AI assistant specializing in Nvidia NIMs (NVIDIA AI Foundation models) and protein design research.\n",
        "        Use the following information to help answer the user's question: {relevant_info}\n",
        "        If you don't have enough information to answer the question, please say so and offer to help with other aspects of Nvidia NIMs or protein design.\"\"\"\n",
        "\n",
        "        history_state.append({\"role\": \"system\", \"content\": system_message})\n",
        "        history_state.append({\"role\": \"user\", \"content\": message})\n",
        "        messages = history_state.copy()\n",
        "\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=messages,\n",
        "            max_tokens=16384,\n",
        "            temperature=0.7,\n",
        "            stream=True\n",
        "        )\n",
        "\n",
        "        # Get the complete response text\n",
        "        response_text = response.choices[0].message.content\n",
        "\n",
        "        # Update chat history\n",
        "        chat_history = chat_history + [(message, response_text)]\n",
        "\n",
        "        # Generate audio for the complete response\n",
        "        audio_handler = AudioResponseHandler(openai_client)\n",
        "        audio_file_path = audio_handler.text_to_speech(response_text)\n",
        "\n",
        "        for chunk in response:\n",
        "            if chunk.choices:\n",
        "                delta = chunk.choices[0].delta\n",
        "                if delta.content:\n",
        "                    token = delta.content\n",
        "                    response_text += token\n",
        "                    chat_history[-1] = (message, response_text)\n",
        "                    yield \"\", chat_history, history_state\n",
        "\n",
        "        history_state.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "\n",
        "        return message, chat_history, history_state, audio_file_path\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in Nvidia NIMs chat: {str(e)}\")\n",
        "        yield \"\", chat_history, history_state\n",
        "\n",
        "def get_molmim_parameters(smiles):\n",
        "    try:\n",
        "        query = f\"\"\"\n",
        "        Analyze the following SMILES string and suggest appropriate parameters for running MolMIM:\n",
        "\n",
        "        {smiles}\n",
        "\n",
        "        Provide the following parameters:\n",
        "        1. algorithm\n",
        "        2. num_molecules\n",
        "        3. property_name\n",
        "        4. minimize\n",
        "        5. min_similarity\n",
        "        6. particles\n",
        "        7. iterations\n",
        "\n",
        "        Explain your reasoning for each parameter.\n",
        "        \"\"\"\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert in molecular generation and optimization using MolMIM.\"},\n",
        "                {\"role\": \"user\", \"content\": query}\n",
        "            ]\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in GPT-4 analysis: {str(e)}\")\n",
        "        raise gr.Error(f\"Error in GPT-4 analysis: {str(e)}\")\n",
        "\n",
        "def generate_molmim_code(smiles, parameters):\n",
        "    try:\n",
        "        params = {}\n",
        "        for line in parameters.split('\\n'):\n",
        "            if ':' in line:\n",
        "                key, value = line.split(':', 1)\n",
        "                params[key.strip()] = value.strip()\n",
        "\n",
        "        code = f\"\"\"\n",
        "import requests\n",
        "\n",
        "def run_molmim(api_key, smiles):\n",
        "    invoke_url = \"https://health.api.nvidia.com/v1/biology/nvidia/molmim/generate\"\n",
        "    headers = {{\n",
        "        \"Authorization\": f\"Bearer {{api_key}}\",\n",
        "        \"Accept\": \"application/json\",\n",
        "    }}\n",
        "    payload = {{\n",
        "        \"algorithm\": \"{params.get('algorithm', 'CMA-ES')}\",\n",
        "        \"num_molecules\": {params.get('num_molecules', '30')},\n",
        "        \"property_name\": \"{params.get('property_name', 'QED')}\",\n",
        "        \"minimize\": {params.get('minimize', 'False')},\n",
        "        \"min_similarity\": {params.get('min_similarity', '0.3')},\n",
        "        \"particles\": {params.get('particles', '30')},\n",
        "        \"iterations\": {params.get('iterations', '10')},\n",
        "        \"smi\": \"{smiles}\"\n",
        "    }}\n",
        "\n",
        "    session = requests.Session()\n",
        "    response = session.post(invoke_url, headers=headers, json=payload)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "def main():\n",
        "    api_key = input(\"Enter your NVIDIA API key: \")\n",
        "    smiles = \"{smiles}\"\n",
        "    result = run_molmim(api_key, smiles)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "        return code\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating MolMIM code: {str(e)}\")\n",
        "        raise gr.Error(f\"Error generating MolMIM code: {str(e)}\")\n",
        "\n",
        "def molmim_app(smiles):\n",
        "    try:\n",
        "        if not smiles:\n",
        "            raise gr.Error(\"Please enter a SMILES string.\")\n",
        "\n",
        "        parameters = get_molmim_parameters(smiles)\n",
        "        code = generate_molmim_code(smiles, parameters)\n",
        "\n",
        "        return parameters, code\n",
        "    except gr.Error as e:\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error in molmim_app: {str(e)}\")\n",
        "        raise gr.Error(f\"An unexpected error occurred: {str(e)}\")\n",
        "\n",
        "# Add these functions with your other function definitions\n",
        "def analyze_sequence_with_gpt4(query_engine, sequence):\n",
        "    try:\n",
        "        query = f\"\"\"\n",
        "        Analyze the following amino acid sequence and suggest optimal parameters for running AlphaFold2 on the NVIDIA GPU Cloud (NGC):\n",
        "\n",
        "        Sequence: {sequence[:100]}...\n",
        "\n",
        "        Provide a clear and concise explanation of the recommended AlphaFold2 parameters for this sequence, focusing on:\n",
        "        1. algorithm\n",
        "        2. e_value\n",
        "        3. iterations\n",
        "        4. databases\n",
        "        5. relax_prediction\n",
        "        6. structure_model_preset\n",
        "        7. structure_models_to_relax\n",
        "        8. num_predictions_per_model\n",
        "        9. max_msa_sequences\n",
        "        10. template_searcher\n",
        "\n",
        "        Explain why each parameter is recommended for this specific sequence.\n",
        "        \"\"\"\n",
        "        with tracing.start_as_current_span(\"analyze_sequence_with_gpt4\") as span:\n",
        "            span.set_attribute(\"sequence_length\", len(sequence))\n",
        "            response = query_engine.query(query)\n",
        "            logger.info(\"Successfully analyzed sequence with GPT-4\")\n",
        "            return str(response)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in GPT-4 analysis: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def generate_alphafold2_code(sequence, analysis):\n",
        "    try:\n",
        "        with tracing.start_as_current_span(\"generate_alphafold2_code\") as span:\n",
        "            span.set_attribute(\"sequence_length\", len(sequence))\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "            Given the following amino acid sequence and analysis, generate a Python script to run AlphaFold2 using the NVIDIA NGC API. Use the template provided below, but modify the parameters in the `data` dictionary based on the analysis.\n",
        "\n",
        "            Sequence: {sequence[:100]}...\n",
        "\n",
        "            Analysis: {analysis}\n",
        "\n",
        "            Template:\n",
        "            ```python\n",
        "            #!/usr/bin/env python3\n",
        "            import os\n",
        "            import requests\n",
        "            import time\n",
        "            from pathlib import Path\n",
        "\n",
        "            # Variables\n",
        "            key = os.getenv(\"NVCF_RUN_KEY\") or input(\"Paste the Run Key: \")\n",
        "            url = os.getenv(\"URL\", \"https://health.api.nvidia.com/v1/biology/deepmind/alphafold2-multimer\")\n",
        "            status_url = os.getenv(\"STATUS_URL\", \"https://health.api.nvidia.com/v1/status\")\n",
        "\n",
        "            sequences = [\n",
        "                \"{sequence}\"\n",
        "            ]\n",
        "\n",
        "            output_file = Path(\"output.json\")\n",
        "\n",
        "            # Request to predict structure from a list of sequences\n",
        "            headers = {{\n",
        "                \"content-type\": \"application/json\",\n",
        "                \"Authorization\": f\"Bearer {{key}}\",\n",
        "                \"NVCF-POLL-SECONDS\": \"5\",\n",
        "            }}\n",
        "            data = {{\n",
        "                \"sequences\": sequences,\n",
        "                \"algorithm\": \"jackhmmer\",\n",
        "                \"e_value\": 0.0001,\n",
        "                \"iterations\": 1,\n",
        "                \"databases\": [\"uniref90\", \"small_bfd\", \"mgnify\"],\n",
        "                \"relax_prediction\": True,\n",
        "                # Add other parameters based on the analysis\n",
        "            }}\n",
        "\n",
        "            print(\"Making request...\")\n",
        "            response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "            # Check the status code\n",
        "            if response.status_code == 200:\n",
        "                output_file.write_text(response.text)\n",
        "                print(f\"Response output to file: {{output_file}}\")\n",
        "            elif response.status_code == 202:\n",
        "                print(\"Request accepted...\")\n",
        "                # Extract reqId header\n",
        "                req_id = response.headers.get(\"nvcf-reqid\")\n",
        "\n",
        "                # Poll the /status endpoint\n",
        "                while True:\n",
        "                    print(\"Polling for response...\")\n",
        "                    status_response = requests.get(f\"{{status_url}}/{{req_id}}\", headers=headers)\n",
        "\n",
        "                    if status_response.status_code != 202:\n",
        "                        output_file.write_text(status_response.text)\n",
        "                        print(f\"Response output to file: {{output_file}}\")\n",
        "                        break\n",
        "\n",
        "                    # Wait before polling again\n",
        "                    time.sleep(5)\n",
        "            else:\n",
        "                print(f\"Unexpected HTTP status: {{response.status_code}}\")\n",
        "                print(f\"Response: {{response.text}}\")\n",
        "            ```\n",
        "\n",
        "            Generate the Python script, including detailed comments explaining each parameter choice based on the analysis.\n",
        "            \"\"\"\n",
        "\n",
        "            response = openai_client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates Python code for running AlphaFold2.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            code = response.choices[0].message.content\n",
        "            return code\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating AlphaFold2 code: {str(e)}\")\n",
        "        raise gr.Error(f\"Error generating AlphaFold2 code: {str(e)}\")\n",
        "\n",
        "def alphafold2_app(sequence):\n",
        "    try:\n",
        "        with tracing.start_as_current_span(\"alphafold2_app\") as span:\n",
        "            if not sequence:\n",
        "                raise gr.Error(\"No sequence provided. Please enter an amino acid sequence.\")\n",
        "\n",
        "            # Remove any whitespace and validate the sequence\n",
        "            sequence = ''.join(sequence.split())\n",
        "            valid_aa = set('ACDEFGHIKLMNPQRSTVWY')\n",
        "            if not set(sequence).issubset(valid_aa):\n",
        "                raise gr.Error(\"Invalid amino acid sequence. Please enter a valid sequence using standard one-letter amino acid codes.\")\n",
        "\n",
        "            span.set_attribute(\"sequence_length\", len(sequence))\n",
        "            span.set_attribute(\"sequence_valid\", True)\n",
        "\n",
        "            # Analyze the sequence using GPT-4\n",
        "            analysis = analyze_sequence_with_gpt4(query_engine, sequence)\n",
        "\n",
        "            # Generate AlphaFold2 code\n",
        "            code = generate_alphafold2_code(sequence, analysis)\n",
        "\n",
        "            return analysis, code\n",
        "    except gr.Error as e:\n",
        "        # Re-raise Gradio errors to display them in the UI\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error in alphafold2_app: {str(e)}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "        raise gr.Error(f\"An unexpected error occurred: {str(e)}\")\n",
        "\n",
        "# Add these GROMACS-specific functions before your main Gradio interface definition\n",
        "def analyze_pdb_structure(file_path):\n",
        "    try:\n",
        "        # Initialize PDB parser\n",
        "        parser = PDB.PDBParser(QUIET=True)\n",
        "        structure = parser.get_structure('protein', file_path)\n",
        "\n",
        "        # Extract basic information\n",
        "        chains = list(structure.get_chains())\n",
        "        residues = list(structure[0].get_residues())\n",
        "        atoms = list(structure.get_atoms())\n",
        "\n",
        "        # Calculate structure properties\n",
        "        n_chains = len(chains)\n",
        "        n_residues = len(residues)\n",
        "        n_atoms = len(atoms)\n",
        "\n",
        "        # Try to get secondary structure information\n",
        "        try:\n",
        "            dssp_dict = dssp_dict_from_pdb_file(file_path)\n",
        "            ss_composition = {}\n",
        "            for residue in dssp_dict:\n",
        "                ss = dssp_dict[residue][2]\n",
        "                ss_composition[ss] = ss_composition.get(ss, 0) + 1\n",
        "        except Exception as e:\n",
        "            ss_composition = {\"Error\": \"Could not calculate secondary structure\"}\n",
        "\n",
        "        structure_info = {\n",
        "            \"chains\": n_chains,\n",
        "            \"residues\": n_residues,\n",
        "            \"atoms\": n_atoms,\n",
        "            \"secondary_structure\": ss_composition\n",
        "        }\n",
        "\n",
        "        # Create a text summary\n",
        "        summary = (\n",
        "            f\"Structure Analysis:\\n\"\n",
        "            f\"- Number of chains: {n_chains}\\n\"\n",
        "            f\"- Number of residues: {n_residues}\\n\"\n",
        "            f\"- Number of atoms: {n_atoms}\\n\"\n",
        "            f\"- Secondary structure composition: {ss_composition}\\n\"\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Successfully analyzed PDB structure: {file_path}\")\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error analyzing PDB structure: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def get_nvidia_info(query_engine, structure_info):\n",
        "    try:\n",
        "        query = f\"Provide information on NVIDIA tools and optimizations relevant to running GROMACS simulations for a protein structure with the following characteristics: {structure_info}\"\n",
        "        response = query_engine.query(query)\n",
        "        logger.info(\"Successfully retrieved NVIDIA information\")\n",
        "        return str(response)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error querying NVIDIA information: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def process_structure(file_path, query_engine, llm, deepseek_client) -> Tuple[Protocol, str]:\n",
        "    try:\n",
        "        # Analyze PDB structure\n",
        "        structure_info = analyze_pdb_structure(file_path)\n",
        "        nvidia_info = get_nvidia_info(query_engine, structure_info)\n",
        "\n",
        "        # Create initial protocol with GPT-4o\n",
        "        protocol_prompt = (\n",
        "            f\"You are an expert in GROMACS molecular dynamics simulations and NVIDIA scientific computing tools. \"\n",
        "            f\"Create a detailed step-by-steo GROMACS simulation protocol for the following protein structure, \"\n",
        "            f\"incorporating NVIDIA-specific tools and optimizations where appropriate. \"\n",
        "            f\"Consider the structure's characteristics when designing the protocol.\\n\\n\"\n",
        "            f\"Structure information:\\n{structure_info}\\n\\n\"\n",
        "            f\"NVIDIA tool information: {nvidia_info}\\n\\n\"\n",
        "            f\"Create a detailed GROMACS protocol for this structure, including system preparation, \"\n",
        "            f\"equilibration, and production MD steps. Include specific NVIDIA optimizations.\"\n",
        "        )\n",
        "\n",
        "        # Use the complete() method directly\n",
        "        protocol_response = llm.complete(protocol_prompt)\n",
        "        initial_steps = protocol_response.text.split('\\n')\n",
        "\n",
        "        # Evaluate and improve protocol using DeepSeek\n",
        "        improved_steps = evaluate_protocol_with_deepseek(initial_steps, deepseek_client)\n",
        "        protocol_text = '\\n'.join(improved_steps)\n",
        "\n",
        "        # Generate NVIDIA benefits description\n",
        "        benefits_prompt = (\n",
        "            f\"Based on the improved GROMACS protocol and the structure information provided, \"\n",
        "            f\"describe in detail the advantages of using NVIDIA technology for this specific \"\n",
        "            f\"protein simulation. Focus on performance improvements, specific NVIDIA features \"\n",
        "            f\"utilized, and how they enhance the simulation process.\\n\\n\"\n",
        "            f\"Structure information: {structure_info}\\n\"\n",
        "            f\"Protocol: {protocol_text}\\n\\n\"\n",
        "            f\"NVIDIA tool information: {nvidia_info}\\n\\n\"\n",
        "            f\"Provide a comprehensive explanation of the benefits of NVIDIA technology in this context.\"\n",
        "        )\n",
        "\n",
        "        benefits_response = llm.complete(benefits_prompt)\n",
        "\n",
        "        logger.info(\"Successfully generated improved GROMACS protocol and NVIDIA benefits description\")\n",
        "        return Protocol(steps=improved_steps), benefits_response.text\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing structure: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def evaluate_protocol_with_deepseek(protocol_steps: List[str], deepseek_client: OpenAI) -> List[str]:\n",
        "    try:\n",
        "        protocol_text = '\\n'.join(protocol_steps)\n",
        "        evaluation_prompt = (\n",
        "            f\"Analyze this GROMACS simulation protocol and create a new detailed GROMACS protocol for this structure, including system preparation, \"\n",
        "            f\"include gromacs commands for system preparation, equilibration, and production MD steps. Include specific NVIDIA optimizations.\"\n",
        "            f\"Current protocol steps:\\n\"\n",
        "            f\"{protocol_text}\\n\\n\"\n",
        "            f\"focusing on optimal use of modern GROMACS features that can be enhanced with NVIDIA GPUs.\"\n",
        "        )\n",
        "\n",
        "        completion = deepseek_client.chat.completions.create(\n",
        "            model=\"deepseek-ai/deepseek-coder-6.7b-instruct\",\n",
        "            messages=[{\"role\": \"user\", \"content\": evaluation_prompt}],\n",
        "            temperature=0.0,\n",
        "            top_p=1,\n",
        "            max_tokens=1024\n",
        "        )\n",
        "\n",
        "        improved_steps = completion.choices[0].message.content.split('\\n')\n",
        "        improved_steps = [step.strip() for step in improved_steps if step.strip()]\n",
        "\n",
        "        logger.info(\"Successfully evaluated and improved protocol with DeepSeek\")\n",
        "        return improved_steps\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in DeepSeek evaluation: {str(e)}\")\n",
        "        return protocol_steps\n",
        "\n",
        "# Modify your gromacs_interface function to pass the llm parameter\n",
        "def gromacs_interface(file):\n",
        "    try:\n",
        "        if file is None:\n",
        "            return \"Please upload a PDB file.\", \"No file uploaded.\"\n",
        "\n",
        "        if hasattr(file, 'name'):\n",
        "            file_path = file.name\n",
        "        else:\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdb\") as temp_file:\n",
        "                temp_file.write(file.read() if hasattr(file, 'read') else file)\n",
        "                file_path = temp_file.name\n",
        "\n",
        "        logger.info(f\"Processing file: {file_path}\")\n",
        "        protocol, nvidia_benefits = process_structure(file_path, query_engine, llm, deepseek_client)\n",
        "\n",
        "        if file_path != getattr(file, 'name', None):\n",
        "            os.unlink(file_path)\n",
        "\n",
        "        protocol_output = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(protocol.steps))\n",
        "        return protocol_output, nvidia_benefits\n",
        "    except Exception as e:\n",
        "        error_msg = f\"An error occurred: {str(e)}\\n\\nPlease check the logs for more details.\"\n",
        "        logger.error(error_msg)\n",
        "        return error_msg, error_msg\n",
        "\n",
        "def get_diffdock_parameters(protein_content, ligand_smiles):\n",
        "    try:\n",
        "        query = f\"\"\"\n",
        "        Analyze the following protein structure and ligand SMILES string and suggest appropriate parameters for running DiffDock:\n",
        "\n",
        "        Protein structure: {protein_content[:500]}...\n",
        "        Ligand SMILES: {ligand_smiles}\n",
        "\n",
        "        Provide the following parameters:\n",
        "        1. num_poses: Number of poses to generate (typically 20-100)\n",
        "        2. time_divisions: Number of time divisions for diffusion (typically 20)\n",
        "        3. steps: Number of diffusion steps (typically 18-25)\n",
        "        4. save_trajectory: Whether to save the diffusion trajectory\n",
        "        5. is_staged: Whether to use staged diffusion\n",
        "        6. batch_size: Batch size for processing\n",
        "\n",
        "        Explain your reasoning for each parameter.\n",
        "        \"\"\"\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert in molecular docking using DiffDock.\"},\n",
        "                {\"role\": \"user\", \"content\": query}\n",
        "            ]\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in GPT-4o analysis: {str(e)}\")\n",
        "        raise gr.Error(f\"Error in GPT-4o analysis: {str(e)}\")\n",
        "\n",
        "def generate_diffdock_code(protein_content, ligand_smiles, parameters):\n",
        "    try:\n",
        "        # Parse parameters from the analysis\n",
        "        params = {}\n",
        "        for line in parameters.split('\\n'):\n",
        "            if ':' in line:\n",
        "                key, value = line.split(':', 1)\n",
        "                key = key.strip()\n",
        "                value = value.strip()\n",
        "\n",
        "                if key in ['num_poses', 'time_divisions', 'steps']:\n",
        "                    params[key] = int(value) if value.isdigit() else 20\n",
        "                elif key in ['save_trajectory', 'is_staged']:\n",
        "                    params[key] = value.lower() == 'true'\n",
        "\n",
        "        code = f\"\"\"\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "\n",
        "def upload_asset(input_data, auth_header):\n",
        "    assets_url = \"https://api.nvcf.nvidia.com/v2/nvcf/assets\"\n",
        "    headers = {{\n",
        "        \"Authorization\": auth_header,\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"accept\": \"application/json\",\n",
        "    }}\n",
        "    s3_headers = {{\n",
        "        \"x-amz-meta-nvcf-asset-description\": \"diffdock-file\",\n",
        "        \"content-type\": \"text/plain\",\n",
        "    }}\n",
        "    payload = {{\n",
        "        \"contentType\": \"text/plain\",\n",
        "        \"description\": \"diffdock-file\"\n",
        "    }}\n",
        "\n",
        "    response = requests.post(assets_url, headers=headers, json=payload)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    asset_url = response.json()[\"uploadUrl\"]\n",
        "    asset_id = response.json()[\"assetId\"]\n",
        "\n",
        "    response = requests.put(asset_url, data=input_data, headers=s3_headers)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    return asset_id\n",
        "\n",
        "def run_diffdock(api_key):\n",
        "    url = \"https://health.api.nvidia.com/v1/biology/mit/diffdock\"\n",
        "    header_auth = f\"Bearer {{api_key}}\"\n",
        "\n",
        "    # Upload protein structure\n",
        "    protein_content = \\\"\\\"\\\"\n",
        "{protein_content}\n",
        "    \\\"\\\"\\\"\n",
        "    protein_id = upload_asset(protein_content, header_auth)\n",
        "\n",
        "    # Upload ligand SMILES\n",
        "    ligand_content = \\\"{ligand_smiles}\\\"\n",
        "    ligand_id = upload_asset(ligand_content, header_auth)\n",
        "\n",
        "    headers = {{\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"NVCF-INPUT-ASSET-REFERENCES\": \",\".join([protein_id, ligand_id]),\n",
        "        \"Authorization\": header_auth\n",
        "    }}\n",
        "\n",
        "    payload = {{\n",
        "        \"ligand\": ligand_id,\n",
        "        \"ligand_file_type\": \"smiles\",\n",
        "        \"protein\": protein_id,\n",
        "        \"num_poses\": {params.get('num_poses', 20)},\n",
        "        \"time_divisions\": {params.get('time_divisions', 20)},\n",
        "        \"steps\": {params.get('steps', 18)},\n",
        "        \"save_trajectory\": {str(params.get('save_trajectory', True)).lower()},\n",
        "        \"is_staged\": {str(params.get('is_staged', True)).lower()}\n",
        "    }}\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    return response.json()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    api_key = os.getenv(\"NVIDIA_API_KEY\") or input(\"Enter your NVIDIA API key: \")\n",
        "    result = run_diffdock(api_key)\n",
        "    print(result)\n",
        "\"\"\"\n",
        "        return code\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating DiffDock code: {str(e)}\")\n",
        "        raise gr.Error(f\"Error generating DiffDock code: {str(e)}\")\n",
        "\n",
        "def execute_diffdock_script(api_key: str, protein_content: str, ligand_smiles: str, parameters: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Execute DiffDock with enhanced error handling and input validation.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Input validation with detailed logging\n",
        "        if not api_key or not api_key.strip():\n",
        "            raise ValueError(\"API key is required\")\n",
        "\n",
        "        if not protein_content or not protein_content.strip():\n",
        "            raise ValueError(\"Protein content is empty\")\n",
        "\n",
        "        if not ligand_smiles or not ligand_smiles.strip():\n",
        "            raise ValueError(\"Ligand SMILES is empty\")\n",
        "\n",
        "        logger.info(f\"Starting DiffDock execution with:\"\n",
        "                   f\"\\nProtein content length: {len(protein_content)}\"\n",
        "                   f\"\\nLigand SMILES length: {len(ligand_smiles)}\")\n",
        "\n",
        "        # Define API endpoints\n",
        "        assets_url = \"https://api.nvcf.nvidia.com/v2/nvcf/assets\"\n",
        "        diffdock_url = \"https://health.api.nvidia.com/v1/biology/mit/diffdock\"\n",
        "\n",
        "        # Set up headers\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {api_key}\",\n",
        "            \"accept\": \"application/json\",\n",
        "            \"content-type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        def upload_asset(content, description=\"diffdock-file\"):\n",
        "            logger.info(f\"Uploading {description} (content length: {len(content)})\")\n",
        "\n",
        "            # Request upload URL\n",
        "            payload = {\n",
        "                \"contentType\": \"text/plain\",\n",
        "                \"description\": description\n",
        "            }\n",
        "            upload_response = requests.post(assets_url, headers=headers, json=payload)\n",
        "\n",
        "            if upload_response.status_code != 200:\n",
        "                logger.error(f\"Asset upload URL request failed: {upload_response.text}\")\n",
        "                raise ValueError(f\"Failed to get upload URL: {upload_response.status_code}\")\n",
        "\n",
        "            upload_data = upload_response.json()\n",
        "            asset_url = upload_data[\"uploadUrl\"]\n",
        "            asset_id = upload_data[\"assetId\"]\n",
        "\n",
        "            # Upload content\n",
        "            s3_headers = {\n",
        "                \"x-amz-meta-nvcf-asset-description\": description,\n",
        "                \"content-type\": \"text/plain\",\n",
        "            }\n",
        "            put_response = requests.put(asset_url, data=content, headers=s3_headers)\n",
        "\n",
        "            if put_response.status_code != 200:\n",
        "                logger.error(f\"Asset upload failed: {put_response.text}\")\n",
        "                raise ValueError(f\"Failed to upload asset: {put_response.status_code}\")\n",
        "\n",
        "            logger.info(f\"Successfully uploaded {description} with asset ID: {asset_id}\")\n",
        "            return asset_id\n",
        "\n",
        "        # Upload protein and ligand with error checking\n",
        "        try:\n",
        "            protein_id = upload_asset(protein_content, \"protein-structure\")\n",
        "            ligand_id = upload_asset(ligand_smiles, \"ligand-smiles\")\n",
        "        except Exception as upload_error:\n",
        "            logger.error(f\"Asset upload failed: {str(upload_error)}\")\n",
        "            return f\"Error uploading assets: {str(upload_error)}\", None, False\n",
        "\n",
        "        # Parse parameters with validation\n",
        "        parsed_params = {\n",
        "            \"num_poses\": 20,\n",
        "            \"time_divisions\": 20,\n",
        "            \"steps\": 18,\n",
        "            \"save_trajectory\": False,\n",
        "            \"is_staged\": True\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            for line in parameters.split('\\n'):\n",
        "                if ':' in line and not line.startswith('-'):\n",
        "                    key, value = line.split(':', 1)\n",
        "                    key = key.strip()\n",
        "                    value = value.strip()\n",
        "\n",
        "                    if key == 'num_poses':\n",
        "                        value = int(value) if value.isdigit() else 20\n",
        "                        parsed_params[key] = max(1, min(100, value))\n",
        "                    elif key == 'time_divisions':\n",
        "                        value = int(value) if value.isdigit() else 20\n",
        "                        parsed_params[key] = max(3, min(20, value))\n",
        "                    elif key == 'steps':\n",
        "                        value = int(value) if value.isdigit() else 18\n",
        "                        parsed_params[key] = max(1, min(18, value))\n",
        "                    elif key in ['save_trajectory', 'is_staged']:\n",
        "                        parsed_params[key] = value.lower() == 'true'\n",
        "        except Exception as parse_error:\n",
        "            logger.error(f\"Parameter parsing failed: {str(parse_error)}\")\n",
        "            return f\"Error parsing parameters: {str(parse_error)}\", None, False\n",
        "\n",
        "        # Prepare payload\n",
        "        payload = {\n",
        "            \"ligand\": ligand_id,\n",
        "            \"ligand_file_type\": \"smiles\",\n",
        "            \"protein\": protein_id,\n",
        "            \"num_poses\": parsed_params[\"num_poses\"],\n",
        "            \"time_divisions\": parsed_params[\"time_divisions\"],\n",
        "            \"steps\": parsed_params[\"steps\"],\n",
        "            \"save_trajectory\": parsed_params[\"save_trajectory\"],\n",
        "            \"is_staged\": True  # Always true when using asset IDs\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Making DiffDock API request with configuration:\"\n",
        "                   f\"\\nnum_poses: {payload['num_poses']}\"\n",
        "                   f\"\\ntime_divisions: {payload['time_divisions']}\"\n",
        "                   f\"\\nsteps: {payload['steps']}\")\n",
        "\n",
        "        # Make request with detailed error handling\n",
        "        response = requests.post(diffdock_url, json=payload, headers=headers)\n",
        "\n",
        "        # Log the full response for debugging\n",
        "        logger.info(f\"DiffDock API response status: {response.status_code}\")\n",
        "        logger.info(f\"DiffDock API response headers: {response.headers}\")\n",
        "        try:\n",
        "            response_json = response.json()\n",
        "            logger.info(f\"DiffDock API response body: {response_json}\")\n",
        "        except:\n",
        "            logger.info(f\"DiffDock API response text: {response.text}\")\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            error_msg = f\"DiffDock API error (status {response.status_code}): {response.text}\"\n",
        "            logger.error(error_msg)\n",
        "            return error_msg, None, False\n",
        "\n",
        "        result = response.json()\n",
        "\n",
        "        # Process results\n",
        "        output_files = {}\n",
        "        if \"docked_poses\" in result:\n",
        "            poses_file = \"diffdock_poses.sdf\"\n",
        "            with open(poses_file, 'w') as f:\n",
        "                f.write(result[\"docked_poses\"])\n",
        "            output_files[\"poses\"] = poses_file\n",
        "\n",
        "        if parsed_params[\"save_trajectory\"] and \"trajectory\" in result:\n",
        "            traj_file = \"diffdock_trajectory.pdb\"\n",
        "            with open(traj_file, 'w') as f:\n",
        "                f.write(result[\"trajectory\"])\n",
        "            output_files[\"trajectory\"] = traj_file\n",
        "\n",
        "        status_message = (\n",
        "            f\"DiffDock execution completed!\\n\"\n",
        "            f\"Configuration:\\n\"\n",
        "            f\"- Generated poses: {parsed_params['num_poses']}\\n\"\n",
        "            f\"- Time divisions: {parsed_params['time_divisions']}\\n\"\n",
        "            f\"- Steps: {parsed_params['steps']}\\n\\n\"\n",
        "            f\"Output files:\\n\"\n",
        "            f\"- Poses: {output_files.get('poses', 'Not available')}\\n\"\n",
        "            f\"- Trajectory: {output_files.get('trajectory', 'Not available')}\\n\"\n",
        "        )\n",
        "\n",
        "        logger.info(\"DiffDock execution completed successfully\")\n",
        "        return status_message, output_files, True\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error in DiffDock execution: {str(e)}\"\n",
        "        logger.error(error_msg)\n",
        "        logger.error(f\"Full traceback: {traceback.format_exc()}\")\n",
        "        return error_msg, None, False\n",
        "\n",
        "def diffdock_app(protein_file, ligand_smiles):\n",
        "    try:\n",
        "        if protein_file is None:\n",
        "            raise gr.Error(\"Please upload a protein structure file (PDB format).\")\n",
        "        if not ligand_smiles:\n",
        "            raise gr.Error(\"Please enter a ligand SMILES string.\")\n",
        "\n",
        "        protein_content = process_pdb_file(protein_file)\n",
        "        parameters = get_diffdock_parameters(protein_content, ligand_smiles)\n",
        "        code = generate_diffdock_code(protein_content, ligand_smiles, parameters)\n",
        "\n",
        "        return parameters, code\n",
        "\n",
        "    except gr.Error as e:\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error in diffdock_app: {str(e)}\")\n",
        "        raise gr.Error(f\"An unexpected error occurred: {str(e)}\")\n",
        "\n",
        "# Custom CSS for styling (your existing CSS)\n",
        "custom_css = \"\"\"\n",
        "<style>\n",
        "    body {\n",
        "        background-color: #1a1a1a;\n",
        "        color: #ffffff;\n",
        "    }\n",
        "    .container {\n",
        "        max-width: 1200px;\n",
        "        margin: 0 auto;\n",
        "    }\n",
        "    .gr-button {\n",
        "        background-color: #76b900 !important;\n",
        "        border: none !important;\n",
        "    }\n",
        "    .gr-button:hover {\n",
        "        background-color: #8ede00 !important;\n",
        "    }\n",
        "    .gr-form {\n",
        "        background-color: #2a2a2a;\n",
        "        border-radius: 10px;\n",
        "        padding: 20px;\n",
        "        margin-bottom: 20px;\n",
        "    }\n",
        "    .gr-box {\n",
        "        background-color: #2a2a2a;\n",
        "        border-radius: 10px;\n",
        "        padding: 20px;\n",
        "        margin-bottom: 20px;\n",
        "    }\n",
        "    .gr-paddle {\n",
        "        background-color: #2a2a2a !important;\n",
        "    }\n",
        "    h1, h2, h3 {\n",
        "        color: #76b900;\n",
        "    }\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize tracing and start the application\n",
        "try:\n",
        "    # Initialize tracing\n",
        "    tracing.initialize_tracing()\n",
        "\n",
        "    # Set up Pinecone index\n",
        "    vector_store_index = setup_pinecone_index()\n",
        "    query_engine = vector_store_index.as_query_engine()\n",
        "\n",
        "    # Initialize audio handler and chat monitor\n",
        "    audio_handler = AudioResponseHandler(openai_client)\n",
        "    chat_monitor = ChatSystemMonitor()\n",
        "\n",
        "    # Create and launch Gradio interface\n",
        "    with tracing.start_as_current_span(\"gradio_interface_setup\") as span:\n",
        "        with gr.Blocks(css=custom_css) as iface:\n",
        "            gr.HTML(\"\"\"\n",
        "                <div style=\"text-align: center; max-width: 800px; margin: 0 auto;\">\n",
        "                    <h1>🧬 Protein Design and Drug Discovery Research Assistant</h1>\n",
        "                    <p>Powered by AI and Molecular Modeling</p>\n",
        "                </div>\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Tabs() as tabs:\n",
        "                with gr.Tab(\"Home\"):\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            gr.Markdown(\"\"\"\n",
        "                            ## Welcome to the Protein Design and Drug Discovery Research Assistant\n",
        "\n",
        "                            This advanced tool combines cutting-edge AI models, molecular modeling techniques, and NVIDIA's latest technologies to streamline protein design and drug discovery workflows. The interface is organized into eight specialized tabs:\n",
        "\n",
        "                            Tab 1: NVIDIA Inference Microservices (NIMs) Home\n",
        "\n",
        "                            Overview of the application and its capabilities\n",
        "                            Introduction to NVIDIA Inference Microservices (NIMs)\n",
        "                            Interactive AI chat assistant for general guidance\n",
        "                            Quick-start guides and tips for getting started\n",
        "                            System status monitoring and voice interaction options\n",
        "\n",
        "                            Tab 2: Literature Search\n",
        "\n",
        "                            Search and analyze scientific papers from arXiv and PubMed\n",
        "                            AI-powered paper summarization with NVIDIA integration analysis\n",
        "                            Voice-enabled discussion of research findings\n",
        "                            Focus on papers relevant to protein design and drug discovery\n",
        "                            Integration with NVIDIA knowledge base for technical insights\n",
        "\n",
        "                            Tab 3: PDB Tools\n",
        "\n",
        "                            Comprehensive Protein Data Bank (PDB) file analysis\n",
        "                            Interactive 3D structure visualization\n",
        "                            Abstract lookup for quick protein information\n",
        "                            Direct PDB file download capabilities\n",
        "                            AI-assisted structure analysis with voice support\n",
        "\n",
        "                            Tab 4: RFdiffusion (Protein Design Step 1)\n",
        "\n",
        "                            Generate novel protein structures using RoseTTAFold Diffusion\n",
        "                            GPU-accelerated protein backbone generation\n",
        "                            Customizable parameters for structure generation\n",
        "                            Integration with NVIDIA's protein design pipeline\n",
        "                            Real-time visualization of generated structures\n",
        "\n",
        "                            Tab 5: ProteinMPNN (Protein Design Step 2)\n",
        "\n",
        "                            Optimize amino acid sequences for generated structures\n",
        "                            Advanced sequence prediction using neural networks\n",
        "                            GPU-accelerated sequence optimization\n",
        "                            Multiple sequence generation with confidence scores\n",
        "                            Direct integration with RFdiffusion outputs\n",
        "\n",
        "                            Tab 6: AlphaFold2 (Protein Design Step 3)\n",
        "\n",
        "                            State-of-the-art protein structure prediction\n",
        "                            GPU-optimized implementation via NVIDIA NGC\n",
        "                            Customizable prediction parameters\n",
        "                            Confidence score analysis\n",
        "                            Integration with previous design steps\n",
        "\n",
        "                            Tab 7: MolMIM\n",
        "\n",
        "                            Molecular generation and optimization using MolMIM\n",
        "                            NVIDIA-powered molecular design\n",
        "                            SMILES-based molecule generation\n",
        "                            Property optimization capabilities\n",
        "                            Integration with drug discovery workflows\n",
        "\n",
        "                            Tab 8: DiffDock\n",
        "\n",
        "                            Advanced molecular docking using diffusion models\n",
        "                            Blind docking capabilities without pocket information\n",
        "                            GPU-accelerated pose generation\n",
        "                            Integrated scoring and ranking\n",
        "                            Visualization of docking results\n",
        "\n",
        "                            Tab 9: Molecular Dynamics with GROMACS\n",
        "\n",
        "                            GPU-optimized molecular dynamics simulations\n",
        "                            Automated protocol generation\n",
        "                            NVIDIA technology integration\n",
        "                            DeepSeek-enhanced optimization\n",
        "                            Complete workflow support\n",
        "\n",
        "                            Each tab features an AI assistant with voice interaction capabilities, providing expert guidance and real-time support for your research needs. The system is fully integrated with NVIDIA's GPU acceleration technologies for optimal performance.\n",
        "                            \"\"\")\n",
        "\n",
        "                        with gr.Column():\n",
        "                            gr.Markdown(\"\"\"\n",
        "                            ## Getting Started with NVIDIA Inference Microservices (NIMs)\n",
        "\n",
        "                            To get started:\n",
        "                            1. Create a free account with NVIDIA at https://build.nvidia.com/explore/discover\n",
        "                            2. Click on your model of choice.\n",
        "                            3. Under Input select the Python tab, and click `Get API Key`\n",
        "                            4. Copy and save the generated key as NVIDIA_API_KEY\n",
        "                            \"\"\")\n",
        "\n",
        "                    gr.Markdown(\"## Nvidia NIMs Chat Assistant\")\n",
        "                    gr.Markdown(\"\"\"Need help getting started? Try asking some of these questions to the AI assistant below:\n",
        "                                - What are Nvidia inference microservices (NIMS)?\n",
        "                                - What are the benefits of Nvidia inference microservices (NIMS)?\n",
        "                                - How can Nvidia inference microservices (NIMS) be used in Protein Design?\n",
        "                                - How can Nvidia inference microservices (NIMS) be used in Drug Discovery?\"\"\")\n",
        "\n",
        "                    # Updated chat interface with monitoring\n",
        "                    with gr.Row():\n",
        "                        with gr.Column(scale=4):\n",
        "                            nims_chatbot = gr.Chatbot(\n",
        "                                height=400,\n",
        "                                bubble_full_width=False,\n",
        "                                show_label=False\n",
        "                            )\n",
        "                            with gr.Row():\n",
        "                                nims_msg = gr.Textbox(\n",
        "                                    placeholder=\"Ask about Nvidia NIMs or protein design...\",\n",
        "                                    show_label=False,\n",
        "                                    container=False,\n",
        "                                    scale=8\n",
        "                                )\n",
        "                                nims_clear = gr.ClearButton(\n",
        "                                    components=[nims_msg, nims_chatbot],\n",
        "                                    scale=1\n",
        "                                )\n",
        "\n",
        "                        with gr.Column(scale=1):\n",
        "                            system_status = gr.JSON(\n",
        "                                label=\"System Status\",\n",
        "                                value=lambda: chat_monitor.get_status()\n",
        "                            )\n",
        "                            voice_selector = gr.Dropdown(\n",
        "                                choices=[\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"],\n",
        "                                value=\"alloy\",\n",
        "                                label=\"Select Voice\",\n",
        "                                container=True\n",
        "                            )\n",
        "                            audio_output = gr.Audio(\n",
        "                                label=\"AI Response Audio\",\n",
        "                                type=\"filepath\"\n",
        "                            )\n",
        "\n",
        "                    # Chat handler with monitoring\n",
        "                    def chat_handler(message, history, state):\n",
        "                        try:\n",
        "                            with tracing.start_as_current_span(\"chat_interaction\") as chat_span:\n",
        "                                chat_span.set_attribute(\"message_length\", len(message))\n",
        "                                result = chat_with_gpt4o(message, history, state)\n",
        "                                chat_monitor.record_success()\n",
        "                                return result\n",
        "                        except Exception as e:\n",
        "                            chat_monitor.record_failure(e)\n",
        "                            logger.error(f\"Chat handler error: {str(e)}\")\n",
        "                            raise gr.Error(f\"Chat system error: {str(e)}\")\n",
        "\n",
        "                    # Voice update handler with monitoring\n",
        "                    def update_voice_handler(voice):\n",
        "                        try:\n",
        "                            audio_handler.set_voice(voice)\n",
        "                            chat_monitor.record_success()\n",
        "                            return None\n",
        "                        except Exception as e:\n",
        "                            chat_monitor.record_failure(e)\n",
        "                            logger.error(f\"Voice update error: {str(e)}\")\n",
        "                            raise gr.Error(f\"Voice update error: {str(e)}\")\n",
        "\n",
        "                    # Connect components with error handling\n",
        "                    nims_msg.submit(\n",
        "                        chat_handler,\n",
        "                        inputs=[nims_msg, nims_chatbot, gr.State([])],\n",
        "                        outputs=[nims_msg, nims_chatbot, gr.State([]), audio_output]\n",
        "                    ).then(\n",
        "                        lambda: chat_monitor.get_status(),\n",
        "                        outputs=[system_status]\n",
        "                    )\n",
        "\n",
        "                    voice_selector.change(\n",
        "                        update_voice_handler,\n",
        "                        inputs=[voice_selector],\n",
        "                        outputs=[audio_output]\n",
        "                    ).then(\n",
        "                        lambda: chat_monitor.get_status(),\n",
        "                        outputs=[system_status]\n",
        "                    )\n",
        "\n",
        "                with gr.TabItem(\"Literature Search\"):\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    ## Scientific Literature Search and Analysis\n",
        "\n",
        "                    Search and analyze scientific papers from arXiv and PubMed. The AI assistant can help you understand the papers\n",
        "                    and relate them to NVIDIA technologies and implementations.\n",
        "\n",
        "                    Features:\n",
        "                    - Search papers across multiple scientific databases\n",
        "                    - Get AI-generated summaries of papers\n",
        "                    - Voice-enabled chat assistance\n",
        "                    - Integration with NVIDIA knowledge base\n",
        "\n",
        "                    Try searching some of these that use the generative models and Nvidia technologies you can learn how to use in this application:\n",
        "                    - Improving Small Molecule Generation using Mutual Information Machine (in Arxiv)\n",
        "                    - PLINDER: The protein-ligand interactions dataset and evaluation resource (in Pubmed)\n",
        "\n",
        "\n",
        "                    \"\"\")\n",
        "\n",
        "                    # Paper Search Section\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            search_query = gr.Textbox(\n",
        "                                label=\"Search Query\",\n",
        "                                placeholder=\"Enter keywords for paper search...\",\n",
        "                                lines=2\n",
        "                            )\n",
        "                            search_source = gr.Radio(\n",
        "                                choices=[\"arxiv\", \"pubmed\"],\n",
        "                                label=\"Source Database\",\n",
        "                                value=\"arxiv\",\n",
        "                                interactive=True\n",
        "                            )\n",
        "                            search_button = gr.Button(\"🔍 Search Papers\", variant=\"primary\")\n",
        "\n",
        "                        with gr.Column():\n",
        "                            paper_summaries = gr.Textbox(\n",
        "                                label=\"Paper Summaries and Analysis\",\n",
        "                                lines=10,\n",
        "                                interactive=False\n",
        "                            )\n",
        "                            summary_audio = gr.Audio(\n",
        "                                label=\"Summary Audio\",\n",
        "                                type=\"filepath\",\n",
        "                                visible=True\n",
        "                            )\n",
        "\n",
        "                    # Chat Assistant Section\n",
        "                    gr.Markdown(\"## Literature Chat Assistant\")\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    Discuss the papers with our AI assistant. It can help you:\n",
        "                    - Understand complex concepts\n",
        "                    - Relate findings to NVIDIA technologies\n",
        "                    - Explain technical details\n",
        "                    - Compare different research approaches\n",
        "                    \"\"\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        # Chat Interface\n",
        "                        with gr.Column(scale=4):\n",
        "                            literature_chatbot = gr.Chatbot(\n",
        "                                height=400,\n",
        "                                bubble_full_width=False,\n",
        "                                show_label=False,\n",
        "                                container=False\n",
        "                            )\n",
        "                            with gr.Row():\n",
        "                                literature_msg = gr.Textbox(\n",
        "                                    placeholder=\"Ask about the papers or related NVIDIA technologies...\",\n",
        "                                    show_label=False,\n",
        "                                    container=False,\n",
        "                                    scale=9\n",
        "                                )\n",
        "                                literature_clear = gr.ClearButton(\n",
        "                                    components=[literature_msg, literature_chatbot],\n",
        "                                    scale=1,\n",
        "                                    variant=\"secondary\"\n",
        "                                )\n",
        "\n",
        "                        # Voice and Audio Controls\n",
        "                        with gr.Column(scale=1):\n",
        "                            voice_selector = gr.Dropdown(\n",
        "                                choices=[\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"],\n",
        "                                value=\"alloy\",\n",
        "                                label=\"Select Voice\",\n",
        "                                info=\"Choose the AI assistant's voice\",\n",
        "                                container=True\n",
        "                            )\n",
        "                            audio_output = gr.Audio(\n",
        "                                label=\"AI Response Audio\",\n",
        "                                type=\"filepath\",\n",
        "                                show_label=True,\n",
        "                                container=True\n",
        "                            )\n",
        "\n",
        "                        def literature_chat(message, chat_history, history_state, paper_summaries):\n",
        "                            \"\"\"\n",
        "                            Enhanced literature chat assistant with proper flow control and error handling\n",
        "                            \"\"\"\n",
        "                            try:\n",
        "                                if history_state is None:\n",
        "                                    history_state = []\n",
        "\n",
        "                                logger.info(\"Starting literature chat interaction\")\n",
        "\n",
        "                                # Phase 1: Query Preparation\n",
        "                                try:\n",
        "                                    logger.info(\"Preparing query and context\")\n",
        "                                    # Create a synchronous query to prevent stalling\n",
        "                                    query_result = query_engine.query(\n",
        "                                        message,\n",
        "                                        response_mode=\"compact\",  # Use compact mode for faster processing\n",
        "                                        timeout=30  # Add timeout to prevent hanging\n",
        "                                    )\n",
        "                                    relevant_context = str(query_result.response)\n",
        "                                    logger.info(\"Context retrieved successfully\")\n",
        "                                except Exception as e:\n",
        "                                    logger.error(f\"Error in query phase: {str(e)}\")\n",
        "                                    relevant_context = \"Unable to retrieve context. Proceeding with basic response.\"\n",
        "\n",
        "                                # Phase 2: Chat Completion\n",
        "                                try:\n",
        "                                    logger.info(\"Initiating chat completion\")\n",
        "                                    system_message = {\n",
        "                                        \"role\": \"system\",\n",
        "                                        \"content\": f\"\"\"You are an AI assistant specializing in scientific literature analysis and NVIDIA technologies.\n",
        "                                        Use this information to inform your response:\n",
        "\n",
        "                                        Paper Summaries: {paper_summaries}\n",
        "                                        Relevant Context: {relevant_context}\n",
        "\n",
        "                                        Focus on:\n",
        "                                        1. Explaining scientific concepts clearly\n",
        "                                        2. Relating findings to NVIDIA technologies\n",
        "                                        3. Providing practical insights for implementation\n",
        "                                        4. Identifying connections between research papers\n",
        "\n",
        "                                        If you don't have enough information to answer a specific question,\n",
        "                                        please say so and offer to help with other aspects of the literature or NVIDIA's capabilities.\"\"\"\n",
        "                                    }\n",
        "\n",
        "                                    messages = [system_message] + history_state + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "                                    response = openai_client.chat.completions.create(\n",
        "                                        model=\"gpt-4o\",\n",
        "                                        messages=messages,\n",
        "                                        max_tokens=16384,\n",
        "                                        temperature=0,\n",
        "                                        stream=False,\n",
        "                                        timeout=60  # Add timeout for chat completion\n",
        "                                    )\n",
        "\n",
        "                                    response_text = response.choices[0].message.content\n",
        "                                    logger.info(\"Chat completion successful\")\n",
        "                                except Exception as e:\n",
        "                                    logger.error(f\"Error in chat completion: {str(e)}\")\n",
        "                                    return message, chat_history, history_state, None\n",
        "\n",
        "                                # Phase 3: Audio Generation\n",
        "                                try:\n",
        "                                    logger.info(\"Starting audio generation\")\n",
        "                                    audio_handler = AudioResponseHandler(openai_client)\n",
        "                                    audio_file_path = audio_handler.text_to_speech(response_text)\n",
        "\n",
        "                                    if not audio_file_path:\n",
        "                                        logger.warning(\"Audio generation returned no file path\")\n",
        "                                        raise ValueError(\"Audio generation failed\")\n",
        "\n",
        "                                    logger.info(f\"Audio generated successfully: {audio_file_path}\")\n",
        "                                except Exception as e:\n",
        "                                    logger.error(f\"Error in audio generation: {str(e)}\")\n",
        "                                    audio_file_path = None\n",
        "                                    response_text += \"\\n\\n(Note: Audio response generation failed. Text response is still available.)\"\n",
        "\n",
        "                                # Update conversation state\n",
        "                                chat_history = chat_history + [(message, response_text)]\n",
        "                                history_state.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "\n",
        "                                logger.info(\"Literature chat interaction completed successfully\")\n",
        "                                return message, chat_history, history_state, audio_file_path\n",
        "\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Critical error in literature chat system: {str(e)}\")\n",
        "                                error_message = \"I apologize, but I encountered an unexpected error. Please try again.\"\n",
        "                                chat_history = chat_history + [(message, error_message)]\n",
        "                                return message, chat_history, history_state, None\n",
        "\n",
        "                        # Update the chat handler with monitoring\n",
        "                        def literature_chat_handler(message, history, state, paper_summaries):\n",
        "                            try:\n",
        "                                with tracing.start_as_current_span(\"literature_chat_interaction\") as chat_span:\n",
        "                                    chat_span.set_attribute(\"message_length\", len(message))\n",
        "                                    result = literature_chat(message, history, state, paper_summaries)\n",
        "                                    chat_monitor.record_success()\n",
        "                                    return result\n",
        "                            except Exception as e:\n",
        "                                chat_monitor.record_failure(e)\n",
        "                                logger.error(f\"Literature chat handler error: {str(e)}\")\n",
        "                                raise gr.Error(f\"Chat system error: {str(e)}\")\n",
        "\n",
        "                    # Voice update handler with monitoring\n",
        "                    def update_literature_voice(voice):\n",
        "                        try:\n",
        "                            audio_handler = AudioResponseHandler(openai_client)\n",
        "                            audio_handler.set_voice(voice)\n",
        "                            chat_monitor.record_success()\n",
        "                            return None\n",
        "                        except Exception as e:\n",
        "                            chat_monitor.record_failure(e)\n",
        "                            logger.error(f\"Voice update error: {str(e)}\")\n",
        "                            raise gr.Error(f\"Voice update error: {str(e)}\")\n",
        "\n",
        "                    # Connect the chat interface\n",
        "                    literature_msg.submit(\n",
        "                        literature_chat,\n",
        "                        inputs=[\n",
        "                            literature_msg,\n",
        "                            literature_chatbot,\n",
        "                            gr.State([]),\n",
        "                            paper_summaries\n",
        "                        ],\n",
        "                        outputs=[\n",
        "                            literature_msg,\n",
        "                            literature_chatbot,\n",
        "                            gr.State([]),\n",
        "                            audio_output\n",
        "                        ]\n",
        "                    )\n",
        "\n",
        "                    # Connect the components with proper error handling\n",
        "                    search_button.click(\n",
        "                        fn=search_and_summarize_papers,\n",
        "                        inputs=[search_query, search_source],\n",
        "                        outputs=[paper_summaries, summary_audio],\n",
        "                        api_name=\"search_papers\"\n",
        "                    )\n",
        "\n",
        "                    # Connect voice selection\n",
        "                    voice_selector.change(\n",
        "                        update_voice_handler,\n",
        "                        inputs=[voice_selector],\n",
        "                        outputs=[audio_output]\n",
        "                    )\n",
        "\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    ---\n",
        "                    ### Tips for Effective Use\n",
        "                    - Use specific keywords in your search\n",
        "                    - Include relevant technical terms\n",
        "                    - Ask follow-up questions for clarification\n",
        "                    - Use the voice feature for hands-free interaction\n",
        "                    \"\"\")\n",
        "\n",
        "                with gr.TabItem(\"Protein Databank (PDB) Tools\"):\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    ## PDB Tools: Structure Analysis and Visualization\n",
        "\n",
        "                    ### Overview\n",
        "                    The PDB Tools module provides comprehensive tools for working with Protein Data Bank (PDB) files, including:\n",
        "                    - Abstract lookup for quick protein information\n",
        "                    - PDB file download capabilities\n",
        "                    - Interactive 3D structure visualization\n",
        "                    - AI-powered structure analysis and assistance\n",
        "\n",
        "                    ### Features\n",
        "                    - **Abstract Lookup**: Quickly retrieve scientific abstracts for any PDB ID\n",
        "                    - **File Download**: Direct download of PDB files for local analysis\n",
        "                    - **3D Visualization**: Interactive molecular visualization with customizable representations\n",
        "                    - **Voice-Enabled AI Assistant**: Get expert help understanding protein structures\n",
        "                    - **NVIDIA Integration**: Access specialized analysis tools and GPU-accelerated features\n",
        "\n",
        "                    ### How to Use\n",
        "                    1. **For Abstract Lookup**:\n",
        "                      - Enter a valid PDB ID (e.g., \"1ak4\")\n",
        "                      - Click \"Lookup PDB Abstract\"\n",
        "\n",
        "                    2. **For File Download**:\n",
        "                      - Enter a PDB ID\n",
        "                      - Click \"Download PDB File\"\n",
        "\n",
        "                    3. **For Structure Visualization**:\n",
        "                      - Upload a PDB file\n",
        "                      - Use the 3D viewer controls to explore the structure\n",
        "\n",
        "                    4. **For AI Assistance**:\n",
        "                      - Ask questions about your structure or PDB tools\n",
        "                      - Use voice interaction for hands-free operation\n",
        "                    \"\"\")\n",
        "\n",
        "                    # Chat interface with voice components\n",
        "                    with gr.Row():\n",
        "                        with gr.Column(scale=4):\n",
        "                            pdb_chatbot = gr.Chatbot(\n",
        "                                height=400,\n",
        "                                bubble_full_width=False,\n",
        "                                show_label=False\n",
        "                            )\n",
        "                            pdb_msg = gr.Textbox(\n",
        "                                placeholder=\"Ask about PDB structures or analysis tools...\",\n",
        "                                show_label=False,\n",
        "                                container=False\n",
        "                            )\n",
        "                            pdb_clear = gr.ClearButton([pdb_msg, pdb_chatbot])\n",
        "\n",
        "                        with gr.Column(scale=1):\n",
        "                            pdb_voice_selector = gr.Dropdown(\n",
        "                                choices=[\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"],\n",
        "                                value=\"alloy\",\n",
        "                                label=\"Select Voice\",\n",
        "                                container=True\n",
        "                            )\n",
        "                            pdb_audio_output = gr.Audio(\n",
        "                                label=\"AI Response Audio\",\n",
        "                                type=\"filepath\"\n",
        "                            )\n",
        "\n",
        "                    gr.Markdown(\"## PDB Abstract Lookup\")\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            pdb_id_input = gr.Textbox(label=\"PDB ID\", placeholder=\"Enter PDB ID...\")\n",
        "                            pdb_lookup_button = gr.Button(\"🔬 Lookup PDB Abstract\", variant=\"primary\")\n",
        "                        with gr.Column():\n",
        "                            pdb_abstract = gr.Textbox(label=\"PDB Abstract\", lines=5)\n",
        "\n",
        "                    gr.Markdown(\"## Download PDB File\")\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            pdb_download_id = gr.Textbox(label=\"PDB ID\", placeholder=\"Enter PDB ID...\")\n",
        "                            pdb_download_button = gr.Button(\"📥 Download PDB File\", variant=\"primary\")\n",
        "                        with gr.Column():\n",
        "                            pdb_download_result = gr.Textbox(label=\"Download Result\")\n",
        "\n",
        "                    gr.Markdown(\"## Visualize PDB Files\")\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            pdb_file_input = gr.File(label=\"Upload PDB File\")\n",
        "                        with gr.Column():\n",
        "                            reps = [\n",
        "                                {\n",
        "                                    \"model\": 0,\n",
        "                                    \"chain\": \"\",\n",
        "                                    \"resname\": \"\",\n",
        "                                    \"style\": \"cartoon\",\n",
        "                                    \"color\": \"spectrum\",\n",
        "                                    \"residue_range\": \"\",\n",
        "                                    \"around\": 0,\n",
        "                                    \"byres\": False,\n",
        "                                    \"visible\": True\n",
        "                                },\n",
        "                                {\n",
        "                                    \"model\": 0,\n",
        "                                    \"chain\": \"\",\n",
        "                                    \"resname\": \"\",\n",
        "                                    \"style\": \"stick\",\n",
        "                                    \"color\": \"element\",\n",
        "                                    \"residue_range\": \"\",\n",
        "                                    \"around\": 0,\n",
        "                                    \"byres\": False,\n",
        "                                    \"visible\": False\n",
        "                                }\n",
        "                            ]\n",
        "                            molecule_3d = Molecule3D(label=\"PDB Visualization\", reps=reps)\n",
        "\n",
        "                    def pdb_chat_with_voice(message, chat_history, history_state, abstract=None, pdb_file=None):\n",
        "                        \"\"\"\n",
        "                        Enhanced PDB chat assistant with proper flow control and error handling\n",
        "                        \"\"\"\n",
        "                        try:\n",
        "                            if history_state is None:\n",
        "                                history_state = []\n",
        "\n",
        "                            logger.info(\"Starting PDB chat interaction\")\n",
        "\n",
        "                            # Phase 1: Query Preparation\n",
        "                            try:\n",
        "                                logger.info(\"Preparing query and context\")\n",
        "                                # Create a synchronous query to prevent stalling\n",
        "                                query_result = query_engine.query(\n",
        "                                    message,\n",
        "                                    response_mode=\"compact\",  # Use compact mode for faster processing\n",
        "                                    timeout=30  # Add timeout to prevent hanging\n",
        "                                )\n",
        "                                relevant_info = str(query_result.response)\n",
        "                                logger.info(\"Context retrieved successfully\")\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Error in query phase: {str(e)}\")\n",
        "                                relevant_info = \"Unable to retrieve context. Proceeding with basic response.\"\n",
        "\n",
        "                            # Phase 2: Create Context\n",
        "                            context = (\n",
        "                                f\"PDB Abstract: {abstract if abstract else 'No abstract loaded'}\\n\"\n",
        "                                f\"PDB File: {'Uploaded' if pdb_file else 'No file uploaded'}\\n\"\n",
        "                                f\"Relevant Information: {relevant_info}\"\n",
        "                            )\n",
        "\n",
        "                            # Phase 3: Chat Completion\n",
        "                            try:\n",
        "                                logger.info(\"Initiating chat completion\")\n",
        "                                system_message = {\n",
        "                                    \"role\": \"system\",\n",
        "                                    \"content\": f\"\"\"You are an AI assistant specializing in protein structure analysis and PDB tools.\n",
        "                                    Use the following information to help answer the user's question:\n",
        "\n",
        "                                    {context}\n",
        "\n",
        "                                    Focus on:\n",
        "                                    1. Analyzing protein structures and features\n",
        "                                    2. Explaining PDB file formats and tools\n",
        "                                    3. Providing practical insights for structure analysis\n",
        "                                    4. Relating findings to protein design and function\n",
        "\n",
        "                                    If you don't have enough information to answer a specific question,\n",
        "                                    please say so and offer to help with other aspects of protein structure analysis.\"\"\"\n",
        "                                }\n",
        "\n",
        "                                messages = [system_message] + history_state + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "                                response = openai_client.chat.completions.create(\n",
        "                                    model=\"gpt-4o\",\n",
        "                                    messages=messages,\n",
        "                                    max_tokens=16384,\n",
        "                                    temperature=0,\n",
        "                                    stream=False,\n",
        "                                    timeout=60  # Add timeout for chat completion\n",
        "                                )\n",
        "\n",
        "                                response_text = response.choices[0].message.content\n",
        "                                logger.info(\"Chat completion successful\")\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Error in chat completion: {str(e)}\")\n",
        "                                return message, chat_history, history_state, None\n",
        "\n",
        "                            # Phase 4: Audio Generation\n",
        "                            try:\n",
        "                                logger.info(\"Starting audio generation\")\n",
        "                                audio_handler = AudioResponseHandler(openai_client)\n",
        "                                audio_file_path = audio_handler.text_to_speech(response_text)\n",
        "\n",
        "                                if not audio_file_path:\n",
        "                                    logger.warning(\"Audio generation returned no file path\")\n",
        "                                    raise ValueError(\"Audio generation failed\")\n",
        "\n",
        "                                logger.info(f\"Audio generated successfully: {audio_file_path}\")\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Error in audio generation: {str(e)}\")\n",
        "                                audio_file_path = None\n",
        "                                response_text += \"\\n\\n(Note: Audio response generation failed. Text response is still available.)\"\n",
        "\n",
        "                            # Update conversation state\n",
        "                            chat_history = chat_history + [(message, response_text)]\n",
        "                            history_state.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "\n",
        "                            logger.info(\"PDB chat interaction completed successfully\")\n",
        "                            return message, chat_history, history_state, audio_file_path\n",
        "\n",
        "                        except Exception as e:\n",
        "                            logger.error(f\"Critical error in PDB chat system: {str(e)}\")\n",
        "                            error_message = \"I apologize, but I encountered an unexpected error. Please try again.\"\n",
        "                            chat_history = chat_history + [(message, error_message)]\n",
        "                            return message, chat_history, history_state, None\n",
        "\n",
        "                    # Update the chat handler with monitoring\n",
        "                    def pdb_chat_handler(message, chat_history, state, abstract, pdb_file):\n",
        "                        try:\n",
        "                            with tracing.start_as_current_span(\"pdb_chat_interaction\") as chat_span:\n",
        "                                chat_span.set_attribute(\"message_length\", len(message))\n",
        "                                result = pdb_chat_with_voice(message, chat_history, state, abstract, pdb_file)\n",
        "                                chat_monitor.record_success()\n",
        "                                return result\n",
        "                        except Exception as e:\n",
        "                            chat_monitor.record_failure(e)\n",
        "                            logger.error(f\"PDB chat handler error: {str(e)}\")\n",
        "                            raise gr.Error(f\"Chat system error: {str(e)}\")\n",
        "\n",
        "                    # Voice selection handler with monitoring\n",
        "                    def update_pdb_voice_handler(voice):\n",
        "                        try:\n",
        "                            audio_handler = AudioResponseHandler(openai_client)\n",
        "                            audio_handler.set_voice(voice)\n",
        "                            chat_monitor.record_success()\n",
        "                            return None\n",
        "                        except Exception as e:\n",
        "                            chat_monitor.record_failure(e)\n",
        "                            logger.error(f\"Voice update error: {str(e)}\")\n",
        "                            raise gr.Error(f\"Voice update error: {str(e)}\")\n",
        "\n",
        "                    # Connect the chat function\n",
        "                    pdb_msg.submit(\n",
        "                        pdb_chat_handler,\n",
        "                        inputs=[\n",
        "                            pdb_msg,\n",
        "                            pdb_chatbot,\n",
        "                            gr.State([]),\n",
        "                            pdb_abstract,\n",
        "                            pdb_file_input\n",
        "                        ],\n",
        "                        outputs=[\n",
        "                            pdb_msg,\n",
        "                            pdb_chatbot,\n",
        "                            gr.State([]),\n",
        "                            pdb_audio_output\n",
        "                        ]\n",
        "                    )\n",
        "\n",
        "                    # Connect voice selection\n",
        "                    pdb_voice_selector.change(\n",
        "                        update_pdb_voice_handler,\n",
        "                        inputs=[pdb_voice_selector],\n",
        "                        outputs=[pdb_audio_output]\n",
        "                    )\n",
        "\n",
        "                    # Connect existing PDB tools\n",
        "                    pdb_lookup_button.click(fetch_pdb_abstract, inputs=[pdb_id_input], outputs=pdb_abstract)\n",
        "                    pdb_download_button.click(download_pdb, inputs=[pdb_download_id], outputs=pdb_download_result)\n",
        "                    pdb_file_input.change(visualize_pdb, inputs=[pdb_file_input], outputs=[molecule_3d])\n",
        "\n",
        "                with gr.TabItem(\"RFdiffusion- Protein Design Step 1\"):\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    ## RFdiffusion: Generate New Protein Structures https://build.nvidia.com/ipd/rfdiffusion\n",
        "\n",
        "                    ### Model Overview\n",
        "                    RFdiffusion (RoseTTAFold Diffusion) is a generative model that creates novel protein structures for scaffolding and binder design tasks. It generates new protein backbones and designs proteins tailored to bind to target molecules.\n",
        "\n",
        "                    ### Key Parameters\n",
        "                    - **input_pdb**: Input PDB file containing protein chains and amino acids for binder target and motifs.\n",
        "                    - **contigs**: Defines the protein to be generated. Example: 'A10-100/0 50-150' keeps amino acids 10-100 in Chain A, then constructs a new chain of 50-150 amino acids.\n",
        "                    - **hotspot_res**: Specifies which region the new protein must contact with the original protein. Format: 'A50,A51,A52' for Chain A, positions 50, 51, 52.\n",
        "                    - **diffusion_steps**: Number of denoising steps (min: 15, default: 50). More steps can improve quality but increase runtime.\n",
        "                    - **random_seed**: Optional. Sets a fixed seed for reproducible results.\n",
        "\n",
        "                    ### How to Use\n",
        "                    1. Upload your PDB file. Because this workflow is for demonstration purposes, it works best for simple, single-chain PDB files. Try these examples to see how RFdiffusion works:\n",
        "                       - 1AKI Lysozyme https://www.rcsb.org/structure/1aki\n",
        "                       - 1GTS Glutaminyl-tRNA synthetase https://www.rcsb.org/structure/1GTS\n",
        "                    2. Click 'Generate RFdiffusion Script' to create a custom python script to run RFdiffusion via the NIM API. An audio file will also be automatically generated to explain the RFdiffusion parameters selected for your protein design run.\n",
        "                    3. Follow this link to the NIM API catalog to generate a RFdiffusion API key under the Python tab: https://build.nvidia.com/ipd/rfdiffusion?snippet_tab=Python\n",
        "                    4. Enter you API key in the 'Execute RFdiffusion' interface and click 'Run RFdiffusion' to make the API call. A new protein structure will be generated and appear in the molecular visualization window below.\n",
        "                    5. A PDB file for your new structure will be automatically downloaded to your file system. Use this PDB file in the ProteinMPNN tab to create new amino acid sequences to fit your new protein structure.\n",
        "                    \"\"\")\n",
        "\n",
        "                    # Update the chat interface to include voice components\n",
        "                    with gr.Row():\n",
        "                        with gr.Column(scale=4):\n",
        "                            rf_chatbot = gr.Chatbot(\n",
        "                                height=400,\n",
        "                                bubble_full_width=False,\n",
        "                                show_label=False\n",
        "                            )\n",
        "                            rf_msg = gr.Textbox(\n",
        "                                placeholder=\"Ask about RFdiffusion or the uploaded PDB file...\",\n",
        "                                show_label=False,\n",
        "                                container=False\n",
        "                            )\n",
        "                            rf_clear = gr.ClearButton([rf_msg, rf_chatbot])\n",
        "\n",
        "                        with gr.Column(scale=1):\n",
        "                            rf_voice_selector = gr.Dropdown(\n",
        "                                choices=[\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"],\n",
        "                                value=\"alloy\",\n",
        "                                label=\"Select Voice\",\n",
        "                                container=True\n",
        "                            )\n",
        "                            rf_audio_output = gr.Audio(\n",
        "                                label=\"AI Response Audio\",\n",
        "                                type=\"filepath\"\n",
        "                            )\n",
        "\n",
        "                    def rf_chat_with_voice(message, chat_history, history_state, parameters, code):\n",
        "                        \"\"\"\n",
        "                        Enhanced RFdiffusion chat assistant with proper flow control and error handling\n",
        "                        \"\"\"\n",
        "                        try:\n",
        "                            if history_state is None:\n",
        "                                history_state = []\n",
        "\n",
        "                            logger.info(\"Starting RFdiffusion chat interaction\")\n",
        "\n",
        "                            # Phase 1: Query Preparation\n",
        "                            try:\n",
        "                                logger.info(\"Preparing query and context\")\n",
        "                                query_result = query_engine.query(\n",
        "                                    message,\n",
        "                                    response_mode=\"compact\",\n",
        "                                    timeout=30\n",
        "                                )\n",
        "                                relevant_info = str(query_result.response)\n",
        "                                logger.info(\"Context retrieved successfully\")\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Error in query phase: {str(e)}\")\n",
        "                                relevant_info = \"Unable to retrieve context. Proceeding with basic response.\"\n",
        "\n",
        "                            # Phase 2: Chat Completion\n",
        "                            try:\n",
        "                                logger.info(\"Initiating chat completion\")\n",
        "                                system_message = {\n",
        "                                    \"role\": \"system\",\n",
        "                                    \"content\": f\"\"\"You are an AI assistant specializing in RFdiffusion and protein structure generation.\n",
        "                                    Use this relevant information to inform your response: {relevant_info}\n",
        "\n",
        "                                    Current Context:\n",
        "                                    Parameters: {parameters}\n",
        "                                    Code: {code}\n",
        "\n",
        "                                    Help the user understand:\n",
        "                                    1. RFdiffusion capabilities and limitations\n",
        "                                    2. Parameter optimization and settings\n",
        "                                    3. Integration with NVIDIA GPU acceleration\n",
        "                                    4. Best practices for protein structure generation\n",
        "\n",
        "                                    If you don't have enough information to answer a specific question,\n",
        "                                    please say so and offer to help with other aspects of RFdiffusion or protein design.\"\"\"\n",
        "                                }\n",
        "\n",
        "                                messages = [system_message] + history_state + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "                                response = openai_client.chat.completions.create(\n",
        "                                    model=\"gpt-4o\",\n",
        "                                    messages=messages,\n",
        "                                    max_tokens=16384,\n",
        "                                    temperature=0,\n",
        "                                    stream=False,\n",
        "                                    timeout=60\n",
        "                                )\n",
        "\n",
        "                                response_text = response.choices[0].message.content\n",
        "                                logger.info(\"Chat completion successful\")\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Error in chat completion: {str(e)}\")\n",
        "                                return message, chat_history, history_state, None\n",
        "\n",
        "                            # Phase 3: Audio Generation\n",
        "                            try:\n",
        "                                logger.info(\"Starting audio generation\")\n",
        "                                audio_handler = AudioResponseHandler(openai_client)\n",
        "                                audio_file_path = audio_handler.text_to_speech(response_text)\n",
        "\n",
        "                                if not audio_file_path:\n",
        "                                    logger.warning(\"Audio generation returned no file path\")\n",
        "                                    raise ValueError(\"Audio generation failed\")\n",
        "\n",
        "                                logger.info(f\"Audio generated successfully: {audio_file_path}\")\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Error in audio generation: {str(e)}\")\n",
        "                                audio_file_path = None\n",
        "                                response_text += \"\\n\\n(Note: Audio response generation failed. Text response is still available.)\"\n",
        "\n",
        "                            # Update conversation state\n",
        "                            chat_history = chat_history + [(message, response_text)]\n",
        "                            history_state.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "\n",
        "                            logger.info(\"RFdiffusion chat interaction completed successfully\")\n",
        "                            return message, chat_history, history_state, audio_file_path\n",
        "\n",
        "                        except Exception as e:\n",
        "                            logger.error(f\"Critical error in RFdiffusion chat system: {str(e)}\")\n",
        "                            error_message = \"I apologize, but I encountered an unexpected error. Please try again.\"\n",
        "                            chat_history = chat_history + [(message, error_message)]\n",
        "                            return message, chat_history, history_state, None\n",
        "\n",
        "                    # Handler function with monitoring\n",
        "                    def rf_chat_handler(message, chat_history, state, parameters, code):\n",
        "                        try:\n",
        "                            with tracing.start_as_current_span(\"rf_chat_interaction\") as chat_span:\n",
        "                                chat_span.set_attribute(\"message_length\", len(message))\n",
        "                                result = rf_chat_with_voice(message, chat_history, state, parameters, code)\n",
        "                                chat_monitor.record_success()\n",
        "                                return result\n",
        "                        except Exception as e:\n",
        "                            chat_monitor.record_failure(e)\n",
        "                            logger.error(f\"RFdiffusion chat handler error: {str(e)}\")\n",
        "                            raise gr.Error(f\"Chat system error: {str(e)}\")\n",
        "\n",
        "                    # Voice update handler with monitoring\n",
        "                    def update_rf_voice_handler(voice):\n",
        "                        try:\n",
        "                            audio_handler = AudioResponseHandler(openai_client)\n",
        "                            audio_handler.set_voice(voice)\n",
        "                            chat_monitor.record_success()\n",
        "                            return None\n",
        "                        except Exception as e:\n",
        "                            chat_monitor.record_failure(e)\n",
        "                            logger.error(f\"Voice update error: {str(e)}\")\n",
        "                            raise gr.Error(f\"Voice update error: {str(e)}\")\n",
        "\n",
        "                    # Add voice components for RFdiffusion parameter analysis\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            rf_file_input = gr.File(label=\"Upload PDB File\")\n",
        "                            rf_generate_button = gr.Button(\"🧬 Generate RFdiffusion Script\", variant=\"primary\")\n",
        "                        with gr.Column():\n",
        "                            rf_parameters_output = gr.Textbox(label=\"RFdiffusion Parameters (AI Analysis)\", lines=10)\n",
        "                            rf_code_output = gr.Code(language=\"python\", label=\"RFdiffusion Python Code\")\n",
        "                            rf_explanation_output = gr.Textbox(\n",
        "                                label=\"Parameter Explanation\",\n",
        "                                lines=5,\n",
        "                                interactive=False\n",
        "                            )\n",
        "                            rf_audio_output = gr.Audio(\n",
        "                                label=\"Listen to Parameter Analysis\",\n",
        "                                type=\"filepath\"\n",
        "                            )\n",
        "\n",
        "                    gr.Markdown(\"## Execute RFdiffusion\")\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            api_key_input = gr.Textbox(\n",
        "                                label=\"NVIDIA API Key\",\n",
        "                                placeholder=\"Enter your NVIDIA API key...\",\n",
        "                                type=\"password\"\n",
        "                            )\n",
        "                            execute_button = gr.Button(\"🚀 Run RFdiffusion\", variant=\"primary\")\n",
        "                        with gr.Column():\n",
        "                            execution_output = gr.Textbox(\n",
        "                                label=\"Execution Results\",\n",
        "                                lines=10,\n",
        "                                interactive=False\n",
        "                            )\n",
        "\n",
        "                    # Structure visualization section\n",
        "                    gr.Markdown(\"## Generated Structure\")\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            output_file = gr.File(\n",
        "                                label=\"Download Generated Structure\",\n",
        "                                visible=True,\n",
        "                                interactive=False\n",
        "                            )\n",
        "                        with gr.Column():\n",
        "                            molecule_viewer = Molecule3D(\n",
        "                                label=\"Structure Visualization\",\n",
        "                                reps=[{\n",
        "                                    \"model\": 0,\n",
        "                                    \"chain\": \"\",\n",
        "                                    \"style\": \"cartoon\",\n",
        "                                    \"color\": \"spectrum\",\n",
        "                                }]\n",
        "                            )\n",
        "\n",
        "                    def run_rfdiffusion(api_key, file, parameters):\n",
        "                        \"\"\"\n",
        "                        Executes RFdiffusion with proper parameter handling\n",
        "\n",
        "                        Args:\n",
        "                            api_key (str): NVIDIA API key\n",
        "                            file: Uploaded PDB file\n",
        "                            parameters (str): AI-analyzed parameters from get_rfdiffusion_parameters\n",
        "                        \"\"\"\n",
        "                        try:\n",
        "                            if file is None:\n",
        "                                return \"Please upload a PDB file first.\", None, None\n",
        "\n",
        "                            if not parameters:\n",
        "                                return \"Please generate parameters first by clicking 'Generate RFdiffusion Script'.\", None, None\n",
        "\n",
        "                            pdb_content = process_pdb_file(file)\n",
        "                            if not pdb_content:\n",
        "                                return \"Error: Could not read PDB file.\", None, None\n",
        "\n",
        "                            # Execute RFdiffusion with parameters\n",
        "                            status, output_pdb, success = execute_rfdiffusion_script(api_key, pdb_content, parameters)\n",
        "\n",
        "                            if not success:\n",
        "                                return status, None, None\n",
        "\n",
        "                            # Create temporary file for visualization and download\n",
        "                            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pdb', mode='w')\n",
        "                            temp_file.write(output_pdb)\n",
        "                            temp_file.close()\n",
        "\n",
        "                            return status, temp_file.name, temp_file.name\n",
        "\n",
        "                        except Exception as e:\n",
        "                            logger.error(f\"Error in RFdiffusion execution: {str(e)}\")\n",
        "                            return f\"Error: {str(e)}\", None, None\n",
        "\n",
        "                    # Update the message submission to use the new handler\n",
        "                    rf_msg.submit(\n",
        "                        rf_chat_handler,\n",
        "                        inputs=[\n",
        "                            rf_msg,\n",
        "                            rf_chatbot,\n",
        "                            gr.State([]),\n",
        "                            rf_parameters_output,\n",
        "                            rf_code_output\n",
        "                        ],\n",
        "                        outputs=[\n",
        "                            rf_msg,\n",
        "                            rf_chatbot,\n",
        "                            gr.State([]),\n",
        "                            rf_audio_output\n",
        "                        ]\n",
        "                    )\n",
        "\n",
        "                    # Update voice selection to use the new handler\n",
        "                    rf_voice_selector.change(\n",
        "                        update_rf_voice_handler,\n",
        "                        inputs=[rf_voice_selector],\n",
        "                        outputs=[rf_audio_output]\n",
        "                    )\n",
        "\n",
        "                    # Connect the execute button with the updated run_rfdiffusion function\n",
        "                    execute_button.click(\n",
        "                        fn=run_rfdiffusion,\n",
        "                        inputs=[\n",
        "                            api_key_input,\n",
        "                            rf_file_input,\n",
        "                            rf_parameters_output  # Add parameters from the analysis\n",
        "                        ],\n",
        "                        outputs=[\n",
        "                            execution_output,\n",
        "                            output_file,\n",
        "                            molecule_viewer\n",
        "                        ]\n",
        "                    )\n",
        "\n",
        "                    # Update the generation button click event\n",
        "                    rf_generate_button.click(\n",
        "                        rfdiffusion_app,\n",
        "                        inputs=[rf_file_input],\n",
        "                        outputs=[\n",
        "                            rf_parameters_output,\n",
        "                            rf_code_output,\n",
        "                            rf_explanation_output,\n",
        "                            rf_audio_output\n",
        "                        ]\n",
        "                    )\n",
        "\n",
        "                with gr.TabItem(\"ProteinMPNN- Protein Design Step 2\"):\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    ## ProteinMPNN: Optimize Amino Acid Sequences https://build.nvidia.com/ipd/proteinmpnn\n",
        "\n",
        "                    ### Model Overview\n",
        "                    ProteinMPNN (Protein Message Passing Neural Network) is a deep learning model that predicts amino acid sequences for given protein backbones. It uses evolutionary, functional, and structural information to generate sequences likely to fold into desired 3D structures.\n",
        "\n",
        "                    ### Key Parameters\n",
        "                    - **input_pdb**: Input protein structure in PDB format.\n",
        "                    - **input_pdb_chains**: Specify chains to design (default: all chains).\n",
        "                    - **ca_only**: Use CA-only model, focusing on alpha carbon atoms (default: false).\n",
        "                    - **use_soluble_model**: Choose between soluble and non-soluble models (default: false).\n",
        "                    - **num_seq_per_target**: Number of sequences to generate per target (default: 1).\n",
        "                    - **sampling_temp**: Controls design diversity (range: 0.1 to 0.3, higher = more diverse).\n",
        "                    - **random_seed**: Set for reproducibility, omit for diverse results.\n",
        "\n",
        "                    ### Advanced Options\n",
        "                    - **pssm_jsonl**: Incorporate evolutionary information to guide mutations.\n",
        "                    - **fixed_positions_jsonl**: Specify residues to remain unchanged.\n",
        "                    - **omit_AAs**: Exclude specific amino acids from the design.\n",
        "                    - **bias_AA_jsonl**: Fine-tune amino acid composition with a bias dictionary.\n",
        "                    - **tied_positions_jsonl**: Ensure specific residues are identical across positions/chains.\n",
        "\n",
        "                    ### How to Use\n",
        "                    1. Upload your PDB file created with RFdiffusion.\n",
        "                    2. Follow this link to the NIM API catalog to generate a ProteinMPNN API key under the Python tab: https://build.nvidia.com/ipd/proteinmpnn?snippet_tab=Python\n",
        "                    3. Input your API key and click 'Generate and Run ProteinMPNN Script' to create a custom script for your sequence optimization task and make the API call. A new amino acid sequence in FASTA format will be generated and appear in the execution results window.\n",
        "                    4. Copy and paste the amino acid sequence in to the AlphaFold2 interface or the AlphaFold3 server to create your new protein structure.\n",
        "                    \"\"\")\n",
        "\n",
        "                    # Add voice components for ProteinMPNN tab\n",
        "                    with gr.Row():\n",
        "                        with gr.Column(scale=4):\n",
        "                            pmpnn_chatbot = gr.Chatbot(\n",
        "                                height=400,\n",
        "                                bubble_full_width=False,\n",
        "                                show_label=False\n",
        "                            )\n",
        "                            pmpnn_msg = gr.Textbox(\n",
        "                                placeholder=\"Ask about ProteinMPNN or your protein design...\",\n",
        "                                show_label=False,\n",
        "                                container=False\n",
        "                            )\n",
        "                            pmpnn_clear = gr.ClearButton([pmpnn_msg, pmpnn_chatbot])\n",
        "\n",
        "                        with gr.Column(scale=1):\n",
        "                            pmpnn_voice_selector = gr.Dropdown(\n",
        "                                choices=[\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"],\n",
        "                                value=\"alloy\",\n",
        "                                label=\"Select Voice\",\n",
        "                                container=True\n",
        "                            )\n",
        "                            pmpnn_audio_output = gr.Audio(\n",
        "                                label=\"AI Response Audio\",\n",
        "                                type=\"filepath\"\n",
        "                            )\n",
        "\n",
        "                    def proteinmpnn_chat(message, chat_history, history_state, parameters, code):\n",
        "                        \"\"\"\n",
        "                        Enhanced ProteinMPNN chat assistant with proper flow control and error handling\n",
        "                        \"\"\"\n",
        "                        try:\n",
        "                            if history_state is None:\n",
        "                                history_state = []\n",
        "\n",
        "                            logger.info(\"Starting ProteinMPNN chat interaction\")\n",
        "\n",
        "                            # Phase 1: Query Preparation\n",
        "                            try:\n",
        "                                logger.info(\"Preparing query and context\")\n",
        "                                query_result = query_engine.query(\n",
        "                                    message,\n",
        "                                    response_mode=\"compact\",\n",
        "                                    timeout=30\n",
        "                                )\n",
        "                                relevant_info = str(query_result.response)\n",
        "                                logger.info(\"Context retrieved successfully\")\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Error in query phase: {str(e)}\")\n",
        "                                relevant_info = \"Unable to retrieve context. Proceeding with basic response.\"\n",
        "\n",
        "                            # Phase 2: Chat Completion\n",
        "                            try:\n",
        "                                logger.info(\"Initiating chat completion\")\n",
        "                                system_message = {\n",
        "                                    \"role\": \"system\",\n",
        "                                    \"content\": f\"\"\"You are an AI assistant specializing in ProteinMPNN and protein sequence design.\n",
        "                                    Use this relevant information to inform your response:\n",
        "\n",
        "                                    Relevant Information: {relevant_info}\n",
        "                                    Parameters: {parameters}\n",
        "                                    Code: {code}\n",
        "\n",
        "                                    Focus on:\n",
        "                                    1. Explaining ProteinMPNN capabilities and limitations\n",
        "                                    2. Guiding parameter optimization\n",
        "                                    3. Interpreting sequence design results\n",
        "                                    4. Best practices for protein sequence design\n",
        "                                    5. NVIDIA GPU acceleration features\n",
        "\n",
        "                                    If you don't have enough information to answer a specific question,\n",
        "                                    please say so and offer to help with other aspects of ProteinMPNN or protein sequence design.\"\"\"\n",
        "                                }\n",
        "\n",
        "                                messages = [system_message] + history_state + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "                                response = openai_client.chat.completions.create(\n",
        "                                    model=\"gpt-4o\",\n",
        "                                    messages=messages,\n",
        "                                    max_tokens=16384,\n",
        "                                    temperature=0,\n",
        "                                    stream=False,\n",
        "                                    timeout=60\n",
        "                                )\n",
        "\n",
        "                                response_text = response.choices[0].message.content\n",
        "                                logger.info(\"Chat completion successful\")\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Error in chat completion: {str(e)}\")\n",
        "                                return message, chat_history, history_state, None\n",
        "\n",
        "                            # Phase 3: Audio Generation\n",
        "                            try:\n",
        "                                logger.info(\"Starting audio generation\")\n",
        "                                audio_handler = AudioResponseHandler(openai_client)\n",
        "                                audio_file_path = audio_handler.text_to_speech(response_text)\n",
        "\n",
        "                                if not audio_file_path:\n",
        "                                    logger.warning(\"Audio generation returned no file path\")\n",
        "                                    raise ValueError(\"Audio generation failed\")\n",
        "\n",
        "                                logger.info(f\"Audio generated successfully: {audio_file_path}\")\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Error in audio generation: {str(e)}\")\n",
        "                                audio_file_path = None\n",
        "                                response_text += \"\\n\\n(Note: Audio response generation failed. Text response is still available.)\"\n",
        "\n",
        "                            # Update conversation state\n",
        "                            chat_history = chat_history + [(message, response_text)]\n",
        "                            history_state.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "\n",
        "                            logger.info(\"ProteinMPNN chat interaction completed successfully\")\n",
        "                            return message, chat_history, history_state, audio_file_path\n",
        "\n",
        "                        except Exception as e:\n",
        "                            logger.error(f\"Critical error in ProteinMPNN chat system: {str(e)}\")\n",
        "                            error_message = \"I apologize, but I encountered an unexpected error. Please try again.\"\n",
        "                            chat_history = chat_history + [(message, error_message)]\n",
        "                            return message, chat_history, history_state, None\n",
        "\n",
        "                    # Add these handler functions right after the proteinmpnn_chat function\n",
        "\n",
        "                    def proteinmpnn_chat_handler(message, chat_history, state, parameters, code):\n",
        "                        \"\"\"Handler function with monitoring\"\"\"\n",
        "                        try:\n",
        "                            with tracing.start_as_current_span(\"proteinmpnn_chat_interaction\") as chat_span:\n",
        "                                chat_span.set_attribute(\"message_length\", len(message))\n",
        "                                result = proteinmpnn_chat(message, chat_history, state, parameters, code)\n",
        "                                chat_monitor.record_success()\n",
        "                                return result\n",
        "                        except Exception as e:\n",
        "                            chat_monitor.record_failure(e)\n",
        "                            logger.error(f\"ProteinMPNN chat handler error: {str(e)}\")\n",
        "                            raise gr.Error(f\"Chat system error: {str(e)}\")\n",
        "\n",
        "                    def update_proteinmpnn_voice_handler(voice):\n",
        "                        \"\"\"Voice update handler with monitoring\"\"\"\n",
        "                        try:\n",
        "                            audio_handler = AudioResponseHandler(openai_client)\n",
        "                            audio_handler.set_voice(voice)\n",
        "                            chat_monitor.record_success()\n",
        "                            return None\n",
        "                        except Exception as e:\n",
        "                            chat_monitor.record_failure(e)\n",
        "                            logger.error(f\"Voice update error: {str(e)}\")\n",
        "                            raise gr.Error(f\"Voice update error: {str(e)}\")\n",
        "\n",
        "                    gr.Markdown(\"## Generate and Execute ProteinMPNN\")\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            pmpnn_file_input = gr.File(label=\"Upload PDB File\")\n",
        "                            pmpnn_api_key = gr.Textbox(\n",
        "                                label=\"NVIDIA API Key (Optional)\",\n",
        "                                placeholder=\"Enter your NVIDIA API key to run ProteinMPNN...\",\n",
        "                                type=\"password\"\n",
        "                            )\n",
        "                            pmpnn_generate_button = gr.Button(\"🧬 Generate & Run ProteinMPNN\", variant=\"primary\")\n",
        "                        with gr.Column():\n",
        "                            pmpnn_parameters_output = gr.Textbox(\n",
        "                                label=\"ProteinMPNN Parameters (AI Analysis)\",\n",
        "                                lines=10\n",
        "                            )\n",
        "                            pmpnn_code_output = gr.Code(\n",
        "                                language=\"python\",\n",
        "                                label=\"ProteinMPNN Python Code\"\n",
        "                            )\n",
        "                            pmpnn_explanation_output = gr.Textbox(  # New component for explanation\n",
        "                                label=\"Parameter Explanation\",\n",
        "                                lines=5,\n",
        "                                interactive=False\n",
        "                            )\n",
        "                            pmpnn_audio_output = gr.Audio(  # New component for audio output\n",
        "                                label=\"Listen to Parameter Analysis\",\n",
        "                                type=\"filepath\"\n",
        "                            )\n",
        "\n",
        "                    # Add execution results section\n",
        "                    gr.Markdown(\"## Execution Results\")\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            pmpnn_execution_output = gr.Textbox(\n",
        "                                label=\"Execution Status\",\n",
        "                                lines=5,\n",
        "                                interactive=False\n",
        "                            )\n",
        "                        with gr.Column():\n",
        "                            pmpnn_fasta_output = gr.Textbox(\n",
        "                                label=\"Generated Sequences (FASTA)\",\n",
        "                                lines=10,\n",
        "                                interactive=False\n",
        "                            )\n",
        "\n",
        "                    # Update the chat submission to use the new handler\n",
        "                    pmpnn_msg.submit(\n",
        "                        proteinmpnn_chat_handler,\n",
        "                        inputs=[\n",
        "                            pmpnn_msg,\n",
        "                            pmpnn_chatbot,\n",
        "                            gr.State([]),\n",
        "                            pmpnn_parameters_output,\n",
        "                            pmpnn_code_output\n",
        "                        ],\n",
        "                        outputs=[\n",
        "                            pmpnn_msg,\n",
        "                            pmpnn_chatbot,\n",
        "                            gr.State([]),\n",
        "                            pmpnn_audio_output\n",
        "                        ]\n",
        "                    )\n",
        "\n",
        "                    # Update voice selection to use the new handler\n",
        "                    pmpnn_voice_selector.change(\n",
        "                        update_proteinmpnn_voice_handler,\n",
        "                        inputs=[pmpnn_voice_selector],\n",
        "                        outputs=[pmpnn_audio_output]\n",
        "                    )\n",
        "\n",
        "                    # Connect the components with the updated function\n",
        "                    pmpnn_generate_button.click(\n",
        "                        proteinmpnn_app,\n",
        "                        inputs=[pmpnn_file_input, pmpnn_api_key],\n",
        "                        outputs=[\n",
        "                            pmpnn_parameters_output,\n",
        "                            pmpnn_code_output,\n",
        "                            pmpnn_explanation_output,\n",
        "                            pmpnn_audio_output,\n",
        "                            pmpnn_execution_output,\n",
        "                            pmpnn_fasta_output\n",
        "                        ]\n",
        "                    )\n",
        "\n",
        "                with gr.TabItem(\"AlphaFold2- Protein Design Step 3\"):\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    ## AlphaFold2: Predict Protein Structures https://build.nvidia.com/deepmind/alphafold2\n",
        "\n",
        "                    ### Model Overview\n",
        "                    AlphaFold2 is DeepMind's revolutionary protein structure prediction system, achieving unprecedented accuracy in predicting 3D protein structures from amino acid sequences. The NVIDIA NGC platform provides an optimized implementation of AlphaFold2.\n",
        "\n",
        "                    ### Key Parameters\n",
        "                    - **algorithm**: MSA search algorithm (jackhmmer/hhblits)\n",
        "                    - **e_value**: E-value threshold for sequence inclusion\n",
        "                    - **iterations**: Number of search iterations\n",
        "                    - **databases**: Sequence databases to search\n",
        "                    - **relax_prediction**: Whether to relax the final structure\n",
        "                    - **structure_model_preset**: Modeling preset (monomer/monomer_casp14/monomer_ptm/multimer)\n",
        "\n",
        "                    ### Hot to Use\n",
        "                    1. Take the amino acid sequence from your ProteinMPNN output or another sequence using only standard one-letter amino acid codes and enter it in the Amino Acid Sequence window.\n",
        "                    2. Click \"Generate AlphaFold2 Script\" to create a customized Python script, the AI assistant will analyze your sequence and suggest optimal parameters for a successful NIM API call.\n",
        "                    3. Copy the generated Python script and set up your environment with required dependencies (pip install requests)\n",
        "                    4. Use the following URL to generate your AF2 API key under the Python tab: https://build.nvidia.com/deepmind/alphafold2?snippet_tab=Python\n",
        "                    5. Run your Python script locally and enter your API key when prompted\n",
        "                    6. If you encounter issues, you can also follow this link to the AlphaFold3 server when you can enter your sequence to complete the protein strucutre prediction from your sequence: https://alphafoldserver.com/\n",
        "\n",
        "                    \"\"\")\n",
        "\n",
        "                    # Chat interface with voice components\n",
        "                    with gr.Row():\n",
        "                        with gr.Column(scale=4):\n",
        "                            af2_chatbot = gr.Chatbot(\n",
        "                                height=400,\n",
        "                                bubble_full_width=False,\n",
        "                                show_label=False\n",
        "                            )\n",
        "                            af2_msg = gr.Textbox(\n",
        "                                placeholder=\"Ask about AlphaFold2 or your protein sequence...\",\n",
        "                                show_label=False,\n",
        "                                container=False\n",
        "                            )\n",
        "                            af2_clear = gr.ClearButton([af2_msg, af2_chatbot])\n",
        "\n",
        "                        with gr.Column(scale=1):\n",
        "                            af2_voice_selector = gr.Dropdown(\n",
        "                                choices=[\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"],\n",
        "                                value=\"alloy\",\n",
        "                                label=\"Select Voice\",\n",
        "                                container=True\n",
        "                            )\n",
        "                            af2_audio_output = gr.Audio(\n",
        "                                label=\"AI Response Audio\",\n",
        "                                type=\"filepath\"\n",
        "                            )\n",
        "\n",
        "                        def chat_with_alphafold2(message, chat_history, history_state):\n",
        "                            \"\"\"\n",
        "                            Enhanced AlphaFold2 chat assistant with proper flow control and error handling\n",
        "                            \"\"\"\n",
        "                            try:\n",
        "                                if history_state is None:\n",
        "                                    history_state = []\n",
        "\n",
        "                                logger.info(\"Starting AlphaFold2 chat interaction\")\n",
        "\n",
        "                                # Phase 1: Query Preparation\n",
        "                                try:\n",
        "                                    logger.info(\"Preparing query and context\")\n",
        "                                    # Create a synchronous query to prevent stalling\n",
        "                                    query_result = query_engine.query(\n",
        "                                        message,\n",
        "                                        response_mode=\"compact\",  # Use compact mode for faster processing\n",
        "                                        timeout=30  # Add timeout to prevent hanging\n",
        "                                    )\n",
        "                                    relevant_context = str(query_result.response)\n",
        "                                    logger.info(\"Context retrieved successfully\")\n",
        "                                except Exception as e:\n",
        "                                    logger.error(f\"Error in query phase: {str(e)}\")\n",
        "                                    relevant_context = \"Unable to retrieve context. Proceeding with basic response.\"\n",
        "\n",
        "                                # Phase 2: Chat Completion\n",
        "                                try:\n",
        "                                    logger.info(\"Initiating chat completion\")\n",
        "                                    system_message = {\n",
        "                                        \"role\": \"system\",\n",
        "                                        \"content\": f\"\"\"You are an AI assistant specializing in AlphaFold2 and protein structure prediction.\n",
        "                                        Use this relevant information to inform your response: {relevant_context}\n",
        "\n",
        "                                        Help users with:\n",
        "                                        1. Understanding AlphaFold2 capabilities and limitations\n",
        "                                        2. Optimizing parameters for different protein types\n",
        "                                        3. Interpreting results and confidence scores\n",
        "                                        4. Best practices for structure prediction\n",
        "                                        5. Integration with NVIDIA GPUs and optimization\n",
        "\n",
        "                                        If you don't have enough information to answer a specific question,\n",
        "                                        please say so and offer to help with other aspects of protein structure prediction.\"\"\"\n",
        "                                    }\n",
        "\n",
        "                                    messages = [system_message] + history_state + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "                                    response = openai_client.chat.completions.create(\n",
        "                                        model=\"gpt-4o\",\n",
        "                                        messages=messages,\n",
        "                                        max_tokens=16384,\n",
        "                                        temperature=0,\n",
        "                                        stream=False,\n",
        "                                        timeout=60  # Add timeout for chat completion\n",
        "                                    )\n",
        "\n",
        "                                    response_text = response.choices[0].message.content\n",
        "                                    logger.info(\"Chat completion successful\")\n",
        "                                except Exception as e:\n",
        "                                    logger.error(f\"Error in chat completion: {str(e)}\")\n",
        "                                    return message, chat_history, history_state, None\n",
        "\n",
        "                                # Phase 3: Audio Generation\n",
        "                                try:\n",
        "                                    logger.info(\"Starting audio generation\")\n",
        "                                    audio_handler = AudioResponseHandler(openai_client)\n",
        "                                    audio_file_path = audio_handler.text_to_speech(response_text)\n",
        "\n",
        "                                    if not audio_file_path:\n",
        "                                        logger.warning(\"Audio generation returned no file path\")\n",
        "                                        raise ValueError(\"Audio generation failed\")\n",
        "\n",
        "                                    logger.info(f\"Audio generated successfully: {audio_file_path}\")\n",
        "                                except Exception as e:\n",
        "                                    logger.error(f\"Error in audio generation: {str(e)}\")\n",
        "                                    audio_file_path = None\n",
        "                                    response_text += \"\\n\\n(Note: Audio response generation failed. Text response is still available.)\"\n",
        "\n",
        "                                # Update conversation state\n",
        "                                chat_history = chat_history + [(message, response_text)]\n",
        "                                history_state.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "\n",
        "                                logger.info(\"AlphaFold2 chat interaction completed successfully\")\n",
        "                                return message, chat_history, history_state, audio_file_path\n",
        "\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Critical error in AlphaFold2 chat system: {str(e)}\")\n",
        "                                error_message = \"I apologize, but I encountered an unexpected error. Please try again.\"\n",
        "                                chat_history = chat_history + [(message, error_message)]\n",
        "                                return message, chat_history, history_state, None\n",
        "\n",
        "                        def alphafold2_chat_handler(message, chat_history, state, parameters=None, code=None):\n",
        "                            \"\"\"Chat handler with monitoring and tracing\"\"\"\n",
        "                            try:\n",
        "                                with tracing.start_as_current_span(\"alphafold2_chat_interaction\") as chat_span:\n",
        "                                    chat_span.set_attribute(\"message_length\", len(message))\n",
        "                                    result = chat_with_alphafold2(message, chat_history, state)\n",
        "                                    chat_monitor.record_success()\n",
        "                                    return result\n",
        "                            except Exception as e:\n",
        "                                chat_monitor.record_failure(e)\n",
        "                                logger.error(f\"AlphaFold2 chat handler error: {str(e)}\")\n",
        "                                raise gr.Error(f\"Chat system error: {str(e)}\")\n",
        "\n",
        "                        def update_alphafold2_voice_handler(voice):\n",
        "                            \"\"\"Voice update handler with monitoring\"\"\"\n",
        "                            try:\n",
        "                                audio_handler = AudioResponseHandler(openai_client)\n",
        "                                audio_handler.set_voice(voice)\n",
        "                                chat_monitor.record_success()\n",
        "                                return None\n",
        "                            except Exception as e:\n",
        "                                chat_monitor.record_failure(e)\n",
        "                                logger.error(f\"Voice update error: {str(e)}\")\n",
        "                                raise gr.Error(f\"Voice update error: {str(e)}\")\n",
        "\n",
        "                    gr.Markdown(\"## Generate AlphaFold2 Python Script\")\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            af2_sequence_input = gr.Textbox(\n",
        "                                label=\"Amino Acid Sequence\",\n",
        "                                placeholder=\"Enter sequence using standard one-letter amino acid codes...\",\n",
        "                                lines=5\n",
        "                            )\n",
        "                            af2_generate_button = gr.Button(\n",
        "                                \"🧬 Generate AlphaFold2 Script\",\n",
        "                                variant=\"primary\"\n",
        "                            )\n",
        "                        with gr.Column():\n",
        "                            af2_parameters_output = gr.Textbox(\n",
        "                                label=\"AlphaFold2 Parameters (AI Analysis)\",\n",
        "                                lines=10\n",
        "                            )\n",
        "                            af2_code_output = gr.Code(\n",
        "                                language=\"python\",\n",
        "                                label=\"AlphaFold2 Python Code\"\n",
        "                            )\n",
        "\n",
        "                    # Update the Gradio interface connections\n",
        "                    af2_msg.submit(\n",
        "                        alphafold2_chat_handler,\n",
        "                        inputs=[\n",
        "                            af2_msg,\n",
        "                            af2_chatbot,\n",
        "                            gr.State([]),\n",
        "                            af2_parameters_output,\n",
        "                            af2_code_output\n",
        "                        ],\n",
        "                        outputs=[\n",
        "                            af2_msg,\n",
        "                            af2_chatbot,\n",
        "                            gr.State([]),\n",
        "                            af2_audio_output\n",
        "                        ]\n",
        "                    )\n",
        "\n",
        "                    af2_voice_selector.change(\n",
        "                        update_alphafold2_voice_handler,\n",
        "                        inputs=[af2_voice_selector],\n",
        "                        outputs=[af2_audio_output]\n",
        "                    )\n",
        "\n",
        "                    af2_generate_button.click(\n",
        "                        fn=alphafold2_app,\n",
        "                        inputs=[af2_sequence_input],\n",
        "                        outputs=[af2_parameters_output, af2_code_output]\n",
        "                    )\n",
        "\n",
        "                    # Add clear button functionality\n",
        "                    af2_clear.click(\n",
        "                        lambda: ([], [], [], None),\n",
        "                        outputs=[af2_msg, af2_chatbot, gr.State([]), af2_audio_output]\n",
        "                    )\n",
        "\n",
        "                    {\n",
        "                        'chatbot': af2_chatbot,\n",
        "                        'msg': af2_msg,\n",
        "                        'voice_selector': af2_voice_selector,\n",
        "                        'audio_output': af2_audio_output,\n",
        "                        'sequence_input': af2_sequence_input,\n",
        "                        'parameters_output': af2_parameters_output,\n",
        "                        'code_output': af2_code_output\n",
        "                    }\n",
        "\n",
        "                # Update the MolMIM tab section with voice support\n",
        "                with gr.TabItem(\"MolMIM\"):\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    ## MolMIM: Molecular Generation and Optimization https://build.nvidia.com/nvidia/molmim-generate\n",
        "\n",
        "                    ### Model Overview\n",
        "                    MolMIM (Molecular Mutual Information Machine) is a latent variable model developed by NVIDIA for generating and optimizing molecules. It uses a transformer architecture trained on a large-scale dataset of molecules in SMILES format.\n",
        "\n",
        "                    ### Key Features\n",
        "                    - Generates new molecules by sampling from the latent space around a seed molecule\n",
        "                    - Performs optimization using the CMA-ES algorithm in the model's latent space\n",
        "                    - Utilizes Mutual Information Machine (MIM) learning for informative and clustered latent codes\n",
        "\n",
        "                    ### How to Use\n",
        "                    1. Enter a SMILES string for your seed molecule\n",
        "                    2. Adjust the parameters as needed\n",
        "                    3. Generate a Python script to run MolMIM using the API\n",
        "                    4. Use the chat assistant for any questions or clarifications\n",
        "                    \"\"\")\n",
        "\n",
        "                    # Add voice components for MolMIM chat\n",
        "                    with gr.Row():\n",
        "                        with gr.Column(scale=4):\n",
        "                            molmim_chatbot = gr.Chatbot(\n",
        "                                height=400,\n",
        "                                bubble_full_width=False,\n",
        "                                show_label=False\n",
        "                            )\n",
        "                            molmim_msg = gr.Textbox(\n",
        "                                placeholder=\"Ask about MolMIM or the generated script...\",\n",
        "                                show_label=False,\n",
        "                                container=False\n",
        "                            )\n",
        "                            molmim_clear = gr.ClearButton([molmim_msg, molmim_chatbot])\n",
        "\n",
        "                        with gr.Column(scale=1):\n",
        "                            molmim_voice_selector = gr.Dropdown(\n",
        "                                choices=[\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"],\n",
        "                                value=\"alloy\",\n",
        "                                label=\"Select Voice\",\n",
        "                                container=True\n",
        "                            )\n",
        "                            molmim_audio_output = gr.Audio(\n",
        "                                label=\"AI Response Audio\",\n",
        "                                type=\"filepath\"\n",
        "                            )\n",
        "\n",
        "                    def molmim_chat_with_gpt4o(message, chat_history, history_state):\n",
        "                        \"\"\"\n",
        "                        Enhanced MolMIM chat assistant with proper flow control and error handling\n",
        "                        \"\"\"\n",
        "                        try:\n",
        "                            if history_state is None:\n",
        "                                history_state = []\n",
        "\n",
        "                            logger.info(\"Starting MolMIM chat interaction\")\n",
        "\n",
        "                            # Phase 1: Query Preparation\n",
        "                            try:\n",
        "                                logger.info(\"Preparing query and context\")\n",
        "                                # Create a synchronous query to prevent stalling\n",
        "                                query_result = query_engine.query(\n",
        "                                    message,\n",
        "                                    response_mode=\"compact\",  # Use compact mode for faster processing\n",
        "                                    timeout=30  # Add timeout to prevent hanging\n",
        "                                )\n",
        "                                relevant_context = str(query_result.response)\n",
        "                                logger.info(\"Context retrieved successfully\")\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Error in query phase: {str(e)}\")\n",
        "                                relevant_context = \"Unable to retrieve context. Proceeding with basic response.\"\n",
        "\n",
        "                            # Phase 2: Chat Completion\n",
        "                            try:\n",
        "                                logger.info(\"Initiating chat completion\")\n",
        "                                system_message = {\n",
        "                                    \"role\": \"system\",\n",
        "                                    \"content\": f\"\"\"You are an AI assistant specializing in molecular design and NVIDIA MolMIM technology.\n",
        "                                    Use this relevant information to inform your response: {relevant_context}\n",
        "\n",
        "                                    Focus on:\n",
        "                                    1. MolMIM capabilities and limitations\n",
        "                                    2. Molecular generation and optimization strategies\n",
        "                                    3. NVIDIA GPU acceleration features\n",
        "                                    4. Best practices for using MolMIM\n",
        "                                    5. Property optimization techniques\n",
        "\n",
        "                                    If you don't have enough information to answer a specific question,\n",
        "                                    please say so and offer to help with other aspects of molecular design or NVIDIA's capabilities.\"\"\"\n",
        "                                }\n",
        "\n",
        "                                messages = [system_message] + history_state + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "                                response = openai_client.chat.completions.create(\n",
        "                                    model=\"gpt-4o\",\n",
        "                                    messages=messages,\n",
        "                                    max_tokens=16384,\n",
        "                                    temperature=0,\n",
        "                                    stream=False,\n",
        "                                    timeout=60  # Add timeout for chat completion\n",
        "                                )\n",
        "\n",
        "                                response_text = response.choices[0].message.content\n",
        "                                logger.info(\"Chat completion successful\")\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Error in chat completion: {str(e)}\")\n",
        "                                return message, chat_history, history_state, None\n",
        "\n",
        "                            # Phase 3: Audio Generation\n",
        "                            try:\n",
        "                                logger.info(\"Starting audio generation\")\n",
        "                                audio_handler = AudioResponseHandler(openai_client)\n",
        "                                audio_file_path = audio_handler.text_to_speech(response_text)\n",
        "\n",
        "                                if not audio_file_path:\n",
        "                                    logger.warning(\"Audio generation returned no file path\")\n",
        "                                    raise ValueError(\"Audio generation failed\")\n",
        "\n",
        "                                logger.info(f\"Audio generated successfully: {audio_file_path}\")\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Error in audio generation: {str(e)}\")\n",
        "                                audio_file_path = None\n",
        "                                response_text += \"\\n\\n(Note: Audio response generation failed. Text response is still available.)\"\n",
        "\n",
        "                            # Update conversation state\n",
        "                            chat_history = chat_history + [(message, response_text)]\n",
        "                            history_state.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "\n",
        "                            logger.info(\"MolMIM chat interaction completed successfully\")\n",
        "                            return message, chat_history, history_state, audio_file_path\n",
        "\n",
        "                        except Exception as e:\n",
        "                            logger.error(f\"Critical error in MolMIM chat system: {str(e)}\")\n",
        "                            error_message = \"I apologize, but I encountered an unexpected error. Please try again.\"\n",
        "                            chat_history = chat_history + [(message, error_message)]\n",
        "                            return message, chat_history, history_state, None\n",
        "\n",
        "                    # Add these handler functions with monitoring\n",
        "                    def molmim_chat_handler(message, chat_history, state):\n",
        "                        \"\"\"Chat handler with monitoring and tracing\"\"\"\n",
        "                        try:\n",
        "                            with tracing.start_as_current_span(\"molmim_chat_interaction\") as chat_span:\n",
        "                                chat_span.set_attribute(\"message_length\", len(message))\n",
        "                                result = molmim_chat_with_gpt4o(message, chat_history, state)\n",
        "                                chat_monitor.record_success()\n",
        "                                return result\n",
        "                        except Exception as e:\n",
        "                            chat_monitor.record_failure(e)\n",
        "                            logger.error(f\"MolMIM chat handler error: {str(e)}\")\n",
        "                            raise gr.Error(f\"Chat system error: {str(e)}\")\n",
        "\n",
        "                    def update_molmim_voice_handler(voice):\n",
        "                        \"\"\"Voice update handler with monitoring\"\"\"\n",
        "                        try:\n",
        "                            audio_handler = AudioResponseHandler(openai_client)\n",
        "                            audio_handler.set_voice(voice)\n",
        "                            chat_monitor.record_success()\n",
        "                            return None\n",
        "                        except Exception as e:\n",
        "                            chat_monitor.record_failure(e)\n",
        "                            logger.error(f\"Voice update error: {str(e)}\")\n",
        "                            raise gr.Error(f\"Voice update error: {str(e)}\")\n",
        "\n",
        "                    gr.Markdown(\"## Create MolMIM Python Script\")\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            molmim_smiles_input = gr.Textbox(label=\"SMILES String\", placeholder=\"Enter SMILES string...\")\n",
        "                            molmim_generate_button = gr.Button(\"🧪 Generate MolMIM Script\", variant=\"primary\")\n",
        "                        with gr.Column():\n",
        "                            molmim_parameters_output = gr.Textbox(label=\"MolMIM Parameters (AI Analysis)\", lines=10)\n",
        "                            molmim_code_output = gr.Code(language=\"python\", label=\"MolMIM Python Code\")\n",
        "\n",
        "                    # Update the Gradio interface connections\n",
        "                    molmim_msg.submit(\n",
        "                        molmim_chat_handler,\n",
        "                        inputs=[\n",
        "                            molmim_msg,\n",
        "                            molmim_chatbot,\n",
        "                            gr.State([])\n",
        "                        ],\n",
        "                        outputs=[\n",
        "                            molmim_msg,\n",
        "                            molmim_chatbot,\n",
        "                            gr.State([]),\n",
        "                            molmim_audio_output\n",
        "                        ]\n",
        "                    )\n",
        "\n",
        "                    # Connect voice selection\n",
        "                    molmim_voice_selector.change(\n",
        "                        update_molmim_voice_handler,\n",
        "                        inputs=[molmim_voice_selector],\n",
        "                        outputs=[molmim_audio_output]\n",
        "                    )\n",
        "\n",
        "                    # Update the clear button functionality\n",
        "                    molmim_clear.click(\n",
        "                        lambda: ([], [], [], None),\n",
        "                        outputs=[molmim_msg, molmim_chatbot, gr.State([]), molmim_audio_output]\n",
        "                    )\n",
        "\n",
        "                    # Connect MolMIM script generation\n",
        "                    molmim_generate_button.click(\n",
        "                        molmim_app,\n",
        "                        inputs=[molmim_smiles_input],\n",
        "                        outputs=[molmim_parameters_output, molmim_code_output]\n",
        "                    )\n",
        "\n",
        "                with gr.TabItem(\"DiffDock\"):\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    ## DiffDock: Molecular Docking via Diffusion Models https://build.nvidia.com/mit/diffdock\n",
        "\n",
        "                    ### Model Overview\n",
        "                    DiffDock is a generative diffusion model for drug discovery in molecular blind docking. It consists of two key components:\n",
        "                    - **Score Model**: Generates potential poses through reverse diffusion\n",
        "                    - **Confidence Model**: Ranks and evaluates generated poses\n",
        "\n",
        "                    Key Features:\n",
        "                    - No binding pocket information required\n",
        "                    - Flexible molecule positioning and orientation\n",
        "                    - Trained on PLINDER dataset for improved accuracy\n",
        "                    - Supports both commercial and non-commercial use\n",
        "\n",
        "                    ### Model Architecture\n",
        "                    - Type: Score-Based Diffusion Model (SBDM)\n",
        "                    - Network: Graph Convolution Neural Network\n",
        "                    - Parameters: 20M in Score model\n",
        "                    - Components: Embedding, 6-layer graph convolution, output layer\n",
        "                    \"\"\")\n",
        "\n",
        "                    # Chat interface with voice components\n",
        "                    with gr.Row():\n",
        "                        with gr.Column(scale=4):\n",
        "                            diffdock_chatbot = gr.Chatbot(\n",
        "                                height=400,\n",
        "                                bubble_full_width=False,\n",
        "                                show_label=False\n",
        "                            )\n",
        "                            diffdock_msg = gr.Textbox(\n",
        "                                placeholder=\"Ask about DiffDock or molecular docking...\",\n",
        "                                show_label=False,\n",
        "                                container=False\n",
        "                            )\n",
        "                            diffdock_clear = gr.ClearButton([diffdock_msg, diffdock_chatbot])\n",
        "\n",
        "                        with gr.Column(scale=1):\n",
        "                            diffdock_voice_selector = gr.Dropdown(\n",
        "                                choices=[\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"],\n",
        "                                value=\"alloy\",\n",
        "                                label=\"Select Voice\",\n",
        "                                container=True\n",
        "                            )\n",
        "                            diffdock_audio_output = gr.Audio(\n",
        "                                label=\"AI Response Audio\",\n",
        "                                type=\"filepath\"\n",
        "                            )\n",
        "\n",
        "                    def diffdock_chat_with_voice(message, chat_history, history_state, parameters=None, code=None):\n",
        "                        \"\"\"\n",
        "                        Enhanced DiffDock chat assistant with proper flow control and error handling\n",
        "                        \"\"\"\n",
        "                        try:\n",
        "                            if history_state is None:\n",
        "                                history_state = []\n",
        "\n",
        "                            logger.info(\"Starting DiffDock chat interaction\")\n",
        "\n",
        "                            # Phase 1: Query Preparation\n",
        "                            try:\n",
        "                                logger.info(\"Preparing query and context\")\n",
        "                                # Create a synchronous query to prevent stalling\n",
        "                                query_result = query_engine.query(\n",
        "                                    message,\n",
        "                                    response_mode=\"compact\",  # Use compact mode for faster processing\n",
        "                                    timeout=30  # Add timeout to prevent hanging\n",
        "                                )\n",
        "                                relevant_context = str(query_result.response)\n",
        "                                logger.info(\"Context retrieved successfully\")\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Error in query phase: {str(e)}\")\n",
        "                                relevant_context = \"Unable to retrieve context. Proceeding with basic response.\"\n",
        "\n",
        "                            # Phase 2: Chat Completion\n",
        "                            try:\n",
        "                                logger.info(\"Initiating chat completion\")\n",
        "                                system_message = {\n",
        "                                    \"role\": \"system\",\n",
        "                                    \"content\": f\"\"\"You are an AI assistant specializing in molecular docking with DiffDock and NVIDIA technologies.\n",
        "                                    Use this relevant information to inform your response: {relevant_context}\n",
        "\n",
        "                                    Current Context:\n",
        "                                    Parameters: {parameters if parameters else 'No parameters provided'}\n",
        "                                    Code: {code if code else 'No code generated'}\n",
        "\n",
        "                                    Focus on:\n",
        "                                    1. DiffDock capabilities and limitations\n",
        "                                    2. Parameter optimization for different types of molecules\n",
        "                                    3. Integration with NVIDIA GPU acceleration\n",
        "                                    4. Best practices for molecular docking\n",
        "                                    5. Interpreting docking results\n",
        "\n",
        "                                    If you don't have enough information to answer a specific question,\n",
        "                                    please say so and offer to help with other aspects of molecular docking or NVIDIA's capabilities.\"\"\"\n",
        "                                }\n",
        "\n",
        "                                messages = [system_message] + history_state + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "                                response = openai_client.chat.completions.create(\n",
        "                                    model=\"gpt-4o\",\n",
        "                                    messages=messages,\n",
        "                                    max_tokens=16384,\n",
        "                                    temperature=0,\n",
        "                                    stream=False,\n",
        "                                    timeout=60  # Add timeout for chat completion\n",
        "                                )\n",
        "\n",
        "                                response_text = response.choices[0].message.content\n",
        "                                logger.info(\"Chat completion successful\")\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Error in chat completion: {str(e)}\")\n",
        "                                return message, chat_history, history_state, None\n",
        "\n",
        "                            # Phase 3: Audio Generation\n",
        "                            try:\n",
        "                                logger.info(\"Starting audio generation\")\n",
        "                                audio_handler = AudioResponseHandler(openai_client)\n",
        "                                audio_file_path = audio_handler.text_to_speech(response_text)\n",
        "\n",
        "                                if not audio_file_path:\n",
        "                                    logger.warning(\"Audio generation returned no file path\")\n",
        "                                    raise ValueError(\"Audio generation failed\")\n",
        "\n",
        "                                logger.info(f\"Audio generated successfully: {audio_file_path}\")\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Error in audio generation: {str(e)}\")\n",
        "                                audio_file_path = None\n",
        "                                response_text += \"\\n\\n(Note: Audio response generation failed. Text response is still available.)\"\n",
        "\n",
        "                            # Update conversation state\n",
        "                            chat_history = chat_history + [(message, response_text)]\n",
        "                            history_state.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "\n",
        "                            logger.info(\"DiffDock chat interaction completed successfully\")\n",
        "                            return message, chat_history, history_state, audio_file_path\n",
        "\n",
        "                        except Exception as e:\n",
        "                            logger.error(f\"Critical error in DiffDock chat system: {str(e)}\")\n",
        "                            error_message = \"I apologize, but I encountered an unexpected error. Please try again.\"\n",
        "                            chat_history = chat_history + [(message, error_message)]\n",
        "                            return message, chat_history, history_state, None\n",
        "\n",
        "                    def diffdock_chat_handler(message, chat_history, state, parameters=None, code=None):\n",
        "                        \"\"\"Chat handler with monitoring and tracing\"\"\"\n",
        "                        try:\n",
        "                            with tracing.start_as_current_span(\"diffdock_chat_interaction\") as chat_span:\n",
        "                                chat_span.set_attribute(\"message_length\", len(message))\n",
        "                                result = diffdock_chat_with_voice(message, chat_history, state, parameters, code)\n",
        "                                chat_monitor.record_success()\n",
        "                                return result\n",
        "                        except Exception as e:\n",
        "                            chat_monitor.record_failure(e)\n",
        "                            logger.error(f\"DiffDock chat handler error: {str(e)}\")\n",
        "                            raise gr.Error(f\"Chat system error: {str(e)}\")\n",
        "\n",
        "                    def update_diffdock_voice_handler(voice):\n",
        "                        \"\"\"Voice update handler with monitoring\"\"\"\n",
        "                        try:\n",
        "                            audio_handler = AudioResponseHandler(openai_client)\n",
        "                            audio_handler.set_voice(voice)\n",
        "                            chat_monitor.record_success()\n",
        "                            return None\n",
        "                        except Exception as e:\n",
        "                            chat_monitor.record_failure(e)\n",
        "                            logger.error(f\"Voice update error: {str(e)}\")\n",
        "                            raise gr.Error(f\"Voice update error: {str(e)}\")\n",
        "\n",
        "                    gr.Markdown(\"## Generate and Execute DiffDock\")\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            diffdock_protein_input = gr.File(\n",
        "                                label=\"Upload Protein Structure (PDB)\",\n",
        "                                file_types=[\".pdb\"]\n",
        "                            )\n",
        "                            diffdock_ligand_input = gr.Textbox(\n",
        "                                label=\"Ligand SMILES String\",\n",
        "                                placeholder=\"Enter SMILES string...\"\n",
        "                            )\n",
        "                            diffdock_api_key = gr.Textbox(\n",
        "                                label=\"NVIDIA API Key\",\n",
        "                                placeholder=\"Enter your NVIDIA API key...\",\n",
        "                                type=\"password\"\n",
        "                            )\n",
        "                            diffdock_generate_button = gr.Button(\"🎯 Generate & Run DiffDock\", variant=\"primary\")\n",
        "\n",
        "                        with gr.Column():\n",
        "                            diffdock_parameters_output = gr.Textbox(\n",
        "                                label=\"DiffDock Parameters (AI Analysis)\",\n",
        "                                lines=10\n",
        "                            )\n",
        "                            diffdock_code_output = gr.Code(\n",
        "                                language=\"python\",\n",
        "                                label=\"DiffDock Python Code\"\n",
        "                            )\n",
        "\n",
        "                    def run_diffdock(api_key, protein_file, ligand_smiles, parameters):\n",
        "                        try:\n",
        "                            logger.info(\"Starting DiffDock run\")\n",
        "\n",
        "                            if not api_key or not protein_file or not ligand_smiles:\n",
        "                                return \"Please provide all required inputs.\", None\n",
        "\n",
        "                            # Log input validation\n",
        "                            logger.info(f\"Validating inputs:\"\n",
        "                                      f\"\\nAPI key present: {bool(api_key)}\"\n",
        "                                      f\"\\nProtein file present: {bool(protein_file)}\"\n",
        "                                      f\"\\nLigand SMILES length: {len(ligand_smiles)}\")\n",
        "\n",
        "                            protein_content = process_pdb_file(protein_file)\n",
        "                            if not protein_content:\n",
        "                                return \"Error: Could not read PDB file.\", None\n",
        "\n",
        "                            status, output_files, success = execute_diffdock_script(\n",
        "                                api_key,\n",
        "                                protein_content,\n",
        "                                ligand_smiles,\n",
        "                                parameters\n",
        "                            )\n",
        "\n",
        "                            # Log the results\n",
        "                            logger.info(f\"DiffDock run completed:\"\n",
        "                                      f\"\\nSuccess: {success}\"\n",
        "                                      f\"\\nOutput files: {output_files if output_files else 'None'}\")\n",
        "\n",
        "                            if success and output_files:\n",
        "                                file_outputs = []\n",
        "                                for name, path in output_files.items():\n",
        "                                    if os.path.exists(path):\n",
        "                                        file_outputs.append(path)\n",
        "                                return status, file_outputs if file_outputs else None\n",
        "                            else:\n",
        "                                return status, None\n",
        "\n",
        "                        except Exception as e:\n",
        "                            logger.error(f\"Error in run_diffdock: {str(e)}\")\n",
        "                            logger.error(f\"Full traceback: {traceback.format_exc()}\")\n",
        "                            return f\"Error: {str(e)}\", None\n",
        "\n",
        "                    # Execution results section\n",
        "                    gr.Markdown(\"## Execution Results\")\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            diffdock_execution_output = gr.Textbox(\n",
        "                                label=\"Execution Status\",\n",
        "                                lines=5,\n",
        "                                interactive=False\n",
        "                            )\n",
        "                        with gr.Column():\n",
        "                            diffdock_poses_output = gr.File(\n",
        "                                label=\"Download Docking Results\",\n",
        "                                file_count=\"multiple\",\n",
        "                                interactive=False\n",
        "                            )\n",
        "\n",
        "                    # Connect the components\n",
        "                    diffdock_generate_button.click(\n",
        "                        diffdock_app,\n",
        "                        inputs=[\n",
        "                            diffdock_protein_input,\n",
        "                            diffdock_ligand_input\n",
        "                        ],\n",
        "                        outputs=[\n",
        "                            diffdock_parameters_output,\n",
        "                            diffdock_code_output\n",
        "                        ]\n",
        "                    )\n",
        "\n",
        "\n",
        "                    # Update the Gradio interface connections\n",
        "                    diffdock_msg.submit(\n",
        "                        diffdock_chat_handler,\n",
        "                        inputs=[\n",
        "                            diffdock_msg,\n",
        "                            diffdock_chatbot,\n",
        "                            gr.State([]),\n",
        "                            diffdock_parameters_output,\n",
        "                            diffdock_code_output\n",
        "                        ],\n",
        "                        outputs=[\n",
        "                            diffdock_msg,\n",
        "                            diffdock_chatbot,\n",
        "                            gr.State([]),\n",
        "                            diffdock_audio_output\n",
        "                        ]\n",
        "                    )\n",
        "\n",
        "                    # Connect voice selection\n",
        "                    diffdock_voice_selector.change(\n",
        "                        update_diffdock_voice_handler,\n",
        "                        inputs=[diffdock_voice_selector],\n",
        "                        outputs=[diffdock_audio_output]\n",
        "                    )\n",
        "\n",
        "                    # Update the clear button functionality\n",
        "                    diffdock_clear.click(\n",
        "                        lambda: ([], [], [], None),\n",
        "                        outputs=[diffdock_msg, diffdock_chatbot, gr.State([]), diffdock_audio_output]\n",
        "                    )\n",
        "\n",
        "                    # Connect execution button\n",
        "                    diffdock_generate_button.click(\n",
        "                        run_diffdock,\n",
        "                        inputs=[\n",
        "                            diffdock_api_key,\n",
        "                            diffdock_protein_input,\n",
        "                            diffdock_ligand_input,\n",
        "                            diffdock_parameters_output\n",
        "                        ],\n",
        "                        outputs=[\n",
        "                            diffdock_execution_output,\n",
        "                            diffdock_poses_output\n",
        "                        ]\n",
        "                    )\n",
        "\n",
        "                with gr.TabItem(\"Molecular Dynamics with GROMACS\"):\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    ## GROMACS Chat Assistant and Protocol Generator https://catalog.ngc.nvidia.com/orgs/hpc/containers/gromacs\n",
        "\n",
        "                    ### Overview\n",
        "                    This tool generates optimized GROMACS molecular dynamics simulation protocols tailored to your protein structure.\n",
        "                    It leverages NVIDIA GPU acceleration and modern GROMACS features for maximum performance.\n",
        "\n",
        "                    ### Features\n",
        "                    - Automatic structure analysis\n",
        "                    - GPU-optimized protocol generation\n",
        "                    - DeepSeek-enhanced protocol optimization\n",
        "                    - NVIDIA technology integration analysis\n",
        "                    - Voice-enabled chat assistance\n",
        "                    \"\"\")\n",
        "\n",
        "                    # Add voice components for GROMACS chat\n",
        "                    with gr.Row():\n",
        "                        with gr.Column(scale=4):\n",
        "                            gromacs_chatbot = gr.Chatbot(\n",
        "                                height=400,\n",
        "                                bubble_full_width=False,\n",
        "                                show_label=False\n",
        "                            )\n",
        "                            gromacs_msg = gr.Textbox(\n",
        "                                placeholder=\"Ask about GROMACS simulations or your protocol...\",\n",
        "                                show_label=False,\n",
        "                                container=False\n",
        "                            )\n",
        "                            gromacs_clear = gr.ClearButton([gromacs_msg, gromacs_chatbot])\n",
        "\n",
        "                        with gr.Column(scale=1):\n",
        "                            gromacs_voice_selector = gr.Dropdown(\n",
        "                                choices=[\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"],\n",
        "                                value=\"alloy\",\n",
        "                                label=\"Select Voice\",\n",
        "                                container=True\n",
        "                            )\n",
        "                            gromacs_audio_output = gr.Audio(\n",
        "                                label=\"AI Response Audio\",\n",
        "                                type=\"filepath\"\n",
        "                            )\n",
        "\n",
        "                    def chat_with_gromacs(message, chat_history, history_state, protocol=None, benefits=None):\n",
        "                        \"\"\"\n",
        "                        Enhanced GROMACS chat assistant with proper flow control and error handling\n",
        "                        \"\"\"\n",
        "                        try:\n",
        "                            if history_state is None:\n",
        "                                history_state = []\n",
        "\n",
        "                            logger.info(\"Starting GROMACS chat interaction\")\n",
        "\n",
        "                            # Phase 1: Query Preparation\n",
        "                            try:\n",
        "                                logger.info(\"Preparing query and context\")\n",
        "                                # Create a synchronous query to prevent stalling\n",
        "                                query_result = query_engine.query(\n",
        "                                    message,\n",
        "                                    response_mode=\"compact\",  # Use compact mode for faster processing\n",
        "                                    timeout=30  # Add timeout to prevent hanging\n",
        "                                )\n",
        "                                relevant_context = str(query_result.response)\n",
        "                                logger.info(\"Context retrieved successfully\")\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Error in query phase: {str(e)}\")\n",
        "                                relevant_context = \"Unable to retrieve context. Proceeding with basic response.\"\n",
        "\n",
        "                            # Phase 2: Chat Completion\n",
        "                            try:\n",
        "                                logger.info(\"Initiating chat completion\")\n",
        "                                system_message = {\n",
        "                                    \"role\": \"system\",\n",
        "                                    \"content\": f\"\"\"You are an AI assistant specializing in GROMACS molecular dynamics simulations and NVIDIA technologies.\n",
        "                                    Use this information to inform your response:\n",
        "\n",
        "                                    Relevant Context: {relevant_context}\n",
        "                                    Current Protocol: {protocol if protocol else 'No protocol loaded'}\n",
        "                                    NVIDIA Benefits: {benefits if benefits else 'No benefits analysis loaded'}\n",
        "\n",
        "                                    Focus on:\n",
        "                                    1. GROMACS simulation setup and optimization\n",
        "                                    2. NVIDIA GPU acceleration features\n",
        "                                    3. Best practices for molecular dynamics\n",
        "                                    4. Troubleshooting common issues\n",
        "                                    5. Integration with other tools and workflows\n",
        "\n",
        "                                    If you don't have enough information to answer a specific question,\n",
        "                                    please say so and offer to help with other aspects of GROMACS or NVIDIA's capabilities.\"\"\"\n",
        "                                }\n",
        "\n",
        "                                messages = [system_message] + history_state + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "                                response = openai_client.chat.completions.create(\n",
        "                                    model=\"gpt-4o\",\n",
        "                                    messages=messages,\n",
        "                                    max_tokens=16384,\n",
        "                                    temperature=0,\n",
        "                                    stream=False,\n",
        "                                    timeout=60  # Add timeout for chat completion\n",
        "                                )\n",
        "\n",
        "                                response_text = response.choices[0].message.content\n",
        "                                logger.info(\"Chat completion successful\")\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Error in chat completion: {str(e)}\")\n",
        "                                return message, chat_history, history_state, None\n",
        "\n",
        "                            # Phase 3: Audio Generation\n",
        "                            try:\n",
        "                                logger.info(\"Starting audio generation\")\n",
        "                                audio_handler = AudioResponseHandler(openai_client)\n",
        "                                audio_file_path = audio_handler.text_to_speech(response_text)\n",
        "\n",
        "                                if not audio_file_path:\n",
        "                                    logger.warning(\"Audio generation returned no file path\")\n",
        "                                    raise ValueError(\"Audio generation failed\")\n",
        "\n",
        "                                logger.info(f\"Audio generated successfully: {audio_file_path}\")\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Error in audio generation: {str(e)}\")\n",
        "                                audio_file_path = None\n",
        "                                response_text += \"\\n\\n(Note: Audio response generation failed. Text response is still available.)\"\n",
        "\n",
        "                            # Update conversation state\n",
        "                            chat_history = chat_history + [(message, response_text)]\n",
        "                            history_state.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "\n",
        "                            logger.info(\"GROMACS chat interaction completed successfully\")\n",
        "                            return message, chat_history, history_state, audio_file_path\n",
        "\n",
        "                        except Exception as e:\n",
        "                            logger.error(f\"Critical error in GROMACS chat system: {str(e)}\")\n",
        "                            error_message = \"I apologize, but I encountered an unexpected error. Please try again.\"\n",
        "                            chat_history = chat_history + [(message, error_message)]\n",
        "                            return message, chat_history, history_state, None\n",
        "\n",
        "                    # Handler functions with monitoring\n",
        "                    def gromacs_chat_handler(message, chat_history, state, protocol=None, benefits=None):\n",
        "                        \"\"\"Chat handler with monitoring and tracing\"\"\"\n",
        "                        try:\n",
        "                            with tracing.start_as_current_span(\"gromacs_chat_interaction\") as chat_span:\n",
        "                                chat_span.set_attribute(\"message_length\", len(message))\n",
        "                                result = chat_with_gromacs(message, chat_history, state, protocol, benefits)\n",
        "                                chat_monitor.record_success()\n",
        "                                return result\n",
        "                        except Exception as e:\n",
        "                            chat_monitor.record_failure(e)\n",
        "                            logger.error(f\"GROMACS chat handler error: {str(e)}\")\n",
        "                            raise gr.Error(f\"Chat system error: {str(e)}\")\n",
        "\n",
        "                    def update_gromacs_voice_handler(voice):\n",
        "                        \"\"\"Voice update handler with monitoring\"\"\"\n",
        "                        try:\n",
        "                            audio_handler = AudioResponseHandler(openai_client)\n",
        "                            audio_handler.set_voice(voice)\n",
        "                            chat_monitor.record_success()\n",
        "                            return None\n",
        "                        except Exception as e:\n",
        "                            chat_monitor.record_failure(e)\n",
        "                            logger.error(f\"Voice update error: {str(e)}\")\n",
        "                            raise gr.Error(f\"Voice update error: {str(e)}\")\n",
        "\n",
        "                    gr.Markdown(\"\"\"## GROMACS Protocol Generator\"\"\")\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            gromacs_file_input = gr.File(\n",
        "                                label=\"Upload Protein Structure (PDB)\",\n",
        "                                file_types=[\".pdb\"]\n",
        "                            )\n",
        "                            gromacs_generate_button = gr.Button(\"🧬 Generate GROMACS Protocol\", variant=\"primary\")\n",
        "                        with gr.Column():\n",
        "                            gromacs_protocol_output = gr.Textbox(\n",
        "                                label=\"GROMACS Protocol (Improved with DeepSeek)\",\n",
        "                                lines=10\n",
        "                            )\n",
        "                            gromacs_benefits_output = gr.Textbox(\n",
        "                                label=\"Benefits of NVIDIA Technology in GROMACS Simulation\",\n",
        "                                lines=5\n",
        "                            )\n",
        "\n",
        "                    # Connect the protocol generator\n",
        "                    gromacs_generate_button.click(\n",
        "                        gromacs_interface,\n",
        "                        inputs=[gromacs_file_input],\n",
        "                        outputs=[gromacs_protocol_output, gromacs_benefits_output]\n",
        "                    )\n",
        "\n",
        "                    # Connect the chat interface\n",
        "                    gromacs_msg.submit(\n",
        "                        gromacs_chat_handler,\n",
        "                        inputs=[\n",
        "                            gromacs_msg,\n",
        "                            gromacs_chatbot,\n",
        "                            gr.State([]),\n",
        "                            gromacs_protocol_output,\n",
        "                            gromacs_benefits_output\n",
        "                        ],\n",
        "                        outputs=[\n",
        "                            gromacs_msg,\n",
        "                            gromacs_chatbot,\n",
        "                            gr.State([]),\n",
        "                            gromacs_audio_output\n",
        "                        ]\n",
        "                    )\n",
        "\n",
        "                    # Connect voice selection\n",
        "                    gromacs_voice_selector.change(\n",
        "                        update_gromacs_voice_handler,\n",
        "                        inputs=[gromacs_voice_selector],\n",
        "                        outputs=[gromacs_audio_output]\n",
        "                    )\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ---\n",
        "            Developed by Joseph Steward. For questions, please contact jsteward2930@gmail.com.\n",
        "            \"\"\")\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        with tracing.start_as_current_span(\"gradio_launch\") as span:\n",
        "            iface.launch(share=True)\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to initialize application: {str(e)}\")\n",
        "    print(f\"Failed to initialize application: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "collapsed": true,
        "id": "3A0BSbGYHkmR",
        "outputId": "55ae4246-875f-4b73-8464-9551522b30da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d3167e7c4c22c51a15.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d3167e7c4c22c51a15.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}